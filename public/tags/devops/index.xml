<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>DevOps on Mischa van den Burg</title>
    <link>https://mischavandenburg.com/tags/devops/</link>
    <description>Recent content in DevOps on Mischa van den Burg</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 07 Jan 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://mischavandenburg.com/tags/devops/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Deploying a simple Linux VM to Azure with Terraform</title>
      <link>https://mischavandenburg.com/zet/terraform-linux-vm/</link>
      <pubDate>Sat, 07 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/terraform-linux-vm/</guid>
      <description>For a project I&amp;rsquo;m setting up my environment with Terraform.
I used this tutorial, but modified the code to make it simpler and easier to understand for beginners. The original uses a random module to generate random names, and generates a new SSH key. Also, this tutorial uses expensive VM tiers and Premium storage, which are not necessary when you are learning.
I also thought the SSH configuration was overcomplicated. My version just takes an SSH keypair stored at ~/.</description>
      <content:encoded><![CDATA[<p>For a project I&rsquo;m setting up my environment with Terraform.</p>
<p>I used <a href="https://learn.microsoft.com/en-us/azure/virtual-machines/linux/quick-create-terraform">this tutorial</a>, but modified the code to make it simpler and easier to understand for beginners. The original uses a random module to generate random names, and generates a new SSH key. Also, this tutorial uses expensive VM tiers and Premium storage, which are not necessary when you are learning.</p>
<p>I also thought the SSH configuration was overcomplicated. My version just takes an SSH keypair stored at <code>~/.ssh/id_rsa.pub</code></p>
<p>To run:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-hcl" data-lang="hcl"><span class="line"><span class="cl"><span class="k">terraform</span> <span class="k">init</span>
</span></span><span class="line"><span class="cl"><span class="k">terraform</span> <span class="k">plan</span>
</span></span><span class="line"><span class="cl"><span class="k">terraform</span> <span class="k">apply</span>
</span></span></code></pre></div><p>The scripts prints the public IP of the newly created VM. You should be able to SSH to it:</p>
<p><code>ssh azureuser@the_printed_ip_address</code></p>
<p>You can find the code in <a href="https://github.com/mischavandenburg/lab/tree/main/terraform/azure-simple-linux-vm">my &ldquo;lab&rdquo; repo on GitHub.</a></p>
<p><a href="https://github.com/mischavandenburg/lab/tree/main/terraform/azure-simple-linux-vm">https://github.com/mischavandenburg/lab/tree/main/terraform/azure-simple-linux-vm</a></p>
<p><a href="https://learn.microsoft.com/en-us/azure/virtual-machines/linux/quick-create-terraform">https://learn.microsoft.com/en-us/azure/virtual-machines/linux/quick-create-terraform</a></p>
<p><a href="https://learn.microsoft.com/en-us/azure/developer/terraform/authenticate-to-azure?source=recommendations&amp;tabs=bash#authenticate-to-azure-via-a-microsoft-account">https://learn.microsoft.com/en-us/azure/developer/terraform/authenticate-to-azure?source=recommendations&amp;tabs=bash#authenticate-to-azure-via-a-microsoft-account</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Starting a Project</title>
      <link>https://mischavandenburg.com/zet/starting-a-project/</link>
      <pubDate>Mon, 02 Jan 2023 21:00:11 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/starting-a-project/</guid>
      <description>I&amp;rsquo;m starting a project with a friend. Developing an application. We make a good team, he&amp;rsquo;s great at coding and knows the backend too.
He&amp;rsquo;ll do the development, I&amp;rsquo;m in charge of hosting. We&amp;rsquo;re setting everything up in Azure DevOps, so it is a great way to practice my Azure skills and apply the things I&amp;rsquo;ve learned in my recently obtained AZ-104 Azure Administrator certification.
Even though it is a small scale hobby project, I still plan to approach it as if it was an enterprise production application.</description>
      <content:encoded><![CDATA[<p>I&rsquo;m starting a project with a friend. Developing an application. We make a good team, he&rsquo;s great at coding and knows the backend too.</p>
<p>He&rsquo;ll do the development, I&rsquo;m in charge of hosting. We&rsquo;re setting everything up in Azure DevOps, so it is a great way to practice my Azure skills and apply the things I&rsquo;ve learned in my recently obtained <a href="/zet/articles/az-104-study-guide/">AZ-104</a> Azure Administrator certification.</p>
<p>Even though it is a small scale hobby project, I still plan to approach it as if it was an enterprise production application. I&rsquo;ll set up a full CI/CD pipeline with testing in a secure manner. Credentials stored in an Azure key vault and images pushed to a private registry.</p>
<p>This is going to be fun!</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>What Are Containers?</title>
      <link>https://mischavandenburg.com/zet/articles/what-are-containers/</link>
      <pubDate>Sun, 01 Jan 2023 16:17:58 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/what-are-containers/</guid>
      <description>When you learn about DevOps, you will come across the concept of a container early on. This is a &amp;ldquo;Mischa Explains&amp;rdquo; article where I attempt to explain a concept in my own words as simply as possible. I use the Feynman technique and pretend to explain it to a 12-year-old.
Virtualization To understand containers, we need to understand virtualization. Virtualization is the process of creating &amp;ldquo;fake computers&amp;rdquo; or &amp;ldquo;virtual computers&amp;rdquo; on a physical computer.</description>
      <content:encoded><![CDATA[<p>When you learn about DevOps, you will come across the concept of a container early on. This is a &ldquo;Mischa Explains&rdquo; article where I attempt to explain a concept in my own words as simply as possible. I use the Feynman technique and pretend to explain it to a 12-year-old.</p>
<h1 id="virtualization">Virtualization</h1>
<p>To understand containers, we need to understand virtualization. Virtualization is the process of creating &ldquo;fake computers&rdquo; or &ldquo;virtual computers&rdquo; on a physical computer.</p>
<p>On your desk, you have a laptop or a desktop PC. This machine has hardware such as a motherboard, a hard disk, and a video card. To run programs on your computer, it needs an Operating System. Usually, Windows, macOS, or Linux are used.</p>
<p>Let&rsquo;s say you have a computer running Windows, but you want to run a program that can only run on Linux. One solution is to buy another laptop and put it beside your Windows laptop on your desk. So now you have two computers with two different operating systems.</p>
<p>Fortunately, there are other solutions. We can use virtualization to make a Virtual Machine. A virtual machine is created by software to imitate a fully functional running computer inside your current operating system. You can create a virtual machine that runs Linux on your Windows computer. Your Windows computer running the Linux virtual machine is known as the **host.</p>
<p>Now you don&rsquo;t need to buy another computer to run your Linux program. Instead, you can boot up your Linux virtual machine and run your program when needed. If you have a powerful computer, you could run ten or more virtual machines, each of which has its own operating system and custom environment.</p>
<h1 id="containers">Containers</h1>
<p>Every time you create a virtual machine, the virtual machine needs a complete operating system to work. So, first, the software creates a virtual processor, virtual video card, and a virtual network interface. Then, it runs a fully functional operating system on that virtual hardware. This takes up a lot of resources.</p>
<p>Containers are lightweight packages of software. They are designed to do a very specific task, and therefore they only contain the resources they need to do that task. Nothing more.</p>
<p>Containers use the operating system of the physical computer to run. They have a very minimal, lightweight operating system inside them, but it only contains the elements they need to do their specific task. Therefore, containers are very easy to distribute, and you can run them very quickly.</p>
<h1 id="containers-are-like-newspapers">Containers are like newspapers</h1>
<p>Containers are like newspapers. Newspapers have a particular task: providing you with the day&rsquo;s news. You cannot use newspapers to study for your mathematics exam. You use your math book to study for your math exam. If you want to be informed of the day&rsquo;s news, you use a newspaper. This is what I mean by containers having a specific task.</p>
<p>Next, newspapers are printed on a specific kind of paper. When you buy an expensive book, it will have a sturdy and durable cover, and the pages are made of nice thick paper that will last a long time. The pages don&rsquo;t tear very quickly, and when the book gets wet, it can withstand it. This thick cover and high-quality papers are like the operating system of a virtual machine.</p>
<p>Newspapers, on the other hand, are printed on very thin paper. Because they are designed to distribute the news to you effectively, newspapers do not need to be stored forever or do any other tasks. If you used thick, expensive paper for newspapers, they would become costly, and no one would buy them anymore. The paper is optimized to bring the news to you.</p>
<p>In the same way, the container only comes with the components it needs to do its specific task. Therefore, the container is optimized for its purpose. As a result, they can be distributed more quickly and do not take up a lot of resources when running.</p>
<p>There are other benefits to containers, such as improving the ability to autoscale your application, but I will expand on those in a future blog post.</p>
<h1 id="further-study">Further study</h1>
<p>To learn more about containers, you can use the following resources:</p>
<p><a href="https://youtu.be/r6YIlPEC4y4">Containers &amp; Friends from John Savill&rsquo;s DevOps Masterclass</a></p>
<p><a href="https://docs.docker.com/get-started/overview/">Docker Documentation</a></p>
<p><a href="https://youtu.be/3c-iBn73dDE">Docker Tutorial for Beginners</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Study Guide: AZ-104 Azure Administrator Associate</title>
      <link>https://mischavandenburg.com/zet/articles/az-104-study-guide/</link>
      <pubDate>Fri, 30 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/az-104-study-guide/</guid>
      <description>TLDR It took me 80 hours of studying to gain this certification. Here are my notes and Anki deck: GitHub repo
Introduction When I did my English degree at University, exams were usually handwritten essays that needed to be done within a 3-hour timeframe. Sometimes we had multiple-choice tests, and I would always consider them a guaranteed pass because multiple-choice was much easier in my experience.
That opinion has changed since I&amp;rsquo;ve obtained a few IT certifications.</description>
      <content:encoded><![CDATA[<h1 id="tldr">TLDR</h1>
<p>It took me 80 hours of studying to gain this certification. Here are my notes and Anki deck:
<a href="https://github.com/mischavandenburg/az-104-azure-administrator">GitHub repo</a></p>
<h1 id="introduction">Introduction</h1>
<p>When I did my English degree at University, exams were usually handwritten essays that needed to be done within a 3-hour timeframe. Sometimes we had multiple-choice tests, and I would always consider them a guaranteed pass because multiple-choice was much easier in my experience.</p>
<p>That opinion has changed since I&rsquo;ve obtained a few IT certifications. These tests are hard! I&rsquo;m typing this while sitting on the bus on my way home from my AZ-104 exam. I passed with an 860 score of 1000, where 700 or higher is a pass. But it was an astonishingly tough exam. Usually, I finish quickly and spend at least half an hour reviewing my answers. I had only 5 minutes to review my questions this time because I had used up all of the available time. The questions required intense concentration and were time-consuming because I needed to compare many options which were very similar to each other. There were no easy questions.</p>
<h1 id="preparation">Preparation</h1>
<p>I studied 80 hours for this exam in a month. I work full-time as a DevOps Engineer, so I study in the evenings and on weekends. I have my Azure Fundamentals and CKA, but I only work with Azure occasionally in my current role.</p>
<p>Here is what I did to prepare for my exam:</p>
<ul>
<li>Go through all of the <a href="https://learn.microsoft.com/en-us/certifications/exams/az-104">Microsoft Learn modules for the AZ-104</a></li>
<li>Watch the entire <a href="https://www.youtube.com/playlist?list=PLlVtbbG169nGlGPWs9xaLKT1KfwqREHbs">AZ-104 study list by John Savill</a></li>
<li>Practice exams on <a href="https://tutorialsdojo.com/">TutorialsDojo</a> until I could pass them with 90%+ scores</li>
<li>Microsoft ESI practice exams</li>
<li><a href="https://learn.microsoft.com/en-us/shows/exam-readiness-zone/preparing-for-az-104-manage-azure-identities-and-governance-1-of-5">Microsoft AZ-104 Exam prep videos</a></li>
</ul>
<h2 id="microsoft-learn">Microsoft Learn</h2>
<p>You really need to master all of the subject matter. Only completing the Microsoft Learn modules is not enough preparation. They are more like summaries. At the end of each module, they provide links to the documentation for the subject for further study. Unfortunately, Microsoft does not go easy on you. It expects you to know obscure details of nearly every service this exam covers. Therefore, I advise going beyond the Microsoft Learn modules and studying the linked articles after each module.</p>
<h2 id="youtube-az-104-study-playlist-by-john-savill">YouTube AZ-104 Study Playlist by John Savill</h2>
<p>I&rsquo;m not sure if it&rsquo;s better to watch this playlist first and then do the Microsoft modules or the other way around. I did the Microsoft modules first, but for my next exam (AZ-400 DevOps Expert), I&rsquo;ll start with the videos and then do the Microsoft Learn modules.</p>
<h2 id="tutorialsdojo">Tutorialsdojo</h2>
<p>These practice exams are excellent. I used them in preparation for my fundamentals exam.</p>
<p>The best thing about them is that they provide extensive documentation and explanation of the questions. So after you finish the exam, you can study a lot with these examples.</p>
<h2 id="esi-practice-exams">ESI Practice Exams</h2>
<p>You&rsquo;re lucky if your organization participates in Microsoft&rsquo;s <a href="esi.microsoft.com/">Enterprise Skills Initiative</a>. The practice exams provided in the ESI environment give you a good indication of what you can expect at the exam. I first did the Tutorialsdojo exams and then moved on to the ESI exams, and I was humiliated. The ESI questions are very complex and hard to solve, and I learned a lot from these exams.</p>
<p>There are 210 questions total, and I worked through all of them, and whenever I failed a question, I did a deeper dive into the question&rsquo;s theme.</p>
<h1 id="studying">studying</h1>
<p>I take notes in Obsidian, and I use Anki for spaced repetition. I highly recommend keeping a deck of Anki cards and continuously testing yourself. You will need to memorize a lot of details. For example, you are expected to remember that storage accounts of the FileStorage type do not support Geo Redundant Storage. You can find my Anki deck in the <a href="https://github.com/mischavandenburg/az-104-azure-administrator">GitHub repo.</a></p>
<h1 id="links">links</h1>
<p><a href="https://learn.microsoft.com/en-us/certifications/exams/az-104">AZ-104 Exam page with learning modules</a></p>
<p><a href="https://www.youtube.com/playlist?list=PLlVtbbG169nGlGPWs9xaLKT1KfwqREHbs">John Savill&rsquo;s AZ-104 Study playlist</a></p>
<p><a href="https://github.com/mischavandenburg/az-104-azure-administrator">GitHub repo containing my notes and Anki deck</a></p>
<p><a href="https://learn.microsoft.com/en-us/shows/exam-readiness-zone/preparing-for-az-104-manage-azure-identities-and-governance-1-of-5">Microsoft AZ-104 Exam prep videos</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Lab Project: GitOps with ArgoCD, Azure Pipelines and Minikube</title>
      <link>https://mischavandenburg.com/zet/articles/lab-argocd-azure-pipelines/</link>
      <pubDate>Sat, 24 Dec 2022 13:49:55 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/lab-argocd-azure-pipelines/</guid>
      <description>This weekend I had a lot of fun with a project. I wanted to learn more about GitOps and try out ArgoCD.
My goal was to be able to deploy an application from a GitHub repo to my local Kubernetes cluster running in minikube. There are many options I could have used, such as running Jenkins in my cluster. But I wanted to use Azure pipelines for practice, which complicates the deployment to my local cluster, because the cluster is not running on Azure.</description>
      <content:encoded><![CDATA[<p>This weekend I had a lot of fun with a project. I wanted to learn more about GitOps and try out ArgoCD.</p>
<p>My goal was to be able to deploy an application from a GitHub repo to my local Kubernetes cluster running in minikube. There are many options I could have used, such as running Jenkins in my cluster. But I wanted to use Azure pipelines for practice, which complicates the deployment to my local cluster, because the cluster is not running on Azure. I also wanted to try out ArgoCD and learn more about GitOps.</p>
<p>The application is a simple web app that I wrote which displays a quote in the browser:</p>
<p><img loading="lazy" src="/app.png" type="" alt=""  /></p>
<h1 id="gitops-and-structure">GitOps and Structure</h1>
<p>GitOps is used to automate the process of provisioning infrastructure. Infrastructure as code is used to generate the same environment every time the environment is deployed.</p>
<p>For my project I have two separate GitHub repos. <a href="https://github.com/mischavandenburg/static-quote-app">The first repo</a> contains the code for a simple web app I created and the Dockerfile to generate the image. I call this my application repo. The other repo is <a href="https://github.com/mischavandenburg/static-quote-app-gitops">my GitOps repo</a> which contains the manifest files to deploy the application in Kubernetes. I decided to leverage Helm to create my manifest files. This way I can create templates and define my desired values in a values.yaml file in the repo.</p>
<p>Ultimately my goal was to use an Azure pipeline to build an image from my application repo and push it to Docker hub. This new image is given a new tag which needs to be stored. The first pipeline should trigger a new pipeline that makes a pull request to the GitOps repo to update the tag in my Helm chart.</p>
<p>ArgoCD will then scan the GitOps repo and realize that the tag has been updated, and deploy the new tag to my cluster.</p>
<h1 id="minikube">Minikube</h1>
<p>I used <a href="https://minikube.sigs.k8s.io/docs/">minikube</a> to deploy my local Kubernetes cluster. Another option is <a href="https://kind.sigs.k8s.io/">kind</a> (Kubernetes In Docker) but I wanted to use a VM approach this time.</p>
<h1 id="argocd">ArgoCD</h1>
<p><a href="https://argo-cd.readthedocs.io/">ArgoCD</a> is a declarative GitOps continuous delivery tool for Kubernetes. This is the solution I used to continuously scan my GitOps repo. When ArgoCD detects a change in the desired state, it will compare it with the state in my running cluster and make changes accordingly. I found a really good <a href="https://redhat-scholars.github.io/argocd-tutorial/argocd-tutorial/02-getting_started.html">tutorial to run ArgoCD in minikube</a>.</p>
<p><img loading="lazy" src="/argocd-dashboard.png" type="" alt="ArgoCD dashboard"  /></p>
<h1 id="azure-pipelines">Azure Pipelines</h1>
<p>With my cluster running on my local machine and my repos set up, I needed to use Azure Pipelines to bring it all together. Building the image and pushing it to Docker Hub wasn&rsquo;t a big deal. But I had two big challenges in my desired setup: I needed to pass the new tag number to a new pipeline, and I needed to use Azure Pipelines to create a new PR to my GitOps repo.</p>
<h3 id="passing-a-value-from-one-pipeline-to-another">Passing a value from one pipeline to another</h3>
<p>Interestingly, this wasn&rsquo;t as easy as it sounds, and from my internet searching it seemed that many people struggled with this. I decided to use the Variable Groups in Azure DevOps. However, after I finished writing my pipeline, I discovered I had no problems with reading the value from the Variable Groups, but it was impossible to update it using existing pipeline tasks. So I had to a bit of hacking to make it work. In the end I had to use the Azure CLI from within the pipeline to update my variable:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">stage</span><span class="p">:</span><span class="w"> </span><span class="l">update_tag</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">jobs</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">job</span><span class="p">:</span><span class="w"> </span><span class="l">update_tag_variable </span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">displayName</span><span class="p">:</span><span class="w"> </span><span class="l">Update Tag Variable</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">steps</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">bash</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">          az pipelines variable-group variable \
</span></span></span><span class="line"><span class="cl"><span class="sd">          update --group-id 202 \
</span></span></span><span class="line"><span class="cl"><span class="sd">          --org $(System.CollectionUri) \
</span></span></span><span class="line"><span class="cl"><span class="sd">          --project $(System.TeamProject) \
</span></span></span><span class="line"><span class="cl"><span class="sd">          --name tag --value $(Build.BuildId)</span><span class="w">          
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">env</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">AZURE_DEVOPS_EXT_PAT</span><span class="p">:</span><span class="w"> </span><span class="l">$(System.AccessToken)</span><span class="w">
</span></span></span></code></pre></div><p>This didn&rsquo;t feel like a very elegant solution, but it was the only solution I could come up with.</p>
<p>I also struggled a lot with permissions. I needed to find the correct service principal to assign the administrator rights to. <a href="https://stackoverflow.com/questions/52986076/having-no-permission-for-updating-variable-group-via-azure-devops-rest-api-from">This post</a> really helped to solve my problem.</p>
<h3 id="submitting-a-pr-to-a-github-repo">Submitting a PR to a GitHub repo</h3>
<p>When I started writing my pipeline I thought it would be very straightforward to just submit a PR to a repo, but I quickly discovered that this is not natively supported in Azure pipelines yet. In fact, I could not find a way to submit a PR at all. I had to settle for a solution that checks out the GitOps repo and creates a new branch. This new branch updates the tag in the values.yaml with the new tag that was passed from the previous pipeline.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">variables</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="l">mischa-quote</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">passed_tag</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l">$[variables.tag]</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">branch_name</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;pipeline-$(passed_tag)&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">pool</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">vmImage</span><span class="p">:</span><span class="w"> </span><span class="l">ubuntu-latest</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">steps</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">checkout</span><span class="p">:</span><span class="w"> </span><span class="l">self</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">persistCredentials</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">clean</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">script</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">      git config --global user.email &#34;mischa@pipeline.com&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">      git config --global user.name &#34;Mischa Pipeline&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">      git switch -c &#34;$(branch_name)&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">      sed -i &#34;s/tag:.*/tag: $(passed_tag)/&#34; values.yaml 
</span></span></span><span class="line"><span class="cl"><span class="sd">      git add .
</span></span></span><span class="line"><span class="cl"><span class="sd">      git commit -m &#34;Update tag to $(passed_tag)&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">      git push --set-upstream origin &#34;$(branch_name)&#34;</span><span class="w">      
</span></span></span></code></pre></div><p>This also felt a bit hacky to do with explicit shell commands, but it was the only way I could find to achieve my goal. I used sed to update the tag.</p>
<h1 id="result">Result</h1>
<p>The resulting deployment pipeline is as follows.</p>
<ol>
<li>I make a commit to my application repo, which triggers a build pipeline in Azure DevOps:</li>
</ol>
<p><img loading="lazy" src="/trigger-pipeline1.png" type="" alt=""  /></p>
<ol start="2">
<li>This resulted in an image pushed to my Docker Hub:</li>
</ol>
<p><img loading="lazy" src="/docker-hub.png" type="" alt=""  /></p>
<ol start="3">
<li>The pipeline created a new branch in my GitOps repo. Unfortunately, I have to make the PR myself, but as you can see, the pipeline successfully updates the values.yaml with the new tag which we also saw in Docker Hub:</li>
</ol>
<p><img loading="lazy" src="/new-branch.png" type="" alt=""  /></p>
<p><img loading="lazy" src="/update-tag.png" type="" alt=""  /></p>
<ol start="4">
<li>When I merged the pull request, ArgoCD detected the change and deployed a new pod with the new tag.</li>
</ol>
<p><img loading="lazy" src="/argocd-sync.png" type="" alt=""  /></p>
<ol start="5">
<li>Running a <code>kubectl describe</code> on the pod also verifies that we have the correct image:</li>
</ol>
<p><img loading="lazy" src="/kubectl-tag.png" type="" alt=""  /></p>
<h1 id="conclusion">Conclusion</h1>
<p>This was a fun challenge, but I learned a lot from solving the problems I encountered and my entire Saturday flew by in an uninterrupted flow state. I had some good practice in setting up Azure pipelines, learned about Helm, and did my first implementation of GitOps. Not bad for a day&rsquo;s work!</p>
<h1 id="links">Links</h1>
<p><a href="https://github.com/mischavandenburg/static-quote-app">Application GitHub repo</a></p>
<p><a href="https://github.com/mischavandenburg/static-quote-app-gitops">GitOps repo</a></p>
<p><a href="https://minikube.sigs.k8s.io/docs/">minikube</a></p>
<p><a href="https://kind.sigs.k8s.io/">kind</a></p>
<p><a href="https://argo-cd.readthedocs.io/">ArgoCD</a></p>
<p><a href="https://redhat-scholars.github.io/argocd-tutorial/argocd-tutorial/02-getting_started.html">tutorial to run ArgoCD in minikube</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Tokens and Identity on the Internet</title>
      <link>https://mischavandenburg.com/zet/articles/identity/</link>
      <pubDate>Sun, 18 Dec 2022 00:55:47 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/identity/</guid>
      <description>Introduction Have you ever thought about your identity on the internet? How does LinkedIn know it is you when you log in to LinkedIn? And when you allow LinkedIn to post to your Twitter, how does LinkedIn access your account and not your kindergarten teacher&amp;rsquo;s account?
This is a &amp;ldquo;Mischa Explains&amp;rdquo; article where I attempt to explain a concept in my own words as simply as possible. I use the Feynman technique and pretend to explain it to a 12-year-old.</description>
      <content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>
<p>Have you ever thought about your identity on the internet? How does LinkedIn know it is you when you log in to LinkedIn? And when you allow LinkedIn to post to your Twitter, how does LinkedIn access your account and not your kindergarten teacher&rsquo;s account?</p>
<p>This is a &ldquo;Mischa Explains&rdquo; article where I attempt to explain a concept in my own words as simply as possible. I use the Feynman technique and pretend to explain it to a 12-year-old.</p>
<h1 id="identity">Identity</h1>
<p>The first step in this process is identity. You need a starting point; for many of us, this can be our Google account. You signed up for this account and probably verified this with your phone number.</p>
<p>This relates to <strong>authentication</strong>. Authentication is the process of verifying identity. You&rsquo;ll need to provide the correct password when you log in to your Google account. You must give a valid password to log in to your account and access the resources. Google uses your password to <strong>authenticate</strong> that it is you.</p>
<h1 id="authorization">Authorization</h1>
<p>Then we have <strong>authorization</strong>. Authorization means granting access to particular resources. For example, let&rsquo;s say you are working in the science classroom at school. In the classroom is a bookcase that everybody can use: it is not dangerous, and every student can take the books they need without asking for permission. In the back of the science classroom is a cabinet that contains chemicals. It would be very dangerous if everybody could go into the cabinet and take out the sulphuric acid. Not everybody might know how dangerous sulphuric acid is. That&rsquo;s why the cabinet is locked.</p>
<p>If you need something from the chemicals cabinet, you need to ask permission from the teacher. You need to be <strong>authorized</strong> by the teacher to take out the sulphuric acid. When you make your request, the teacher may ask you questions to ensure you know what you are doing. He might even ask you for your school ID card because he has not seen you before. The teacher <strong>authenticates</strong> you by asking for your school ID, and then he <strong>authorizes</strong> you to take out the sulphuric acid.</p>
<h1 id="tokens">Tokens</h1>
<p>How do we accomplish this on the internet?</p>
<p>To verify identities on the internet, we have identity providers. Google is an identity provider. Azure AD is also an identity provider. An open-source identity provider is Keycloak.</p>
<p>Identity providers use <em>tokens</em> to verify identity and authorize access to resources. There are two types of tokens: ID tokens and access tokens. And for each token, there is an associated protocol.</p>
<h2 id="id-tokens">ID tokens</h2>
<p>OpenID Connect, also known as OIDC, is an open standard for authentication. Identity providers have agreed with each other that they will use this standard. When you go through an OpenID workflow, the result is an ID token, proving that the user has been authenticated.</p>
<p>Your school ID card is the ID token in our science class example. When you started at your school, you went through a registration process. Your parents probably handled this. Your name was written down, and the school verified that it was you by looking at your passport and talking to your parents. The result of this process was your school ID card, which you use to borrow books from the library. The school ID card proves that you are a student of that school and that you can use the facilities at the school.</p>
<h2 id="access-tokens">Access tokens</h2>
<p>These are specifically designed to allow access to a resource. For example, this resource could be a file on a server or a database.</p>
<p>Access tokens are strictly for authorization and use the OAuth 2.0 standard.</p>
<p>In our science class example, the token would be the key to the chemicals cabinet. The teacher authorizes you to access the cabinet and gives you the key to the cabinet.</p>
<h2 id="putting-it-together">Putting it Together</h2>
<p>Now let&rsquo;s put it together with an example.</p>
<p>You just created a new Facebook account and want to add all your friends. However, you have a Google account, and Facebook can use the contacts in your Google account to automatically add all of your friends.</p>
<p>Your Google account can only be accessed by you, and your contacts are locked away behind a password. But it is possible to grant Facebook access to this.</p>
<p>On Facebook, you select the &ldquo;import contacts from Google&rdquo; function. Facebook sends you to Google, and Google will ask you to log in. Google is the teacher in our science class example. Google needs you to <strong>authenticate</strong> to prove that it is you. When this is done, Google generates an ID token using OIDC for Facebook: Google gives Facebook a school ID that it can use.</p>
<p>Next, Facebook needs access to the contacts in your Google account. In our example, Facebook asks to take the sulphuric acid from the chemicals cabinet. You will see a menu that specifies what Facebook wants to do, and you need to give your permission. When you give your permission, Google generates an OAuth 2.0 token for Facebook. In other words, Google gives the key to the chemicals cabinet to Facebook, and Facebook is now authorized to take the sulphuric acid.</p>
<p>When both of these tokens are generated, Facebook contacts Google and asks if it can take the sulphuric acid from the chemicals cabinet.</p>
<p>Google, the teacher, asks Facebook for the school ID, and Facebook shows the ID card it received earlier. When Google is satisfied with the ID and successfully authenticates Facebook, it gives Facebook the key to the chemicals cabinet. Facebook is now authorized to take out the sulphuric acid. Facebook is now authorized to access the contacts in your Google account.</p>
<h1 id="links">Links</h1>
<p>You can use these resources to learn more about this topic:</p>
<p><a href="https://www.youtube.com/watch?v=t18YB3xDfXI">An Illustrated Guide to OAuth and OpenID connect</a></p>
<p><a href="https://www.youtube.com/watch?v=M4JIvUIE17c">ID Tokens vs Access Tokens - Do you know the difference?</a></p>
<p><a href="https://learn.microsoft.com/en-us/azure/active-directory/develop/id-tokens">Microsoft Learn: ID Tokens</a></p>
<p><a href="https://learn.microsoft.com/en-us/azure/active-directory/develop/security-tokens">Microsoft Learn: Security Tokens</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Podcast Tip: The system that runs Norway&#39;s welfare payments</title>
      <link>https://mischavandenburg.com/zet/articles/nav-podcast/</link>
      <pubDate>Fri, 02 Dec 2022 20:45:46 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/nav-podcast/</guid>
      <description>An interesting podcast episode describing the system that runs Norway&amp;rsquo;s welfare payments. It was interesting to hear that they focus on open source and that everything runs one one big kubernetes cluster.
It was also very inspiring to hear that they went from 4 nightly deployments a year to 1700 deployments a week.
Link to the podcast episode:
The system that runs Norway&amp;rsquo;s welfare payments</description>
      <content:encoded><![CDATA[<p>An interesting podcast episode describing the system that runs Norway&rsquo;s welfare payments. It was interesting to hear that they focus on open source and that everything runs one one big kubernetes cluster.</p>
<p>It was also very inspiring to hear that they went from 4 nightly deployments a year to 1700 deployments a week.</p>
<p>Link to the podcast episode:</p>
<p><a href="https://changelog.com/shipit/78">The system that runs Norway&rsquo;s welfare payments</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Book Notes: The Unicorn Project</title>
      <link>https://mischavandenburg.com/zet/articles/unicorn-project/</link>
      <pubDate>Tue, 11 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/unicorn-project/</guid>
      <description>This book is the sequel to the Phoenix project. Both books are set at Parts Unlimited, a fictitious company that supplies car parts to DIY mechanics and repair shops. Phoenix is a new system that Parts Unlimited has worked on for multiple years. It is supposed to handle order processing and communication between manufacturing, stores, and clients. Phoenix will also play a role in sales and marketing. The company has been gathering customer data for years, but it cannot use any of this data yet.</description>
      <content:encoded><![CDATA[<p>
  <img loading="lazy" src="https://m.media-amazon.com/images/W/IMAGERENDERING_521856-T1/images/I/91UM5i4nirL.jpg" alt="Unicorn Project"  /></p>
<p>This book is the sequel to <a href="/zet/articles/phoenix-project">the Phoenix project</a>. Both books are set at Parts Unlimited, a fictitious company that supplies car parts to DIY mechanics and repair shops. Phoenix is a new system that Parts Unlimited has worked on for multiple years. It is supposed to handle order processing and communication between manufacturing, stores, and clients. Phoenix will also play a role in sales and marketing. The company has been gathering customer data for years, but it cannot use any of this data yet. Phoenix will enable it to generate targeted marketing campaigns from the data when it&rsquo;s finished.</p>
<p>But as we saw in the previous book, it is far from finished, and things go wrong all the time. The company is not doing well, the stock prices are falling, and they need an edge over the competition. Phoenix will be their edge, but they&rsquo;ve been working on it for years. Eventually, management decided that Phoenix needed to be deployed in two weeks. But it is far from ready.</p>
<p>The main character in the Unicorn project is Maxine, a senior developer who temporarily transferred to a different department. She had to work on the Phoenix project against her will because of an unfortunate situation that needed a scapegoat. However, Maxine decides to make the best out of it, and she&rsquo;d like to begin with one thing: to get a Phoenix build going on her laptop.</p>
<p>Very quickly, she finds it impossible to run a full build of the Phoenix project due to missing files and other elements. She is appalled and makes it her mission to get the build going, but she meets another hurdle every step of the way. Missing credentials. Missing binaries and libraries. And for each of these hurdles, she must submit a ticket with a different department. Very soon, she has over 20 tickets running with long waiting times. Just to get a build going on her machine so she can work! Dozens of developers were hired to work on the Phoenix project. But when she asks them if they&rsquo;ve managed to get a build going yet, Maxine is horrified to discover that they&rsquo;ve tried for several months but haven&rsquo;t made any progress. Maxine has made more progress in a week.</p>
<blockquote>
<hr>
<p>&ldquo;Everyone around here thinks features are important because they can see them in their app, on the web page, or in the API. But no one seems to realize how important the build process is. Developers cannot be productive without a great build, integration, and test process.&rdquo;</p>
<ul>
<li>Unicorn Project</li>
</ul>
<hr>
</blockquote>
<p>After a few weeks, Maxine receives an invitation to have a drink with a group of people who are very interested in her. When she arrives at the bar, she meets the Rebellion: a group of developers, managers, and people from Operations, who are tired of the old organizational structure and want to make real changes. They think out of the box and experiment with new technologies, even though they are not authorized to do so.</p>
<p>With the Rebellion, Maxine significantly improves the build and deployment process. They recognized that Phoenix actually never was being built in its entirety. Developers were always working on parts of the application. However, after a lot of struggle, they create a build process that enables each developer to become operational on his first day.</p>
<p>This is the first step of a long series of exciting events that lead to Phoenix becoming a success. By the end of the book, they have a completely new development and testing process, and they can deploy changes to production without needing to take the entire application down. This allows them to create targeted marketing campaigns and respond to changes in the market. The first campaign was a huge success and generated the highest sales ever.</p>
<p>Maxine&rsquo;s struggle with the build process was an eye-opening experience for me. It gave me a very practical example of the need for DevOps principles to enable delivering value to customers. It is also something I recognize in my current organization. For example, projects can get stuck on a firewall change that needs to be approved by an external party. By implementing DevOps principles and arranging teams according to the &ldquo;you build it, you run it&rdquo; principle, teams can be responsible for the entire process from idea to production and therefore have a very short release cycle for their application.</p>
<p>I thoroughly enjoyed the first part of the book. However, the second part was less engaging to me. It became long-winded and felt like butter spread over too much bread. The author demonstrates a high level of technical experience and knowledge through his descriptions of processes, deployments, and fictional applications. Although I understand the intention of making Parts Unlimited a believable company, I think it could have been accomplished with much less detail and words.</p>
<p>The second part has more corporate drama, such as temporarily suspended managers without any clear reason. The focus shifts from a development and operations perspective to a managerial perspective. Maybe I will reread the book in a few years and this part will make a lot more sense to me then. The same happened when I reread the Phoenix project. I could not understand some aspects of the book, which became much clearer to me when I revisited it after gaining experience in the field.</p>
<p>I highly recommend this book to anyone working as a developer, DevOps Engineer, or in operations, especially if you are starting your career. The book gave me a lot of insights into &ldquo;the old way of working&rdquo; and a better understanding of the need for DevOps principles in the modern IT landscape. However, make sure to read the Phoenix project first.</p>
<h2 id="the-unicorn-project-a-novel-about-developers-digital-disruption-and-thriving-in-the-age-of-data-by-gene-kim">The Unicorn Project: A Novel about Developers, Digital Disruption, and Thriving in the Age of Data by Gene Kim</h2>
]]></content:encoded>
    </item>
    
    <item>
      <title>Book Notes: The Phoenix Project</title>
      <link>https://mischavandenburg.com/zet/articles/phoenix-project/</link>
      <pubDate>Fri, 16 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/phoenix-project/</guid>
      <description>When I started my DevOps traineeship, I borrowed this book from my boss and read it from cover to cover. I loved the story and the characters; it helped me understand &amp;ldquo;the old way&amp;rdquo; of doing things and the merits of implementing DevOps principles.
I reread the book ten months later. In the meantime, I&amp;rsquo;ve learned many new skills and technologies and started working full-time as a DevOps engineer.</description>
      <content:encoded><![CDATA[<p>
  <img loading="lazy" src="https://m.media-amazon.com/images/W/IMAGERENDERING_521856-T1/images/I/914-sUgELZL.jpg" alt="Phoenix Project"  /></p>
<p>When I started my DevOps traineeship, I borrowed this book from my boss and read it from cover to cover. I loved the story and the characters; it helped me understand &ldquo;the old way&rdquo; of doing things and the merits of implementing DevOps principles.</p>
<p>I reread the book ten months later. In the meantime, I&rsquo;ve learned many new skills and technologies and started working full-time as a DevOps engineer.</p>
<p>Not surprisingly, it made a lot more sense to me this time, and I&rsquo;m sure it will be even better when I reread it a few years later. In this article, I&rsquo;ll share my thoughts and notes on the book.</p>
<h2 id="the-story">The Story</h2>
<p>The main character is Bill Palmer, a mid-level IT manager in a manufacturing company called Parts Unlimited. Within a few pages, he is called into the CEO&rsquo;s office, and he is promoted to the VP of Operations, putting him in charge of IT, much against his own will or desire.</p>
<p>The situation Bill enters is a humorously chaotic one. We are thrown straight into a Sev 1 incident where managers point fingers and shout at each other. We quickly get the impression that this is a dysfunctional department that only performs tasks for the manager who shouts the loudest while fighting off crippling outages.</p>
<blockquote>
<p>It’s like the Wild West out here. We’re mostly shooting from the hip.”<br>
<em>The Phoenix Project</em></p>
</blockquote>
<p>The bulk of the story revolves around how Bill, together with his team of managers, Wes, Patty, and John, manage to turn this chaos into a department that does work according to a streamlined plan in a much more predictable manner.</p>
<p>To achieve this goal, Bill is introduced to Erik, a prospective board member of the company. Erik becomes Bill&rsquo;s mentor and guides Bill through the process of creating order in the chaos. Their interaction reminds me of Zen masters training their disciples by asking deep questions which don&rsquo;t have an immediately apparent answer.</p>
<h2 id="master--disciple">Master &amp; Disciple</h2>
<p>Erik takes Bill to the manufacturing plant of Parts Unlimited and tries to impress upon Bill that manufacturing planning principles from Lean can be applied to IT work. Erik argues that an IT department could be structured like a factory production line, but Bill is not ready to accept this.</p>
<p>A fundamental notion from manufacturing principles is that work should always be moving forwards along the production line, never backward. But unfortunately, this is very often the case in the &ldquo;old&rdquo; way of working: the development team works on an application for several months, and when they are finished with it, they throw it over the fence to the Operations people, whose job it is to deploy the application.</p>
<blockquote>
<p>One of the developers had actually walked in a couple of minutes ago and said, “Look, it’s running on my laptop. How hard can it be?”<br>
<em>The Phoenix Project</em></p>
</blockquote>
<p>However, as we see happening time after time in the book, usually the application is incompatible with the infrastructure it is deployed to. As a result, the application needs to go back to development. According to manufacturing theory, this is a situation where work goes backward through the production line, which we must avoid at all costs.</p>
<h2 id="implementation">Implementation</h2>
<p>Erik challenges Bill to start doing ten deployments a day instead of one deployment every nine months. Understandably, this is a ridiculous notion to Bill. The last few deployments were disastrous events that required his entire department to pull all-nighters through the weekend, and still, the stores were not managing to process all orders and payments.</p>
<p>However, Bill takes his mentor&rsquo;s advice and figures out a way to do it together with his team. One of the main problems they uncovered was the inconsistent deployment and production environments.</p>
<p>The solution to this problem was to involve the operations people in the development stage right from the beginning, so the development environment matched the production environment exactly. The environments were standardized and put in code with version control, and things started progressing quickly.</p>
<blockquote>
<p>As Wes talks, I think about Erik challenging me to think like a plant manager as opposed to a work center supervisor. I suddenly realize that he probably meant that I needed to span the departmental boundaries of Development and IT Operations.
<em>The Phoenix Project</em></p>
</blockquote>
<p>This is just one of the problems addressed by melting away the fence between Development and Operations. By the end of the book, the two camps started to work together much better. They come closer to the target of 10 deployments a day, and the DevOps way of working was born.</p>
<h2 id="closing-thoughts">Closing Thoughts</h2>
<p>I think this book is a must-read for anyone considering entering the DevOps field or anyone already working with DevOps.</p>
<p>As a nerd who loves structure and organization, the theme of the story is incredibly entertaining and satisfying to me. The authors excellently capture the transition from an utterly disorganized situation to a predictable environment with happy co-workers. Actually, I&rsquo;m a little embarrassed by how much joy this transition brings me.</p>
<p>Especially the second time around, it helped me better understand the underlying principles that enable the DevOps way of working in an organization. Moreover, it paints a great picture of how an organization can change for the better by embracing DevOps principles and how these changes express themselves in the improved quality and speed of software development and deployment. All of these advantages lead to delivering better value to the customer, which is the core focus of any productive and creative endeavor involving customers and end users.</p>
<h2 id="the-phoenix-project-written-by-gene-kim-kevin-behr-george-spafford">The Phoenix Project, written by Gene Kim, Kevin Behr, George Spafford</h2>
]]></content:encoded>
    </item>
    
    <item>
      <title>Certified Kubernetes Administrator (CKA) Exam Tips</title>
      <link>https://mischavandenburg.com/zet/articles/cka-tips/</link>
      <pubDate>Tue, 13 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/cka-tips/</guid>
      <description>I recently obtained my CKA certification. I started this certification journey with zero knowledge of Kubernetes. However, I was already working as a DevOps Engineer, and I know a fair bit of Linux. I daily drive Arch Linux and have LPIC-1 certification. It was handy to know where files are located on Linux systems and how to interact with systemd services. I also knew yaml quite well because I work with Ansible daily.</description>
      <content:encoded><![CDATA[<p>I recently obtained my CKA certification. I started this certification journey with zero knowledge of Kubernetes. However, I was already working as a DevOps Engineer, and I know a fair bit of Linux. I daily drive Arch Linux and have LPIC-1 certification. It was handy to know where files are located on Linux systems and how to interact with systemd services. I also knew yaml quite well because I work with Ansible daily. I passed on my first try, and I did one session of killer.sh.</p>
<h3 id="my-preparation">My preparation</h3>
<ul>
<li><a href="https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests/">KodeKloud CKA Course</a></li>
<li><a href="https://killer.sh/">Killer.sh Mock Exam</a></li>
<li><a href="https://killercoda.com/killer-shell-cka">Killercoda</a></li>
</ul>
<p>I kept track of the time I spent on this certification. In total, I spent 80 hours on study and practice.</p>
<h3 id="in-hindsight">In Hindsight</h3>
<p>I spent too much time repeating things during the KodeKloud course. This is the one thing I would do differently if I could start over. I went over some modules multiple times and kept meticulous notes. However, I have hardly used any of those notes. But they will be nice to have for the future.</p>
<p>I learned most from the killer.sh exams. So I would advise you to go through the KodeKloud course and do all the exercises, but don&rsquo;t spend too much time repeating stuff. If you don&rsquo;t understand the topic at all, it is, of course, necessary to repeat it. But you don&rsquo;t need to know all the details.</p>
<h3 id="killersh">Killer.sh</h3>
<p>After I finished the KodeKloud course, I purchased the exam voucher and started the killer.sh on Saturday morning. I wanted to simulate the exam experience as much as possible, so I set the timer and did not allow myself to stand up for two hours. My first round was humiliating. I only managed to get 24 out of 125 points. A little shocked by the experience, I spent the whole Saturday going through all the solutions of the exercises that killer.sh provides. The explanations they give are extensive, and I found them helpful. Saturday evening, I went out for dinner with friends, and on Sunday morning, I passed killer.sh. I spent the whole Sunday studying the solutions more and more, and on my last try on Sunday evening, I scored 115 out of 125.</p>
<h2 id="tips">Tips:</h2>
<ul>
<li>
<p>I know tmux quite well and used it extensively during the killer.sh, but it was not necessary during the exam. No need to learn it if you don&rsquo;t know it already.</p>
</li>
<li>
<p>Knowing vim well will save you a lot of time at the exam. For example, dG to delete all lines until the end of the file from your current location. Run &ldquo;vimtutor&rdquo; on a Linux system to learn the basics.</p>
</li>
<li>
<p>You cannot use bookmarks. Learn how to search the docs efficiently. One handy one I figured out was to control + F and enter &ldquo;kind: Pod&rdquo; or &ldquo;kind: PersistentVolume&rdquo; to immediately go to the example YAML.</p>
</li>
<li>
<p>my exam environment did not need much extra configuration. All I added to my .bashrc was alias v=vim and export do=&quot;&ndash;dry-run=client -o yaml&quot; so you can use &ldquo;k run Nginx $do &gt; Nginx.YAML&rdquo;</p>
</li>
<li>
<p>The exam environment is not as bad as people make it out to be on the internet. There is a little delay while scrolling through the docs in the browser, but working in the terminal didn&rsquo;t give me any problems. Get used to the environment on killer.sh, and there should not be any surprises in the real exam environment.</p>
</li>
<li>
<p>Skip questions you cannot solve immediately. But don&rsquo;t spend time reviewing all the questions, sorting by the highest % and doing those first. You will lose a lot of time evaluating all of these questions. It is much better to solve the questions during your first pass through and skip the ones you cannot immediately solve.</p>
</li>
<li>
<p>When the 120-minute timer ran out, I was presented with a screen that said &ldquo;quit&rdquo; or &ldquo;request more time.&rdquo; I was pretty sure I could not get more time for this exam, so I just pressed &ldquo;quit.&rdquo; After I pressed quit, the application closed immediately, and there was no confirmation whatsoever that they received my exam results or anything. This was extremely disorienting, and I was left doubting if I had done it correctly. Eventually, I could see in the Linux Foundation portal that my exam was in Grading status.</p>
</li>
<li>
<p>Speed is of the essence. An hour before my exam, I used killercoda to get into the mood and get things up to speed. Learn to solve things quickly and don&rsquo;t spend time having to arrange terminal windows on your screen or stumbling around in vim. You cannot afford to lose time on these things.</p>
</li>
<li>
<p>Finally, this video is an excellent summary of all the necessary tips and information: <a href="https://www.youtube.com/watch?v=8VK9NJ3pObU"></a></p>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>How to Upgrade Java and Jenkins on Ubuntu 18.04</title>
      <link>https://mischavandenburg.com/how-to-upgrade-java-and-jenkins-on-ubuntu-18-04/</link>
      <pubDate>Tue, 19 Jul 2022 18:01:26 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/how-to-upgrade-java-and-jenkins-on-ubuntu-18-04/</guid>
      <description>Last week I upgraded Jenkins to the latest version on the server infrastructure at work. Starting with the Jenkins 2.357 release, Java 11 or Java 17 will be required to run Jenkins. Also, the upcoming LTS release will require Java 11.
This means that I also needed to update Java on our Jenkins servers. Here are the steps that I did to perform the Jenkins and Java upgrade.
SSH into the server and stop the service.</description>
      <content:encoded><![CDATA[<p>Last week I upgraded Jenkins to the latest version on the server infrastructure at work. Starting with the Jenkins 2.357 release, Java 11 or Java 17 <a href="https://www.jenkins.io/blog/2022/06/28/require-java-11/">will be required to run Jenkins</a>. Also, the upcoming LTS release will require Java 11.</p>
<p>This means that I also needed to update Java on our Jenkins servers. Here are the steps that I did to perform the Jenkins and Java upgrade.</p>
<p>SSH into the server and stop the service. Then get the latest upgrades for your server, which is good practice:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">service jenkins stop
</span></span><span class="line"><span class="cl">apt-get update
</span></span><span class="line"><span class="cl">apt-get upgrade
</span></span></code></pre></div><p>Depending on your setup, the apt-get upgrade command might upgrade Jenkins to the latest version that does not require Java 11+. In my case, that was 3.346.</p>
<p><strong>When you get a question about updating your current config file, take the default option. This option keeps your current configuration.</strong></p>
<p>However, if your Jenkins is installed from a binary or another source, you might need to upgrade Jenkins to 3.346 using the Jenkins.war file:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> /usr/share/jenkins
</span></span><span class="line"><span class="cl">mv jenkins.war jenkins.war.old
</span></span><span class="line"><span class="cl">wget https://updates.jenkins-ci.org/latest/jenkins.war
</span></span><span class="line"><span class="cl">service jenkins start
</span></span></code></pre></div><p>When you start Jenkins, it will be updated to the latest version that does not require Java 11 or higher. You will notice that there will be a new folder called migrate in /usr/share/jenkins , and the jenkins.war is now located in /usr/share/java</p>
<p>This is where I got confused because it did not patch to the latest version, only up to 3.346 and the jenkins.war file was no longer being updated from the /usr/share/jenkins folder.</p>
<p>The reason is that this update moves the .war file to the /usr/share/java directory.</p>
<h1 id="java">java</h1>
<p>To get Jenkins to the latest version, we need to install or update Java and check if it has worked:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">apt-get install default-jre
</span></span><span class="line"><span class="cl">java -version
</span></span></code></pre></div><p>Now that you have updated the java version, you are ready to update Jenkins to the latest version.</p>
<p>Notice that we use the /usr/share/java folder now, instead of /usr/share/jenkins</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">service jenkins stop
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> /usr/share/java
</span></span><span class="line"><span class="cl">mv jenkins.war jenkins.war.old
</span></span><span class="line"><span class="cl">wget https://updates.jenkins-ci.org/latest/jenkins.war
</span></span><span class="line"><span class="cl">service jenkins start
</span></span></code></pre></div><h1 id="nodes">nodes</h1>
<p>When I accessed the Jenkins GUI, everything seemed fine, and my version was up to 3.358.</p>
<p>However, I noticed that the build nodes were all offline. When inspecting the logs, I saw the following error:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">java.io.EOFException
</span></span><span class="line"><span class="cl">	at java.base/java.io.ObjectInputStream<span class="nv">$PeekInputStream</span>.readFully<span class="o">(</span>ObjectInputStream.java:2905<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at java.base/java.io.ObjectInputStream<span class="nv">$BlockDataInputStream</span>.readShort<span class="o">(</span>ObjectInputStream.java:3400<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at java.base/java.io.ObjectInputStream.readStreamHeader<span class="o">(</span>ObjectInputStream.java:936<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at java.base/java.io.ObjectInputStream.&lt;init&gt;<span class="o">(</span>ObjectInputStream.java:379<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at hudson.remoting.ObjectInputStreamEx.&lt;init&gt;<span class="o">(</span>ObjectInputStreamEx.java:49<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at hudson.remoting.Command.readFrom<span class="o">(</span>Command.java:142<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at hudson.remoting.Command.readFrom<span class="o">(</span>Command.java:128<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at hudson.remoting.AbstractSynchronousByteArrayCommandTransport.read<span class="o">(</span>AbstractSynchronousByteArrayCommandTransport.java:35<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at hudson.remoting.SynchronousCommandTransport<span class="nv">$ReaderThread</span>.run<span class="o">(</span>SynchronousCommandTransport.java:61<span class="o">)</span>
</span></span><span class="line"><span class="cl">Caused: java.io.IOException: Unexpected termination of the channel
</span></span><span class="line"><span class="cl">	at hudson.remoting.SynchronousCommandTransport<span class="nv">$ReaderThread</span>.run<span class="o">(</span>SynchronousCommandTransport.java:75<span class="o">)</span>
</span></span></code></pre></div><p>Observing that the error had something to do with Java, I ssh’d into the build nodes and updated Java there as well with the same command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">apt-get install default-jre
</span></span></code></pre></div><p>After updating Java on the build node, head back to the GUI on the master node and restart the build node.</p>
<p>It should now be online again.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>5 Reasons Why I Changed my Career to IT in my Thirties</title>
      <link>https://mischavandenburg.com/5-reasons-why-i-changed-my-career-to-it-in-my-thirties/</link>
      <pubDate>Mon, 04 Jul 2022 20:04:33 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/5-reasons-why-i-changed-my-career-to-it-in-my-thirties/</guid>
      <description>In this post, I’d like to share the five main reasons why I changed my career to IT in my thirties. Making a career change can be daunting, especially when you are past your twenties, and employers can get more skeptical of hiring and training you. However, when I passed my twenties and became more serious and intentional about my life and career, I decided to take the plunge and hope for the best.</description>
      <content:encoded><![CDATA[<p>In this post, I’d like to share the five main reasons why I changed my career to IT in my thirties. Making a career change can be daunting, especially when you are past your twenties, and employers can get more skeptical of hiring and training you. However, when I passed my twenties and became more serious and intentional about my life and career, I decided to take the plunge and hope for the best. These are listed in no particular order.</p>
<h2 id="1-job-opportunities">#1 job opportunities</h2>
<p>IT has always been a field with many job opportunities, and with the current movements of digitalization and automation, there is no sign that it will slow down.</p>
<p>According to <a href="https://www.bls.gov/ooh/computer-and-information-technology/home.htm">the U.S. Bureau of Labor Statistics</a>, “Employment in computer and information technology occupations is projected to grow 13 percent from 2020 to 2030, faster than the average for all occupations. These occupations are projected to add about 667,600 new jobs. Demand for these workers will stem from greater emphasis on cloud computing, the collection and storage of big data, and information security.”</p>
<p>The situation is no different here in the Netherlands. Currently, there is a shortage of people in IT, and employers are much more willing to provide training to motivated individuals to make a change.</p>
<h2 id="2-remote-work">#2 remote work</h2>
<p>I think remote work is one of the best parts of living in post-pandemic 2022. I am an introvert, and having a quiet, stable space without distractions, which is the same from day to day, is a huge boost to my productivity.</p>
<p>Secondly, I think it is crucial to be mindful of your posture and body while working at a desk. For example, I am dependent on having a standing desk which I adjust more than ten times a day. I also need a chair suitable for my body type to avoid getting stiff and getting a sore back. Although some offices take care of providing these facilities to their employees, I think it is beneficial to invest in your own setup, which you can tailor to your own needs.</p>
<p>Thirdly, working from anywhere in the world is a massive advantage. I am not very interested in living a digital nomad lifestyle, working from a MacBook in coffee shops, but I think it’s great that you can spend some time abroad while working from that location.</p>
<h2 id="3-personal-interest">#3 personal interest</h2>
<p>This is a big one. You should not change your career to IT just because it earns well or because you think you can work from the beach in Thailand. I have been tinkering with computers and programming languages since I was a kid and have always enjoyed it. I always found myself “the computer guy” in groups of friends or colleagues.</p>
<p>However, for some reason, I never managed to make my career out of it until now, and I get a lot of satisfaction from my work every day after I made the change.</p>
<h2 id="4-high-income">#4 high income</h2>
<p>It is no secret that tech jobs are some of the best paying jobs in the U.S., having<a href="https://www.bls.gov/oes/current/oes_nat.htm#15-0000"> a mean wage of $99,860</a>. And if you work your way up into management, there are even higher salaries. Here <a href="https://www.nationaleberoepengids.nl/salaris/ict">in the Netherlands</a>, it is also a financially sound choice, with a mean wage of €47.200</p>
<h2 id="5-fast-changing-field">#5 fast-changing field</h2>
<p>IT is a broad field with many little niches you can get into, and every niche is constantly developing. Being in IT means you will need to stay on board by continuing to learn the new technologies and languages to keep on track.</p>
<p>This is also what makes it exciting to me, being a life-long learner. There is always more to learn and some cutting-edge technology to become familiar with, which can improve your workflow and your deployments.</p>
<h2 id="links">links:</h2>
<p><a href="https://www.bls.gov/ooh/computer-and-information-technology/home.htm">Computer and Information Technology Occupations – US Bureau of Labor Statistics</a></p>
<p><a href="https://www.bls.gov/oes/current/oes_nat.htm#15-0000">May 2021 National Occupational Employment and Wage Estimates</a></p>
<p><a href="https://www.nationaleberoepengids.nl/salaris/ict">Salaries in IT - Dutch</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Docker LEMP Stack deployed with Ansible</title>
      <link>https://mischavandenburg.com/docker-lemp-stack-deployed-with-ansible/</link>
      <pubDate>Sun, 30 Jan 2022 16:20:24 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/docker-lemp-stack-deployed-with-ansible/</guid>
      <description>In order to learn more about Docker and Ansible I am working on an assignment to take an existing application and to break it down into containers. However, in order to be able to understand this process properly, I first needed to understand more about Docker and containerisation.
I wrote a playbook that installs Docker and deploys a fully containerised LEMP stack on a virtual machine.
You can have a look at the Github repo with the result here.</description>
      <content:encoded><![CDATA[<p>In order to learn more about Docker and Ansible I am working on an assignment to take an existing application and to break it down into containers. However, in order to be able to understand this process properly, I first needed to understand more about Docker and containerisation.</p>
<p>I wrote a playbook that installs Docker and deploys a fully containerised LEMP stack on a virtual machine.</p>
<p><a href="https://github.com/mischavandenburg/lemp_docker_ansible">You can have a look at the Github repo with the result here. </a></p>
<p>The repo is using the “ansible-galaxy init” role structure. You will find the playbooks as follows: roles/your_choice/tasks/main.yml</p>
<h2 id="docker">Docker</h2>
<p>I was very excited to learn more about Docker and containerisation. I was familiar with the concept of virtualisation, which is creating virtual versions of fully functional machines on a host operating system. But the concept of containerisation was new to me.</p>
<p>As I understand it, containerisation differs drastically from virtualisation because containers are able to use resources from host directly. They do not need an entire operating system to run, and therefore they are a much more lightweight.</p>
<p>This means that resources can be used much more efficiently which eventually can mean cost reduction in your cloud infrastructure.</p>
<p>Docker is a very popular platform for building and running containers. It seemed like the best option to get started with deploying my own containers.</p>
<h2 id="lemp-stack">LEMP Stack</h2>
<p>My colleague recommended me <a href="https://tech.osteel.me/posts/docker-for-local-web-development-part-1-a-basic-lemp-stack">this tutorial</a> to become more familiar with Docker. It uses a LEMP stack as an example application. When I told friends about the fact that I was building a LEMP stack, they corrected me and said it was a LAMP stack.</p>
<p>The LAMP stack is a collection of software built out of these elements:</p>
<p>L – Linux: the operating system</p>
<p>A – Apace: webserver</p>
<p>M – MySQL: database</p>
<p>P – PHP: server scripting language</p>
<p>However, in a LEMP stack, we use NGINX as a webserver, which is pronounced “Engine X”, hence the E in LEMP stack. Therefore, LEMP is the correct way to spell it, and it is used in all the tutorials that I have been using.</p>
<p>I highly recommend <a href="https://tech.osteel.me/posts/docker-for-local-web-development-part-1-a-basic-lemp-stack">the tutorial</a> in order to learn how to deploy your first collection of containers. Deploying one container is relatively easy with Docker, but it gets a little more complicated when deploying several containers and making them communicate with each other in order to combine them into one application. But this tutorial does a great job at showing you how it’s done and it is especially good at explaining the steps along the way.</p>
<h2 id="docker-compose-vs-ansible">Docker Compose vs. Ansible</h2>
<p>Docker Compose is a tool you can use to run multi-container applications. With the help of the tutorial, it was fairly easy to understand and hit the ground running by deploying multiple containers into one network.</p>
<p>Now let’s have a look at how we actually set up the containers. In the Docker Compose file, the NGINX container was defined like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;3.8&#39;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="c">#Services</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">services</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="c">#Nginx Service</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">nginx</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">nginx:1.19</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="m">80</span><span class="p">:</span><span class="m">80</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="l">./src:/var/www/php</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="l">./.docker/nginx/conf.d:/etc/nginx/conf.d</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">depends_on</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="l">php</span><span class="w">
</span></span></span></code></pre></div><p>It looks pretty straightforward, right? Almost like pseudocode. We tell Docker which image to pull from the Docker Hub, and we tell it to route container’s port 80 to our host’s port 80. This ensures that the web server can be accessed from the outside, provided you have opened this port in the firewall.</p>
<p>Next there is the volumes section: this mounts certain directories on the host into the container so it is accessible. In this case this was necessary to transfer the web server configuration and the index.php which we wanted to serve to the outside.</p>
<p>Having successfully deployed my LEMP using Docker Compose, the next step was to automate this process by using Ansible. Ansible is a very powerful tool which enables you to automate configuration management and application deployment by writing scripts called playbooks.</p>
<h5 id="why-was-it-necessary-to-introduce-ansible">Why was it necessary to introduce Ansible?</h5>
<p>By using Docker Compose, you would need to have Docker and Docker Compose installed on the virtual machine before you could start running the containers.</p>
<p>However, Ansible gives you the power to take a completely fresh virtual machine, configure it from scratch, and install Docker and its necessary dependencies, followed by deploying the containers.</p>
<h4 id="now-lets-take-a-look-at-the-same-container-defined-in-ansible">Now let’s take a look at the same container defined in Ansible:</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl">- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">start nginx </span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">docker_container</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">nginx:1.19</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">detach</span><span class="p">:</span><span class="w"> </span><span class="kc">yes</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="m">80</span><span class="p">:</span><span class="m">80</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">networks</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">network_one</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="l">/src:/var/www/php</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="l">/.docker/nginx/conf.d:/etc/nginx/conf.d</span><span class="w">
</span></span></span></code></pre></div><p>Although there are some differences, they look very similar. Converting my Docker Compose file to an Ansible playbook was quite a natural and easy experience. It also helps that both are written in YAML and therefore use the same indentation conventions.</p>
<p>A few differences we can observe:</p>
<p>In the Ansible playbook we invoke the docker_container module, whereas they are defined as services in the Docker Compose file. Another difference is that we need to set up the network ourselves. In the Docker Compose file, we just specified the containers and Docker Compose created a network automatically and made sure that all containers were connected to it.</p>
<p>However, it isn’t very complicated in Ansible either:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl">- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">setup network</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">docker_network</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">network_one</span><span class="w">
</span></span></span></code></pre></div><p>We simply call the docker_network module and tell it to make a network called network_one. All we need to do then is make sure to set the networks: parameter to network_one in the docker_container module as we saw above.</p>
<p>The last point to note is the detach parameter. This means that the container will keep running in the background after it is started.</p>
<h3 id="result">Result</h3>
<p>After some debugging here and there and making sure all of the elements were in place, eventually we get the satisfying message that everything went according to plan:</p>
<p><img loading="lazy" src="/success.png" type="" alt="Successful play from Ansible"  /></p>
<p>The result is a webpage being served on the server ip:</p>
<p><img loading="lazy" src="/webpage.png" type="" alt="The final web page served by Nginx "  /></p>
<p>I know, it is not the prettiest or most intricate design. But remember that I am working towards becoming a DevOps Engineer, not a Front End Developer 😉</p>
<p>We can also enter the phpMyAdmin dashboard by adding port 8080 to our ip in the browser:</p>
<p><img loading="lazy" src="/php.png" type="" alt="The PHP page."  /></p>
<h2 id="conclusion">Conclusion</h2>
<p>The assignment of deploying a LEMP stack in separate containers has been very useful and I learned a lot from the process. There were a few more modules that needed to be configured in Ansible as opposed to the Docker Compose method, but the tradeoff is that Ansible is much more powerful and enables you to configure the server from scratch. You can have a look at the code in the GitHub repo to see all of the changes I needed to do.</p>
<p>The only part that I needed to do by hand is to create the VM in the Microsoft Azure portal, open the ports and configure the SSH keys. The next step in my learning process will be to learn how I can automate this step as well. This means that I will need to learn Terraform.</p>
<p>By using Terraform I will be truly deploying this stack as Infrastructure as Code, but doing all of these steps with Ansible has given me a much better understanding of Infrastructure as Code already.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>My Journey Into DevOps So Far</title>
      <link>https://mischavandenburg.com/my-journey-into-devops-so-far/</link>
      <pubDate>Fri, 28 Jan 2022 08:56:38 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/my-journey-into-devops-so-far/</guid>
      <description>In 2021 I had reached a certain stage in my life where I had the liberty to make a choice. I was 31 years old and had just finished an important chapter of my life, and was ready to begin a new one. After thinking about where I wanted to steer my professional career, I decided to follow my passion for programming and automation and attempt to make a career out of my interests in tech and IT.</description>
      <content:encoded><![CDATA[<p>In 2021 I had reached a certain stage in my life where I had the liberty to make a choice. I was 31 years old and had just finished an important chapter of my life, and was ready to begin a new one. After thinking about where I wanted to steer my professional career, I decided to follow my passion for programming and automation and attempt to make a career out of my interests in tech and IT.</p>
<p>I was in a fortunate situation, because there were large shortages of people on the IT job market in The Netherlands. Consequently, employers were much more willing to train their employees to perform the roles that they required to fulfill.</p>
<h2 id="bots-and-scripts">Bots and Scripts</h2>
<p>I have always loved messing around with computers, and I learned to write small and simple programs at a very young age. At the same time I was an avid gamer, and spent many hours slaying monsters in online RPG’s. One day I came across the concept of a bot: a program that plays the computer game for you, and I was hooked. This is where I developed my ‘fetish’ for automation.</p>
<p>I was very lucky that I had a friend who shared my interests, and together we built our own automation projects (called ‘botfarms’) in which we ran large amounts of bots that played a certain game for us. This army of bots generated in-game currency which we could sell for actual money. They weren’t huge profits, but it was an amazingly satisfying feeling to be the overlords of an army of automations that actually generated some income for us.</p>
<h2 id="from-bots-to-devops">From Bots to DevOps</h2>
<p>These botfarms were hosted on servers which set up ourselves. In order to save costs we rented Linux servers, and I spent many evenings figuring out how configure them via the command line. Often I would suddenly snap out of my flow at 3am and realise I had to go to work at 7 in the morning again.</p>
<p>Although I did not manage to make these personal interests into a personal career, my friend eventually became a Data Engineer. After making the decision to make a career switch to IT I needed to figure out which direction I wanted to go in, because IT covers a very broad range of topics and skills. Based on on our shared interests and previous projects, he recommended me to become a DevOps Engineer.</p>
<p>I very quickly realised that he was right on the money with his suggestion and I started to become very enthusiastic to learn how to make a living by working with computers and automation.</p>
<h2 id="traineeship">Traineeship</h2>
<p>As I mentioned before, employers in The Netherlands are now willing to train potential candidates, and I used my hobby projects as a way to demonstrate my genuine interest and affinity with IT and automation. I was offered a traineeship to become a DevOps Engineer in 2021. After a period of training I started working for the City of Amsterdam and I&rsquo;ve been part of an IT4IT operations team since.</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
