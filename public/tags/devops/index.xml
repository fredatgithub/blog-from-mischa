<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>DevOps on Mischa van den Burg</title>
    <link>https://mischavandenburg.com/tags/devops/</link>
    <description>Recent content in DevOps on Mischa van den Burg</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 24 Dec 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://mischavandenburg.com/tags/devops/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Wrote A Go Program That Posts To Mastodon</title>
      <link>https://mischavandenburg.com/zet/go-mastodon-post/</link>
      <pubDate>Sun, 24 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/go-mastodon-post/</guid>
      <description>This morning I had some fun writing a Go program which takes the latest blog post from my RSS feed and posts it to Mastodon. It is based on my twitter cli I wrote a while ago.
It was fun to write some Go again, it has been far too long. I&amp;rsquo;ve been writing mostly Terraform and yaml recently and working a lot with infrastructure. I haven&amp;rsquo;t been doing any projects at home that required programming.</description>
      <content:encoded><![CDATA[<p>This morning I had some fun writing a Go program which takes the latest blog post from my RSS feed and posts it to Mastodon. It is based on my <a href="https://github.com/mischavandenburg/twitter-cli">twitter cli</a> I wrote a while ago.</p>
<p>It was fun to write some Go again, it has been far too long. I&rsquo;ve been writing mostly Terraform and yaml recently and working a lot with infrastructure. I haven&rsquo;t been doing any projects at home that required programming. But recently I started up my personal blog and completely re-architected my social media setup.</p>
<p>I&rsquo;m going to post all of my personal stuff on Mastodon from here on out, not on X. However, I still wanted to post my tech content on Mastodon as well and I needed a tool to help me with that.</p>
<p>I have this program installed as a binary on my computer and I call it from my <a href="https://github.com/mischavandenburg/dotfiles/blob/main/scripts/blog">blog script</a> which I use to generate my website with Hugo and publish it to my web hosting solution. Now it can post to Twitter / X and Mastodon as well.</p>
<p>The Go code I wrote is available here:</p>
<p><a href="https://github.com/mischavandenburg/mastodon">https://github.com/mischavandenburg/mastodon</a></p>
<h2 id="links">Links:</h2>
<p>202312241112</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: Deploying MkDocs To Azure Using With Azure DevOps</title>
      <link>https://mischavandenburg.com/zet/video-mkdocs-azure-webapp/</link>
      <pubDate>Fri, 28 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/video-mkdocs-azure-webapp/</guid>
      <description>In this video, I will walk you through the following steps:
Set up a repo in Azure DevOps with MkDocs files Set up a Static Web App in Azure portal and link it to the repo Configure the Azure DevOps pipeline for MkDocs deployment Configure a custom theme for the website Demonstrate the pull request workflow for updating the docs By the end of this video, you will have a fully functional MkDocs site hosted on Azure Static Web Apps.</description>
      <content:encoded><![CDATA[
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/_-D1Qz6jtEU" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<p>In this video, I will walk you through the following steps:</p>
<ul>
<li>Set up a repo in Azure DevOps with MkDocs files</li>
<li>Set up a Static Web App in Azure portal and link it to the repo</li>
<li>Configure the Azure DevOps pipeline for MkDocs deployment</li>
<li>Configure a custom theme for the website</li>
<li>Demonstrate the pull request workflow for updating the docs</li>
</ul>
<p>By the end of this video, you will have a fully functional MkDocs site hosted on Azure Static Web Apps. You will also learn how to use Azure DevOps and Static Web Apps to collaborate on your documentation projects.</p>
<p>MkDocs is a tool that lets you create beautiful documentation websites from plain text files. You can write your docs in Markdown, a simple and easy-to-use markup language, and MkDocs will convert them into a static HTML site. You can also customize the look and feel of your site with themes and plugins.</p>
<p>Azure Static Web Apps is a service that hosts static web content and serverless APIs. It automatically builds and deploys your web app from a GitHub or Azure DevOps repository. It also provides features such as authentication, authorization, custom domains, and SSL certificates.</p>
<p>Timestamps:</p>
<p>0:00 Intro
0:40 What is MkDocs?
2:00 Set up the repo
5:00 Create web app
9:25 Configure the pipeline
11:30 Adding a few documents
15:25 Adding a custom theme
18:00 Adding docs with a pull request
21:30 Outro</p>
<h2 id="links">Links:</h2>
<p>202307281307</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Wrote A Script To Delete All Resource Groups In An Azure Subscription</title>
      <link>https://mischavandenburg.com/zet/script-delete-all-resource-groups/</link>
      <pubDate>Sat, 08 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/script-delete-all-resource-groups/</guid>
      <description>In the Azure portal you can&amp;rsquo;t select multiple resource groups for deletion.
I have a sponsored subscription to play around in which I sometimes wish to clean completely.
I wrote this script to delete all resource groups using bash.
#!/bin/bash # script to delete all resource groups in a subscription using Azure CLI # get the current subscription name to confirm subscription=$(az account show --query name -o tsv) echo &amp;#34;Use this script with caution!</description>
      <content:encoded><![CDATA[<p>In the Azure portal you can&rsquo;t select multiple resource groups for deletion.</p>
<p>I have a sponsored subscription to play around in which I sometimes wish to clean completely.</p>
<p>I wrote this script to delete all resource groups using bash.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="cp">#!/bin/bash
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="c1"># script to delete all resource groups in a subscription using Azure CLI</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># get the current subscription name to confirm</span>
</span></span><span class="line"><span class="cl"><span class="nv">subscription</span><span class="o">=</span><span class="k">$(</span>az account show --query name -o tsv<span class="k">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;Use this script with caution!&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;You are about to delete all resource groups in the subscription: </span><span class="nv">$subscription</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># prompt for confirmation</span>
</span></span><span class="line"><span class="cl"><span class="nb">read</span> -p <span class="s2">&#34;Are you sure? (y/n) &#34;</span> will_delete
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="o">[[</span> <span class="nv">$will_delete</span> <span class="o">==</span> <span class="o">[</span>Yy<span class="o">]</span>* <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">	<span class="nb">echo</span> <span class="s2">&#34;Deleting resource groups...&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="nv">groups</span><span class="o">=</span><span class="k">$(</span>az group list --query <span class="s2">&#34;[].name&#34;</span> -o tsv<span class="k">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1"># Loop through each group name and delete it</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> group in <span class="nv">$groups</span><span class="p">;</span> <span class="k">do</span>
</span></span><span class="line"><span class="cl">		az group delete --name <span class="s2">&#34;</span><span class="nv">$group</span><span class="s2">&#34;</span> --yes --no-wait
</span></span><span class="line"><span class="cl">	<span class="k">done</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="nb">echo</span> <span class="s2">&#34;All resource groups have been deleted.&#34;</span>
</span></span><span class="line"><span class="cl">	<span class="nb">exit</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl"><span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;Exiting without deleting any resource groups.&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;Probably wise.&#34;</span>
</span></span></code></pre></div><h2 id="links">Links:</h2>
<p>202307081507</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Choosing A Cloud And The Importance Of Specialization</title>
      <link>https://mischavandenburg.com/zet/choosing-a-cloud-provider-and-specialization/</link>
      <pubDate>Mon, 03 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/choosing-a-cloud-provider-and-specialization/</guid>
      <description>As a DevOps Engineer or Cloud Native Engineer, I think it’s important to specialize in one cloud provider in the beginning of your career. Specializing allows you to gain deep knowledge and expertise in the specific tools and services related to that cloud provider, making you a valuable asset to any company using that provider. Although cloud computing is similar in essence no matter which provider you choose, each cloud provider has its own vocabulary and way of structuring things.</description>
      <content:encoded><![CDATA[<p>As a DevOps Engineer or Cloud Native Engineer, I think it’s important to specialize in one cloud provider in the beginning of your career. Specializing allows you to gain deep knowledge and expertise in the specific tools and services related to that cloud provider, making you a valuable asset to any company using that provider. Although cloud computing is similar in essence no matter which provider you choose, each cloud provider has its own vocabulary and way of structuring things.</p>
<p>After my first year as a DevOps Engineer I was still trying to figure out which one to choose and which one would suit me best. With the rapid pace of innovation in the cloud computing industry, it can be challenging to stay up-to-date with the latest developments and features for multiple providers. I tried to keep up with the developments for each provider and tried to be a broad professional who could work with all of them. However, I think it is best to specialize early in your career, and then you can learn a different one after you have gained some significant experience with your first one.</p>
<p>After several insightful conversations with my colleagues, and after working with Azure for a year as a consultant, I decided to go all-in on Azure, and it has been a liberating experience so far. By focusing solely on Azure, I am able to fully immerse myself in their ecosystem and gain a deep understanding of their tools and services. I feel I have a sense of direction now, and I can limit the content I consume and the news I read to this one particular ecosystem only. And that feels like a relief to me.</p>
<p>This is of course very personal for everybody, but I’m someone who loves to learn and who tends to be a bit of a “jack of all trades” who knows a little bit of everything but who doesn’t have very deep knowledge of a particular subject in particular. By choosing a cloud and going all-in on it, I can become a specialist and go deep in Azure.</p>
<h1 id="why-i-chose-azure">Why I chose Azure</h1>
<p>I chose Azure because Microsoft is already very established in the Dutch market. Governments and large corporations typically have their entire user base stored in Active Directory and they use Microsoft Office licenses already. When they decide to move to the cloud, Microsoft offers them very good terms if they choose Microsoft instead of AWS, for example.</p>
<p>Azure’s strong presence in the Dutch market and their ability to offer attractive terms to existing Microsoft customers make them a popular choice for companies looking to move to the cloud. With 73 percent, Microsoft Azure took most of the market share for cloud computing in the Netherlands in 2020. Azure had within the healthcare sector the highest share with 90 percent. Microsoft has some very substantial clients within government, health care, and the corporate world.</p>
<p>These previous reasons are all related with job security and the assurance that there will be a long career in Azure ahead of me. But it&rsquo;s not only because of these reasons why I chose Azure. After gaining experience with Azure as a consultant, I&rsquo;ve discovered that the cloud ecosystem Microsoft has created suits me very well. I&rsquo;m very impressed with the different solutions that Azure is able to provide to solve business needs. Not every business is interested or capable of running everything on virtual machines, and Azure&rsquo;s PAAS and SAAS offerings provide level of of abstraction which can be tailored to any business out there.</p>
<p>Moreover, I really like the tooling. Although I really wanted to gain more experience with Terraform when I started my current contract, I&rsquo;ve fallen a little bit in love with Bicep after I started to work with it every day. I love the fact that Bicep uses Azure itself as a state file, rather than having to manage a state file which can be a pain when using it from pipelines or with multiple people. The Bicep language server (which can be used in Neovim as well) is a very pleasant editing experience. Azure DevOps really nailed it with being a &ldquo;one stop shop&rdquo; for everything from project management and tickets to CI/CD with Azure Pipelines. Although it has its quirks, Azure DevOps is a real pleasure to work with. And I also think that the Azure CLI is a very well-designed interface to the cloud platform.</p>
<h1 id="beyond-blue">Beyond Blue</h1>
<p>After I made my choice, I decided to join Beyond Blue. I was already working for Fullstaq, and Beyond Blue is a sister company of Fullstaq. Beyond Blue is a team of Azure Experts who love to go deep and who have a huge amount of experience and knowledge on Microsoft Azure. We help start-ups, scale-ups, and enterprises in both the public and private sector create scalable, flexible, and secure cloud-native solutions build on Microsoft Azure.</p>
<p>We have Microsoft Valued Professionals (MVP), Microsoft Certified Trainers (MCT), Azure Kubernetes Service and containerization experts on board who are ready to tackle each and every challenge for our clients.</p>
<h1 id="summing-up">Summing up</h1>
<p>To sum up, I think specializing early has the following benefits:</p>
<ul>
<li>Limits the amount of news and new developments you have to follow</li>
<li>Allows learning terminology quicker becuase you stay within one ecosystem</li>
<li>Certifications tend to build upon each other within a single provider</li>
<li>Gives a sense of direction in learning and development</li>
<li>Improves familiarity with the documentation and tooling</li>
</ul>
<p>You can always broaden your horizon after you have gained a few years with the first provider you chose, if you&rsquo;d like to.</p>
<h2 id="links">Links:</h2>
<p>202306241706</p>
<p><a href="https://www.statista.com/statistics/1056500/distribution-of-popular-cloud-services-in-the-netherlands-by-sector/">https://www.statista.com/statistics/1056500/distribution-of-popular-cloud-services-in-the-netherlands-by-sector/</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: Finishing Pipeline Setup  &amp; Working on KeyVault Template - Azure Kubernetes Lab Series</title>
      <link>https://mischavandenburg.com/zet/video-finish-pipeline-setup-aks-series/</link>
      <pubDate>Fri, 30 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/video-finish-pipeline-setup-aks-series/</guid>
      <description>Finish deploying keyvault using pipeline Get the random name generation to work Lessons Learned Subscriptions need to be registered with resource providers, apparently https://learn.microsoft.com/en-us/azure/azure-resource-manager/troubleshooting/error-register-resource-provider?tabs=azure-cli
acccesPolicies are mandatory on KeyVaults, but not when RBAC is enabled Assign contributor role to Azure DevOps service connection to be able to create resource groups from pipeline Achieved Setting up connection between pipeline and Azure subscription Assign correct rights to the service connection so it is allowed to deploy new resource groups (and other resources) Learned about provider registrations Made progress on creating unique names for resources Successfully deployed new resource group and key vault from the pipeline Next time: Look into random string creation with utcNow</description>
      <content:encoded><![CDATA[
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/eooZ3OHl5Mc" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<ul>
<li>Finish deploying keyvault using pipeline</li>
<li>Get the random name generation to work</li>
</ul>
<h1 id="lessons-learned">Lessons Learned</h1>
<ul>
<li>Subscriptions need to be registered with resource providers, apparently</li>
</ul>
<p><a href="https://learn.microsoft.com/en-us/azure/azure-resource-manager/troubleshooting/error-register-resource-provider?tabs=azure-cli">https://learn.microsoft.com/en-us/azure/azure-resource-manager/troubleshooting/error-register-resource-provider?tabs=azure-cli</a></p>
<ul>
<li>acccesPolicies are mandatory on KeyVaults, but not when RBAC is enabled</li>
<li>Assign contributor role to Azure DevOps service connection to be able to create resource groups from pipeline</li>
</ul>
<h1 id="achieved">Achieved</h1>
<ul>
<li>Setting up connection between pipeline and Azure subscription</li>
<li>Assign correct rights to the service connection so it is allowed to deploy new resource groups (and other resources)</li>
<li>Learned about provider registrations</li>
<li>Made progress on creating unique names for resources</li>
<li>Successfully deployed new resource group and key vault from the pipeline</li>
</ul>
<h1 id="next-time">Next time:</h1>
<p>Look into random string creation with utcNow</p>
<p><a href="https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-functions-date#utcnow">https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-functions-date#utcnow</a></p>
<p>or newGuid</p>
<p><a href="https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-functions-string#newguid">https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-functions-string#newguid</a></p>
<p>Links:</p>
<p>202306281806</p>
<h2 id="links">Links:</h2>
<p>202306302206</p>
<p><a href="https://youtu.be/eooZ3OHl5Mc">https://youtu.be/eooZ3OHl5Mc</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: Setting Up A Simple Azure Pipeline To Deploy A Keyvault</title>
      <link>https://mischavandenburg.com/zet/video-aks-lab-pipeline-first-setup/</link>
      <pubDate>Fri, 30 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/video-aks-lab-pipeline-first-setup/</guid>
      <description>Write KeyVault template Write pipeline code set up Azure DevOps pipeline Lessons Learned Always make sure to use az deployment group instead of az group deployment Because it has older Bicep version and will be deprecated Make sure to be in correct Directory to be able to sync subscriptions for service connection Links: 202306302206
https://youtu.be/WnA8V3uq7P8</description>
      <content:encoded><![CDATA[
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/WnA8V3uq7P8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<ul>
<li>Write KeyVault template</li>
<li>Write pipeline code</li>
<li>set up Azure DevOps pipeline</li>
</ul>
<h1 id="lessons-learned">Lessons Learned</h1>
<ul>
<li>Always make sure to use <code>az deployment group</code> instead of <code>az group deployment</code></li>
<li>Because it has older Bicep version and will be deprecated</li>
<li>Make sure to be in correct Directory to be able to sync subscriptions for service connection</li>
</ul>
<h2 id="links">Links:</h2>
<p>202306302206</p>
<p><a href="https://youtu.be/WnA8V3uq7P8">https://youtu.be/WnA8V3uq7P8</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>What is Azure CNI Overlay for AKS?</title>
      <link>https://mischavandenburg.com/zet/azure-aks-cni-overlay/</link>
      <pubDate>Wed, 14 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/azure-aks-cni-overlay/</guid>
      <description>CNI? CNI stands for Container Network Interface. It allows communication between pods and services.
Current Azure CNI limitations Let&amp;rsquo;s take a practical example. We have an enterprise environment where a large network is utilized, spanning multiple clouds and on-prem infrastructure hubs. To enable seamless communication across these sections, they must belong to the same network. As a result, specific IP address ranges are assigned to each section, with AWS, On-Prem A, and Azure each having their respective ranges.</description>
      <content:encoded><![CDATA[<h1 id="cni">CNI?</h1>
<p>CNI stands for Container Network Interface. It allows communication between pods and services.</p>
<h2 id="current-azure-cni-limitations">Current Azure CNI limitations</h2>
<p>Let&rsquo;s take a practical example. We have an enterprise environment where a large network is utilized, spanning multiple clouds and on-prem infrastructure hubs. To enable seamless communication across these sections, they must belong to the same network. As a result, specific IP address ranges are assigned to each section, with AWS, On-Prem A, and Azure each having their respective ranges.</p>
<p>Let&rsquo;s say Azure is assigned the following ranges:</p>
<p>10.60.0.0/16</p>
<p>10.61.0.0/16</p>
<p>10.62.0.0/16</p>
<p>This means that the networks in each of these ranges would have a maximum possible amount of 65534 addresses per range.</p>
<p>With the current Azure CNI (i.e. the non-overlay version), all pods are assigned an IP address from one of these ranges. It also uses direct VNet routing.  Since the pods use VNet IP&rsquo;s, there is a maximum of 65.000 pods per cluster. In other words, there is a risk for IP exhaustion, which limits the scalability of your workloads. Moreover, pod subnets cannot be shared across clusters.</p>
<p>It is crucial to carefully plan the number of pods you expect to deploy. If the required number of IP addresses exceeds the available addresses in the subnet, you will not be able to run your pods.</p>
<p>Now, these ranges are large and you can anticipate the growth of your resources. For now we are fine. But to design an infrastructure which is truly scalable and extendable, you will need to look into different options. This is where the Azure CNI Overlay comes in.</p>
<h2 id="benefits-of-azure-cni-overlay">Benefits of Azure CNI Overlay</h2>
<p>An Overlay network is an abstracted, virtual network which is put on top of your current network infrastructure. Nodes are assigned IP addresses from the VNets that they are deployed in, but pods get assigned IP addresses from the Overlay network.</p>
<p>Pods are assigned addresses from a private CIDR which is logically separate from the VNet hosting the nodes. They do not use up the IP addressess of the VNets, which means that your workloads become nearly infinitely scalable within your assigned IP address ranges when you are operating in this type of corporate networking infrastructure with IP range limitations. You can scale up to literally thousands of nodes without worrying about IP exhaustion.</p>
<p><img loading="lazy" src="/cnioverlay.png" type="" alt=""  /></p>
<p>Additionally, the Overlay network can also span across multiple AKS clusters. This opens up a whole world of possibilities where pods from separate workloads on separate clusters could communicate with each other directly using the high speed native direct routing of the Azure network.</p>
<h2 id="limitations">Limitations</h2>
<p>Azure CNI Overlay also comes with some limitations. A big one is that you cannot use Application Gateway as an Ingress Controller (AGIC) for an Overlay cluster.</p>
<p>Other notable limitations:</p>
<ul>
<li>Windows support is still in Preview</li>
<li>Virtual Machine Availability Sets (VMAS) are not supported for Overlay</li>
<li>Dualstack networking is not supported in Overlay</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>In conclusion, the Azure CNI Overlay provides a powerful solution to address the challenges of IP exhaustion and scalability in Azure AKS. By implementing the overlay network, organizations can overcome the limitations of the non-overlay version of Azure CNI and achieve a truly scalable and manageable infrastructure.</p>
<p>Azure CNI Overlay is currently in preview for Windows and GA for Linux nodes, but I&rsquo;m very excited about the developments. I&rsquo;ll be following them closely and I hope to be a part of its implementation at my current contract.</p>
<h1 id="links">Links:</h1>
<p><a href="https://learn.microsoft.com/en-us/azure/aks/azure-cni-overlay">https://learn.microsoft.com/en-us/azure/aks/azure-cni-overlay</a></p>
<p><a href="https://www.youtube.com/watch?v=kLBLaCC_dNs">https://www.youtube.com/watch?v=kLBLaCC_dNs</a></p>
<p><a href="https://azure.microsoft.com/en-us/updates/generally-available-azure-cni-overlay-for-linux/">https://azure.microsoft.com/en-us/updates/generally-available-azure-cni-overlay-for-linux/</a></p>
<p>202306131506</p>
<p>[[aks-networking-essentials]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Deploying Simple Applications to AKS with Draft</title>
      <link>https://mischavandenburg.com/zet/deploy-draft-azure/</link>
      <pubDate>Fri, 09 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/deploy-draft-azure/</guid>
      <description>Not sure how this will play out with more complex applications, but I can definitely see how this would accellerate the process for developers to get their first versions deployed without the toil of setting up manifests and pipelines. Will definitely play around with this soon and I&amp;rsquo;m curious to see how far Microsoft will take this!
Links: 202306092006</description>
      <content:encoded><![CDATA[<p>Not sure how this will play out with more complex applications, but I can definitely see how this would accellerate the process for developers to get their first versions deployed without the toil of setting up manifests and pipelines. Will definitely play around with this soon and I&rsquo;m curious to see how far Microsoft will take this!</p>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/PqhdX8-SZYw" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<h2 id="links">Links:</h2>
<p>202306092006</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>The Ups And Downs Of A Devops Engineer</title>
      <link>https://mischavandenburg.com/zet/ups-and-downs-of-devops-engineer/</link>
      <pubDate>Thu, 04 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/ups-and-downs-of-devops-engineer/</guid>
      <description>Next winter I&amp;rsquo;ll be entering my third year as a DevOps Engineer. When you first break into this field there is an overwhelming amount of things to learn. Frankly, this will always be the case. But I&amp;rsquo;m reaching a point where I have gained experience with most of the main areas and tooling, and I can start seeing the relations between them and how they compare to one another.
Just like any other job or activity, there are things that you like and that suit you well, and there are things that you don&amp;rsquo;t like.</description>
      <content:encoded><![CDATA[<p>Next winter I&rsquo;ll be entering my third year as a DevOps Engineer. When you first break into this field there is an overwhelming amount of things to learn. Frankly, this will always be the case. But I&rsquo;m reaching a point where I have gained experience with most of the main areas and tooling, and I can start seeing the relations between them and how they compare to one another.</p>
<p>Just like any other job or activity, there are things that you like and that suit you well, and there are things that you don&rsquo;t like. And now the first phases of overwhelm are confidently behind me, I&rsquo;m starting to learn the things I prefer doing over others.</p>
<p>Being a DevOps Engineer does not mean you get to write code all day. I&rsquo;m currently working in an organization where we&rsquo;re not working fully DevOps yet. Rather than deploying many times in small increments, we are still using patching windows. Managing infrastructure in this way means you&rsquo;ll need to communicate with teams to schedule the patching and submit changes to the approval board. It can involve a lot of emailing and talking back and forth. And sometimes things go wrong and everything needs to be rescheduled again. Although it is valuable experience, and I certainly have the communication skills to handle these kinds of situations, I&rsquo;m learning that I lean more towards the dev side of things.</p>
<p>Today was a fun day at work. I&rsquo;ve been working on a PR for a while whenever I wasn&rsquo;t required to work on tickets. The PR involved refactoring a Bicep template for our Azure Key Vault deployments, which were using parameter files with a lot of repeated code. After getting a lot of good feedback on my code changes, and submitting it for review multiple times, it was finally approved and I could get to work.</p>
<p>I used bash, jq and vim to clean up the parameter files for each workload and submitted PRs. My colleague, who helped me with the PR&rsquo;s, was standing by and ready to approve them as soon as I submitted them. Some pipelines ran into some trouble and required a few more PR&rsquo;s to fix, and there was a bit of pressure because we needed to get the pipelines working before any other changes needed to be deployed the next day.</p>
<p>By working together like this we managed to clean up 2850 lines of code in one day and it was a great feeling to get this job done. I had been working on this for a couple of weeks and it&rsquo;s so satisfying to move that ticket status to &ldquo;done&rdquo;.</p>
<p>I really enjoy the coding! And I really enjoy working with Infrastructure as Code! Somehow it really gives me a lot of pleasure knowing that the code I&rsquo;m writing will be transformed to running infrastructure after it is submitted. I love writing programs in my free time as well, but there is just something magical seeing your parameter file being turned into a functional Kubernetes cluster or Key Vault.</p>
<p>Working as a DevOps Engineer, I&rsquo;m generally having so much fun. I really enjoyed myself when I was learning jq and used scripts to change hundreds of lines of code at a time and submitting them as a PR. And some days, the work is less enjoyable, but it is all part of the job. It all serves to deliver value to the end customer, and if I can have fun most of the time while delivering value, I know I&rsquo;m in the right place.</p>
<p>In any case, there hasn&rsquo;t been a second during these past two years where I regretted my choice of becoming a DevOps Engineer. I love my job, I found my passion in my work, and I&rsquo;m so excited for the future. Cloud computing is never standing still, Kubernetes is starting to become more and more popular, and my skills grow with every day that passes.</p>
<p>I&rsquo;m a happy camper.</p>
<h2 id="links">Links:</h2>
<p>202305042005</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>The Difference Between DevOps, Cloud and Cloud Native</title>
      <link>https://mischavandenburg.com/zet/cloud-cloudnative-devops/</link>
      <pubDate>Sun, 26 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/cloud-cloudnative-devops/</guid>
      <description>I found an excellent video by Rob Muhlenstein explaining the differences between Cloud, Cloud Native and DevOps. Here are the notes I wrote.
Cloud These are primarily cloud services. The external cloud.
&amp;ldquo;Something as a Service&amp;rdquo;.
Amazon Azure GCP Cloud Native This is Cloud Native: The CNCF Landscape
Cloud Native is the technology that makes the cloud possible, and all the technology dependent on those services.
Computing Edge Computing
High Performance Computing</description>
      <content:encoded><![CDATA[<p>I found <a href="https://youtu.be/gyjRriOyw-k">an excellent video</a> by Rob Muhlenstein explaining the differences between Cloud, Cloud Native and DevOps. Here are the notes I wrote.</p>
<h1 id="cloud">Cloud</h1>
<p>These are primarily <em>cloud services</em>. The external cloud.</p>
<p>&ldquo;Something as a Service&rdquo;.</p>
<ul>
<li>Amazon</li>
<li>Azure</li>
<li>GCP</li>
</ul>
<h1 id="cloud-native">Cloud Native</h1>
<p>This is Cloud Native: <a href="https://landscape.cncf.io/">The CNCF Landscape</a></p>
<p>Cloud Native is the technology that makes the cloud possible, and all the technology dependent on those services.</p>
<h2 id="computing">Computing</h2>
<ul>
<li>
<p>Edge Computing</p>
</li>
<li>
<p>High Performance Computing</p>
</li>
<li>
<p>Encapsulates all of the technologies that are involved with containerization of work, jobs and nodes</p>
</li>
<li>
<p>Deployment of compute resources as nodes</p>
</li>
<li>
<p>This is why Google&rsquo;s Borg was called Borg</p>
</li>
<li>
<p>Computers are drones of a larger collective</p>
</li>
<li>
<p>Every node puts all the resources into the collective.</p>
</li>
</ul>
<p>The collective is all the nodes combined, and Kubernetes is the Borg that orchestrates everything. It sees available resources and allocates the work that needs to be done.</p>
<p>Borg is the internal system developed at Google to run their infrastructure. You can read about it in the <a href="https://sre.google/books/">Site Reliability Engineering</a> books and I highly recommend them.</p>
<blockquote>
<p>Kubernetes is /proc for the cloud</p>
</blockquote>
<blockquote>
<p>Rob Muhlenstein</p>
</blockquote>
<h2 id="most-important-technologies">Most Important Technologies</h2>
<ul>
<li>
<p>Docker, Dockerfiles</p>
</li>
<li>
<p>Kubernetes</p>
</li>
<li>
<p>Helm</p>
</li>
<li>
<p>Harbor</p>
</li>
<li>
<p>Different registries, harbor, quay</p>
</li>
<li>
<p>It is a lot of Python and POSIX shell</p>
</li>
<li>
<p>Go for infrastructure application development</p>
</li>
<li>
<p>Kubernetes and Helm have won the game</p>
</li>
</ul>
<h2 id="containers-size-matters">Containers: Size Matters</h2>
<ul>
<li>Size matters (again) in the cloud</li>
<li>The smaller your container the better, because it takes less resources and less costs</li>
</ul>
<h1 id="devops">DevOps</h1>
<p>DevOps is not the same as Cloud Native. It is one piece of it, a specific set of practices and actions that can be done within Cloud Native.</p>
<ul>
<li>How you write software and release it</li>
<li>CI/CD</li>
<li>Focused on getting the software out</li>
<li>GitLab has become the one stop shop</li>
<li>Purpose is to write software and get it published fast</li>
<li>GitOps</li>
</ul>
<h1 id="summary">Summary</h1>
<p>In summary, &ldquo;cloud&rdquo; stands for the services offered by cloud providers such as AWS, Azure and GCP. Cloud Native stands for all of the technology that makes these cloud services possible. DevOps is part of Cloud Native, but definitely not the same thing. DevOps is concerned with how software is written and released.</p>
<h1 id="links">Links:</h1>
<p>202303262003</p>
<p><a href="https://youtu.be/gyjRriOyw-k">https://youtu.be/gyjRriOyw-k</a></p>
<p><a href="https://landscape.cncf.io/">https://landscape.cncf.io/</a></p>
<p><a href="https://sre.google/books/">https://sre.google/books/</a></p>
<p>[[rwxrob]]</p>
<p>[[devops]]</p>
<p>[[Kubernetes]]</p>
<p>[[Cloud Native]]</p>
<p>[[cncf]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Running Docker and Kubernetes on Mac M2</title>
      <link>https://mischavandenburg.com/zet/docker-kubernetes-on-mac-m2/</link>
      <pubDate>Sat, 18 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/docker-kubernetes-on-mac-m2/</guid>
      <description>The past few days I&amp;rsquo;ve been trying out a few options to run Docker containers and a Kubernetes clusters on my new MacBook Pro M2.
Unfortunately you can&amp;rsquo;t just run brew install docker and expect it to work. Additionally, Docker desktop requires that you purchase a license if you use it for work purposes.
Minikube works fine as well, but the networking driver for qemu is not fully supported yet, and I haven&amp;rsquo;t tried any of the other alternatives because I found something better.</description>
      <content:encoded><![CDATA[<p>The past few days I&rsquo;ve been trying out a few options to run Docker containers and a Kubernetes clusters on my new MacBook Pro M2.</p>
<p>Unfortunately you can&rsquo;t just run <code>brew install docker</code> and expect it to work. Additionally, Docker desktop requires that you purchase a license if you use it for work purposes.</p>
<p>Minikube works fine as well, but the networking driver for qemu is not fully supported yet, and I haven&rsquo;t tried any of the other alternatives because I found something better.</p>
<p>Rancher Desktop provides everything that you need. It sets up a local VM where it will run a Kubernetes cluster using k3s. It will configure the containerd container engine for you which you can interact with using <code>nerdctl</code>.</p>
<p>To install:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">brew install rancher
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#after installing rancher, start it up and wait for it to boot the VM.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">alias</span> <span class="nv">docker</span><span class="o">=</span>nerdctl
</span></span><span class="line"><span class="cl">docker run hello-world
</span></span></code></pre></div><p>And you&rsquo;re good to go. Rancher will add the rancher-desktop to your kube context.</p>
<p>To test your Kubernetes cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">k get pods
</span></span><span class="line"><span class="cl">k get nodes
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># test running a pod</span>
</span></span><span class="line"><span class="cl">k run nginx --image<span class="o">=</span>nginx
</span></span><span class="line"><span class="cl">k expose pod nginx --port<span class="o">=</span><span class="m">80</span> --type<span class="o">=</span>NodePort
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># inspect your services and look for 80:31066/TCP under PORT(S)</span>
</span></span><span class="line"><span class="cl">k get svc
</span></span><span class="line"><span class="cl">curl localhost:31066
</span></span></code></pre></div><p>Or visit localhost:31066 in your browser. Replace 31066 with the port you found listed under your services.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Azure DevOps Personal Access Tokens are always for authenticating into ADO</title>
      <link>https://mischavandenburg.com/zet/azure-personal-access-tokens/</link>
      <pubDate>Fri, 13 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/azure-personal-access-tokens/</guid>
      <description>The PAT (Personal Access Token) often comes up during practice tests for the AZ-400.
One way to remember when to use a PAT is that these are only for authenticating into Azure DevOps, never to external services.
For example, you might get a question on connecting your Azure DevOps project with a GitHub account from Azure DevOps, and PAT will show up as one of the alternative answers. By remembering that PATs are only for authenticating into ADO, you can elminate this alternative, and make your choice easier.</description>
      <content:encoded><![CDATA[<p>The PAT (Personal Access Token) often comes up during practice tests for the AZ-400.</p>
<p>One way to remember when to use a PAT is that these are only for authenticating <strong>into</strong> Azure DevOps, never to external services.</p>
<p>For example, you might get a question on connecting your Azure DevOps project with a GitHub account from Azure DevOps, and PAT will show up as one of the alternative answers. By remembering that PATs are only for authenticating into ADO, you can elminate this alternative, and make your choice easier.</p>
<p>Personal Access Tokens are an alternative to passwords but should be treated in exactly the same way.</p>
<p><a href="https://learn.microsoft.com/en-us/azure/devops/organizations/accounts/use-personal-access-tokens-to-authenticate?view=azure-devops&amp;tabs=Windows">https://learn.microsoft.com/en-us/azure/devops/organizations/accounts/use-personal-access-tokens-to-authenticate?view=azure-devops&amp;tabs=Windows</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>How to build and deploy a Docker container to an Azure VM using Azure Pipelines</title>
      <link>https://mischavandenburg.com/zet/docker-to-azure-vm/</link>
      <pubDate>Sun, 08 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/docker-to-azure-vm/</guid>
      <description>I wanted to build an application from a Dockerfile and deploy it to a VM. I used a default Svelte setup as an example app.
Naturally, Azure prefers that you deploy containers to services such as Azure Container Instances or App Services, so they don&amp;rsquo;t provide modules for the pipelines to deploy to docker servers as far as I could tell.
I searched for a long time but I could not find a solution.</description>
      <content:encoded><![CDATA[<p>I wanted to build an application from a Dockerfile and deploy it to a VM. I used a default Svelte setup as an example app.</p>
<p>Naturally, Azure prefers that you deploy containers to services such as Azure Container Instances or App Services, so they don&rsquo;t provide modules for the pipelines to deploy to docker servers as far as I could tell.</p>
<p>I searched for a long time but I could not find a solution. In the end I just ran shell commands from the pipeline to run the container on on the server.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">steps</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">script</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">      sudo docker stop svelte-test
</span></span></span><span class="line"><span class="cl"><span class="sd">      sudo docker rm svelte-test
</span></span></span><span class="line"><span class="cl"><span class="sd">      sudo docker run --name svelte-test -p 8080:80 -d mischavandenburg/svelte-test:$(Build.BuildId)</span><span class="w">      
</span></span></span></code></pre></div><p>You can find the full pipeline code, the app and Dockerfile in my lab repo:</p>
<p><a href="https://github.com/mischavandenburg/lab/tree/main/azure-pipelines/docker-to-azure-vm">https://github.com/mischavandenburg/lab/tree/main/azure-pipelines/docker-to-azure-vm</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>How to deploy to a Linux VM in Azure with Azure Pipelines</title>
      <link>https://mischavandenburg.com/zet/azure-pipelines-deploy-vm/</link>
      <pubDate>Sun, 08 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/azure-pipelines-deploy-vm/</guid>
      <description>To reach a VM from Azure Pipelines, you need to set up an environment.
Create your Linux VM in Azure.
In Azure DevOps, click envirnoments, new, and select the Virtual Machine option.
A command is generated for you. SSH into your VM and run the command.
Now the VM should show up under environments in Azure DevOps.
Set up a repo with an azure-pipelines.yml with these contents to test. under environment, set the same name as you did in Azure DevOps for your environment.</description>
      <content:encoded><![CDATA[<p>To reach a VM from Azure Pipelines, you need to set up an environment.</p>
<p>Create your Linux VM in Azure.</p>
<p>In Azure DevOps, click envirnoments, new, and select the Virtual Machine option.</p>
<p><img loading="lazy" src="/env1.png" type="" alt=""  /></p>
<p>A command is generated for you. SSH into your VM and run the command.</p>
<p><img loading="lazy" src="/env2.png" type="" alt=""  /></p>
<p>Now the VM should show up under environments in Azure DevOps.</p>
<p>Set up a repo with an azure-pipelines.yml with these contents to test. under <code>environment</code>, set the same name as you did in Azure DevOps for your environment.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">trigger</span><span class="p">:</span><span class="w"> 
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="l">main</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">pool</span><span class="p">:</span><span class="w"> 
</span></span></span><span class="line"><span class="cl"><span class="w">   </span><span class="nt">vmImage</span><span class="p">:</span><span class="w"> </span><span class="l">ubuntu-latest</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">jobs</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">deployment</span><span class="p">:</span><span class="w"> </span><span class="l">VMDeploy</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">displayName</span><span class="p">:</span><span class="w"> </span><span class="l">Deploy to VM</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">environment</span><span class="p">:</span><span class="w"> 
</span></span></span><span class="line"><span class="cl"><span class="w">   </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">dev</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">   </span><span class="nt">resourceType</span><span class="p">:</span><span class="w"> </span><span class="l">VirtualMachine</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">strategy</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">     </span><span class="nt">runOnce</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">deploy</span><span class="p">:</span><span class="w">   
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">steps</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span>- <span class="nt">script</span><span class="p">:</span><span class="w"> </span><span class="l">echo &#34;Hello world&#34;</span><span class="w">
</span></span></span></code></pre></div><p>You can see it when the deploy runs on the VM:</p>
<p><img loading="lazy" src="/env3.png" type="" alt=""  /></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Deploying a Linux VM to Azure with Terraform</title>
      <link>https://mischavandenburg.com/zet/terraform-linux-vm/</link>
      <pubDate>Sat, 07 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/terraform-linux-vm/</guid>
      <description>For a project I&amp;rsquo;m setting up my environment with Terraform.
I used this tutorial, but modified the code to make it simpler and easier to understand for beginners. The original uses a random module to generate random names, and generates a new SSH key. Also, this tutorial uses expensive VM tiers and Premium storage, which are not necessary when you are learning.
I also thought the SSH configuration was overcomplicated. My version just takes an SSH keypair stored at ~/.</description>
      <content:encoded><![CDATA[<p>For a project I&rsquo;m setting up my environment with Terraform.</p>
<p>I used <a href="https://learn.microsoft.com/en-us/azure/virtual-machines/linux/quick-create-terraform">this tutorial</a>, but modified the code to make it simpler and easier to understand for beginners. The original uses a random module to generate random names, and generates a new SSH key. Also, this tutorial uses expensive VM tiers and Premium storage, which are not necessary when you are learning.</p>
<p>I also thought the SSH configuration was overcomplicated. My version just takes an SSH keypair stored at <code>~/.ssh/id_rsa.pub</code></p>
<p>To run:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-hcl" data-lang="hcl"><span class="line"><span class="cl"><span class="k">terraform</span> <span class="k">init</span>
</span></span><span class="line"><span class="cl"><span class="k">terraform</span> <span class="k">plan</span>
</span></span><span class="line"><span class="cl"><span class="k">terraform</span> <span class="k">apply</span>
</span></span></code></pre></div><p>The scripts prints the public IP of the newly created VM. You should be able to SSH to it:</p>
<p><code>ssh azureuser@the_printed_ip_address</code></p>
<p>You can find the code in <a href="https://github.com/mischavandenburg/lab/tree/main/terraform/azure-simple-linux-vm">my &ldquo;lab&rdquo; repo on GitHub.</a></p>
<p><a href="https://github.com/mischavandenburg/lab/tree/main/terraform/azure-simple-linux-vm">https://github.com/mischavandenburg/lab/tree/main/terraform/azure-simple-linux-vm</a></p>
<p><a href="https://learn.microsoft.com/en-us/azure/virtual-machines/linux/quick-create-terraform">https://learn.microsoft.com/en-us/azure/virtual-machines/linux/quick-create-terraform</a></p>
<p><a href="https://learn.microsoft.com/en-us/azure/developer/terraform/authenticate-to-azure?source=recommendations&amp;tabs=bash#authenticate-to-azure-via-a-microsoft-account">https://learn.microsoft.com/en-us/azure/developer/terraform/authenticate-to-azure?source=recommendations&amp;tabs=bash#authenticate-to-azure-via-a-microsoft-account</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Starting a Project</title>
      <link>https://mischavandenburg.com/zet/starting-a-project/</link>
      <pubDate>Mon, 02 Jan 2023 21:00:11 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/starting-a-project/</guid>
      <description>I&amp;rsquo;m starting a project with a friend. Developing an application. We make a good team, he&amp;rsquo;s great at coding and knows the backend too.
He&amp;rsquo;ll do the development, I&amp;rsquo;m in charge of hosting. We&amp;rsquo;re setting everything up in Azure DevOps, so it is a great way to practice my Azure skills and apply the things I&amp;rsquo;ve learned in my recently obtained AZ-104 Azure Administrator certification.
Even though it is a small scale hobby project, I still plan to approach it as if it was an enterprise production application.</description>
      <content:encoded><![CDATA[<p>I&rsquo;m starting a project with a friend. Developing an application. We make a good team, he&rsquo;s great at coding and knows the backend too.</p>
<p>He&rsquo;ll do the development, I&rsquo;m in charge of hosting. We&rsquo;re setting everything up in Azure DevOps, so it is a great way to practice my Azure skills and apply the things I&rsquo;ve learned in my recently obtained <a href="/zet/articles/az-104-study-guide/">AZ-104</a> Azure Administrator certification.</p>
<p>Even though it is a small scale hobby project, I still plan to approach it as if it was an enterprise production application. I&rsquo;ll set up a full CI/CD pipeline with testing in a secure manner. Credentials stored in an Azure key vault and images pushed to a private registry.</p>
<p>This is going to be fun!</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>What Are Containers?</title>
      <link>https://mischavandenburg.com/zet/articles/what-are-containers/</link>
      <pubDate>Sun, 01 Jan 2023 16:17:58 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/what-are-containers/</guid>
      <description>When you learn about DevOps, you will come across the concept of a container early on. This is a &amp;ldquo;Mischa Explains&amp;rdquo; article where I attempt to explain a concept in my own words as simply as possible. I use the Feynman technique and pretend to explain it to a 12-year-old.
Virtualization To understand containers, we need to understand virtualization. Virtualization is the process of creating &amp;ldquo;fake computers&amp;rdquo; or &amp;ldquo;virtual computers&amp;rdquo; on a physical computer.</description>
      <content:encoded><![CDATA[<p>When you learn about DevOps, you will come across the concept of a container early on. This is a &ldquo;Mischa Explains&rdquo; article where I attempt to explain a concept in my own words as simply as possible. I use the Feynman technique and pretend to explain it to a 12-year-old.</p>
<h1 id="virtualization">Virtualization</h1>
<p>To understand containers, we need to understand virtualization. Virtualization is the process of creating &ldquo;fake computers&rdquo; or &ldquo;virtual computers&rdquo; on a physical computer.</p>
<p>On your desk, you have a laptop or a desktop PC. This machine has hardware such as a motherboard, a hard disk, and a video card. To run programs on your computer, it needs an Operating System. Usually, Windows, macOS, or Linux are used.</p>
<p>Let&rsquo;s say you have a computer running Windows, but you want to run a program that can only run on Linux. One solution is to buy another laptop and put it beside your Windows laptop on your desk. So now you have two computers with two different operating systems.</p>
<p>Fortunately, there are other solutions. We can use virtualization to make a Virtual Machine. A virtual machine is created by software to imitate a fully functional running computer inside your current operating system. You can create a virtual machine that runs Linux on your Windows computer. Your Windows computer running the Linux virtual machine is known as the **host.</p>
<p>Now you don&rsquo;t need to buy another computer to run your Linux program. Instead, you can boot up your Linux virtual machine and run your program when needed. If you have a powerful computer, you could run ten or more virtual machines, each of which has its own operating system and custom environment.</p>
<h1 id="containers">Containers</h1>
<p>Every time you create a virtual machine, the virtual machine needs a complete operating system to work. So, first, the software creates a virtual processor, virtual video card, and a virtual network interface. Then, it runs a fully functional operating system on that virtual hardware. This takes up a lot of resources.</p>
<p>Containers are lightweight packages of software. They are designed to do a very specific task, and therefore they only contain the resources they need to do that task. Nothing more.</p>
<p>Containers use the operating system of the physical computer to run. They have a very minimal, lightweight operating system inside them, but it only contains the elements they need to do their specific task. Therefore, containers are very easy to distribute, and you can run them very quickly.</p>
<h1 id="containers-are-like-newspapers">Containers are like newspapers</h1>
<p>Containers are like newspapers. Newspapers have a particular task: providing you with the day&rsquo;s news. You cannot use newspapers to study for your mathematics exam. You use your math book to study for your math exam. If you want to be informed of the day&rsquo;s news, you use a newspaper. This is what I mean by containers having a specific task.</p>
<p>Next, newspapers are printed on a specific kind of paper. When you buy an expensive book, it will have a sturdy and durable cover, and the pages are made of nice thick paper that will last a long time. The pages don&rsquo;t tear very quickly, and when the book gets wet, it can withstand it. This thick cover and high-quality papers are like the operating system of a virtual machine.</p>
<p>Newspapers, on the other hand, are printed on very thin paper. Because they are designed to distribute the news to you effectively, newspapers do not need to be stored forever or do any other tasks. If you used thick, expensive paper for newspapers, they would become costly, and no one would buy them anymore. The paper is optimized to bring the news to you.</p>
<p>In the same way, the container only comes with the components it needs to do its specific task. Therefore, the container is optimized for its purpose. As a result, they can be distributed more quickly and do not take up a lot of resources when running.</p>
<p>There are other benefits to containers, such as improving the ability to autoscale your application, but I will expand on those in a future blog post.</p>
<h1 id="further-study">Further study</h1>
<p>To learn more about containers, you can use the following resources:</p>
<p><a href="https://youtu.be/r6YIlPEC4y4">Containers &amp; Friends from John Savill&rsquo;s DevOps Masterclass</a></p>
<p><a href="https://docs.docker.com/get-started/overview/">Docker Documentation</a></p>
<p><a href="https://youtu.be/3c-iBn73dDE">Docker Tutorial for Beginners</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Study Guide: AZ-104 Azure Administrator Associate</title>
      <link>https://mischavandenburg.com/zet/articles/az-104-study-guide/</link>
      <pubDate>Fri, 30 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/az-104-study-guide/</guid>
      <description>TLDR It took me 80 hours of studying to gain this certification. Here are my notes and Anki deck: GitHub repo
Introduction When I did my English degree at University, exams were usually handwritten essays that needed to be done within a 3-hour timeframe. Sometimes we had multiple-choice tests, and I would always consider them a guaranteed pass because multiple-choice was much easier in my experience.
That opinion has changed since I&amp;rsquo;ve obtained a few IT certifications.</description>
      <content:encoded><![CDATA[<h1 id="tldr">TLDR</h1>
<p>It took me 80 hours of studying to gain this certification. Here are my notes and Anki deck:
<a href="https://github.com/mischavandenburg/az-104-azure-administrator">GitHub repo</a></p>
<h1 id="introduction">Introduction</h1>
<p>When I did my English degree at University, exams were usually handwritten essays that needed to be done within a 3-hour timeframe. Sometimes we had multiple-choice tests, and I would always consider them a guaranteed pass because multiple-choice was much easier in my experience.</p>
<p>That opinion has changed since I&rsquo;ve obtained a few IT certifications. These tests are hard! I&rsquo;m typing this while sitting on the bus on my way home from my AZ-104 exam. I passed with an 860 score of 1000, where 700 or higher is a pass. But it was an astonishingly tough exam. Usually, I finish quickly and spend at least half an hour reviewing my answers. I had only 5 minutes to review my questions this time because I had used up all of the available time. The questions required intense concentration and were time-consuming because I needed to compare many options which were very similar to each other. There were no easy questions.</p>
<h1 id="preparation">Preparation</h1>
<p>I studied 80 hours for this exam in a month. I work full-time as a DevOps Engineer, so I study in the evenings and on weekends. I have my Azure Fundamentals and CKA, but I only work with Azure occasionally in my current role.</p>
<p>Here is what I did to prepare for my exam:</p>
<ul>
<li>Go through all of the <a href="https://learn.microsoft.com/en-us/certifications/exams/az-104">Microsoft Learn modules for the AZ-104</a></li>
<li>Watch the entire <a href="https://www.youtube.com/playlist?list=PLlVtbbG169nGlGPWs9xaLKT1KfwqREHbs">AZ-104 study list by John Savill</a></li>
<li>Practice exams on <a href="https://tutorialsdojo.com/">TutorialsDojo</a> until I could pass them with 90%+ scores</li>
<li>Microsoft ESI practice exams</li>
<li><a href="https://learn.microsoft.com/en-us/shows/exam-readiness-zone/preparing-for-az-104-manage-azure-identities-and-governance-1-of-5">Microsoft AZ-104 Exam prep videos</a></li>
</ul>
<h2 id="microsoft-learn">Microsoft Learn</h2>
<p>You really need to master all of the subject matter. Only completing the Microsoft Learn modules is not enough preparation. They are more like summaries. At the end of each module, they provide links to the documentation for the subject for further study. Unfortunately, Microsoft does not go easy on you. It expects you to know obscure details of nearly every service this exam covers. Therefore, I advise going beyond the Microsoft Learn modules and studying the linked articles after each module.</p>
<h2 id="youtube-az-104-study-playlist-by-john-savill">YouTube AZ-104 Study Playlist by John Savill</h2>
<p>I&rsquo;m not sure if it&rsquo;s better to watch this playlist first and then do the Microsoft modules or the other way around. I did the Microsoft modules first, but for my next exam (AZ-400 DevOps Expert), I&rsquo;ll start with the videos and then do the Microsoft Learn modules.</p>
<h2 id="tutorialsdojo">Tutorialsdojo</h2>
<p>These practice exams are excellent. I used them in preparation for my fundamentals exam.</p>
<p>The best thing about them is that they provide extensive documentation and explanation of the questions. So after you finish the exam, you can study a lot with these examples.</p>
<h2 id="esi-practice-exams">ESI Practice Exams</h2>
<p>You&rsquo;re lucky if your organization participates in Microsoft&rsquo;s <a href="esi.microsoft.com/">Enterprise Skills Initiative</a>. The practice exams provided in the ESI environment give you a good indication of what you can expect at the exam. I first did the Tutorialsdojo exams and then moved on to the ESI exams, and I was humiliated. The ESI questions are very complex and hard to solve, and I learned a lot from these exams.</p>
<p>There are 210 questions total, and I worked through all of them, and whenever I failed a question, I did a deeper dive into the question&rsquo;s theme.</p>
<h1 id="studying">studying</h1>
<p>I take notes in Obsidian, and I use Anki for spaced repetition. I highly recommend keeping a deck of Anki cards and continuously testing yourself. You will need to memorize a lot of details. For example, you are expected to remember that storage accounts of the FileStorage type do not support Geo Redundant Storage. You can find my Anki deck in the <a href="https://github.com/mischavandenburg/az-104-azure-administrator">GitHub repo.</a></p>
<h1 id="links">links</h1>
<p><a href="https://learn.microsoft.com/en-us/certifications/exams/az-104">AZ-104 Exam page with learning modules</a></p>
<p><a href="https://www.youtube.com/playlist?list=PLlVtbbG169nGlGPWs9xaLKT1KfwqREHbs">John Savill&rsquo;s AZ-104 Study playlist</a></p>
<p><a href="https://github.com/mischavandenburg/az-104-azure-administrator">GitHub repo containing my notes and Anki deck</a></p>
<p><a href="https://learn.microsoft.com/en-us/shows/exam-readiness-zone/preparing-for-az-104-manage-azure-identities-and-governance-1-of-5">Microsoft AZ-104 Exam prep videos</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Lab Project: GitOps with ArgoCD, Azure Pipelines and Minikube</title>
      <link>https://mischavandenburg.com/zet/articles/lab-argocd-azure-pipelines/</link>
      <pubDate>Sat, 24 Dec 2022 13:49:55 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/lab-argocd-azure-pipelines/</guid>
      <description>This weekend I had a lot of fun with a project. I wanted to learn more about GitOps and try out ArgoCD.
My goal was to be able to deploy an application from a GitHub repo to my local Kubernetes cluster running in minikube. There are many options I could have used, such as running Jenkins in my cluster. But I wanted to use Azure pipelines for practice, which complicates the deployment to my local cluster, because the cluster is not running on Azure.</description>
      <content:encoded><![CDATA[<p>This weekend I had a lot of fun with a project. I wanted to learn more about GitOps and try out ArgoCD.</p>
<p>My goal was to be able to deploy an application from a GitHub repo to my local Kubernetes cluster running in minikube. There are many options I could have used, such as running Jenkins in my cluster. But I wanted to use Azure pipelines for practice, which complicates the deployment to my local cluster, because the cluster is not running on Azure. I also wanted to try out ArgoCD and learn more about GitOps.</p>
<p>The application is a simple web app that I wrote which displays a quote in the browser:</p>
<p><img loading="lazy" src="/app.png" type="" alt=""  /></p>
<h1 id="gitops-and-structure">GitOps and Structure</h1>
<p>GitOps is used to automate the process of provisioning infrastructure. Infrastructure as code is used to generate the same environment every time the environment is deployed.</p>
<p>For my project I have two separate GitHub repos. <a href="https://github.com/mischavandenburg/static-quote-app">The first repo</a> contains the code for a simple web app I created and the Dockerfile to generate the image. I call this my application repo. The other repo is <a href="https://github.com/mischavandenburg/static-quote-app-gitops">my GitOps repo</a> which contains the manifest files to deploy the application in Kubernetes. I decided to leverage Helm to create my manifest files. This way I can create templates and define my desired values in a values.yaml file in the repo.</p>
<p>Ultimately my goal was to use an Azure pipeline to build an image from my application repo and push it to Docker hub. This new image is given a new tag which needs to be stored. The first pipeline should trigger a new pipeline that makes a pull request to the GitOps repo to update the tag in my Helm chart.</p>
<p>ArgoCD will then scan the GitOps repo and realize that the tag has been updated, and deploy the new tag to my cluster.</p>
<h1 id="minikube">Minikube</h1>
<p>I used <a href="https://minikube.sigs.k8s.io/docs/">minikube</a> to deploy my local Kubernetes cluster. Another option is <a href="https://kind.sigs.k8s.io/">kind</a> (Kubernetes In Docker) but I wanted to use a VM approach this time.</p>
<h1 id="argocd">ArgoCD</h1>
<p><a href="https://argo-cd.readthedocs.io/">ArgoCD</a> is a declarative GitOps continuous delivery tool for Kubernetes. This is the solution I used to continuously scan my GitOps repo. When ArgoCD detects a change in the desired state, it will compare it with the state in my running cluster and make changes accordingly. I found a really good <a href="https://redhat-scholars.github.io/argocd-tutorial/argocd-tutorial/02-getting_started.html">tutorial to run ArgoCD in minikube</a>.</p>
<p><img loading="lazy" src="/argocd-dashboard.png" type="" alt="ArgoCD dashboard"  /></p>
<h1 id="azure-pipelines">Azure Pipelines</h1>
<p>With my cluster running on my local machine and my repos set up, I needed to use Azure Pipelines to bring it all together. Building the image and pushing it to Docker Hub wasn&rsquo;t a big deal. But I had two big challenges in my desired setup: I needed to pass the new tag number to a new pipeline, and I needed to use Azure Pipelines to create a new PR to my GitOps repo.</p>
<h3 id="passing-a-value-from-one-pipeline-to-another">Passing a value from one pipeline to another</h3>
<p>Interestingly, this wasn&rsquo;t as easy as it sounds, and from my internet searching it seemed that many people struggled with this. I decided to use the Variable Groups in Azure DevOps. However, after I finished writing my pipeline, I discovered I had no problems with reading the value from the Variable Groups, but it was impossible to update it using existing pipeline tasks. So I had to a bit of hacking to make it work. In the end I had to use the Azure CLI from within the pipeline to update my variable:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">stage</span><span class="p">:</span><span class="w"> </span><span class="l">update_tag</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">jobs</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">job</span><span class="p">:</span><span class="w"> </span><span class="l">update_tag_variable </span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">displayName</span><span class="p">:</span><span class="w"> </span><span class="l">Update Tag Variable</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">steps</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">bash</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">          az pipelines variable-group variable \
</span></span></span><span class="line"><span class="cl"><span class="sd">          update --group-id 202 \
</span></span></span><span class="line"><span class="cl"><span class="sd">          --org $(System.CollectionUri) \
</span></span></span><span class="line"><span class="cl"><span class="sd">          --project $(System.TeamProject) \
</span></span></span><span class="line"><span class="cl"><span class="sd">          --name tag --value $(Build.BuildId)</span><span class="w">          
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">env</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">AZURE_DEVOPS_EXT_PAT</span><span class="p">:</span><span class="w"> </span><span class="l">$(System.AccessToken)</span><span class="w">
</span></span></span></code></pre></div><p>This didn&rsquo;t feel like a very elegant solution, but it was the only solution I could come up with.</p>
<p>I also struggled a lot with permissions. I needed to find the correct service principal to assign the administrator rights to. <a href="https://stackoverflow.com/questions/52986076/having-no-permission-for-updating-variable-group-via-azure-devops-rest-api-from">This post</a> really helped to solve my problem.</p>
<h3 id="submitting-a-pr-to-a-github-repo">Submitting a PR to a GitHub repo</h3>
<p>When I started writing my pipeline I thought it would be very straightforward to just submit a PR to a repo, but I quickly discovered that this is not natively supported in Azure pipelines yet. In fact, I could not find a way to submit a PR at all. I had to settle for a solution that checks out the GitOps repo and creates a new branch. This new branch updates the tag in the values.yaml with the new tag that was passed from the previous pipeline.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">variables</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="l">mischa-quote</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">passed_tag</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l">$[variables.tag]</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">branch_name</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;pipeline-$(passed_tag)&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">pool</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">vmImage</span><span class="p">:</span><span class="w"> </span><span class="l">ubuntu-latest</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">steps</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">checkout</span><span class="p">:</span><span class="w"> </span><span class="l">self</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">persistCredentials</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">clean</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">script</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">      git config --global user.email &#34;mischa@pipeline.com&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">      git config --global user.name &#34;Mischa Pipeline&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">      git switch -c &#34;$(branch_name)&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">      sed -i &#34;s/tag:.*/tag: $(passed_tag)/&#34; values.yaml 
</span></span></span><span class="line"><span class="cl"><span class="sd">      git add .
</span></span></span><span class="line"><span class="cl"><span class="sd">      git commit -m &#34;Update tag to $(passed_tag)&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">      git push --set-upstream origin &#34;$(branch_name)&#34;</span><span class="w">      
</span></span></span></code></pre></div><p>This also felt a bit hacky to do with explicit shell commands, but it was the only way I could find to achieve my goal. I used sed to update the tag.</p>
<h1 id="result">Result</h1>
<p>The resulting deployment pipeline is as follows.</p>
<ol>
<li>I make a commit to my application repo, which triggers a build pipeline in Azure DevOps:</li>
</ol>
<p><img loading="lazy" src="/trigger-pipeline1.png" type="" alt=""  /></p>
<ol start="2">
<li>This resulted in an image pushed to my Docker Hub:</li>
</ol>
<p><img loading="lazy" src="/docker-hub.png" type="" alt=""  /></p>
<ol start="3">
<li>The pipeline created a new branch in my GitOps repo. Unfortunately, I have to make the PR myself, but as you can see, the pipeline successfully updates the values.yaml with the new tag which we also saw in Docker Hub:</li>
</ol>
<p><img loading="lazy" src="/new-branch.png" type="" alt=""  /></p>
<p><img loading="lazy" src="/update-tag.png" type="" alt=""  /></p>
<ol start="4">
<li>When I merged the pull request, ArgoCD detected the change and deployed a new pod with the new tag.</li>
</ol>
<p><img loading="lazy" src="/argocd-sync.png" type="" alt=""  /></p>
<ol start="5">
<li>Running a <code>kubectl describe</code> on the pod also verifies that we have the correct image:</li>
</ol>
<p><img loading="lazy" src="/kubectl-tag.png" type="" alt=""  /></p>
<h1 id="conclusion">Conclusion</h1>
<p>This was a fun challenge, but I learned a lot from solving the problems I encountered and my entire Saturday flew by in an uninterrupted flow state. I had some good practice in setting up Azure pipelines, learned about Helm, and did my first implementation of GitOps. Not bad for a day&rsquo;s work!</p>
<h1 id="links">Links</h1>
<p><a href="https://github.com/mischavandenburg/static-quote-app">Application GitHub repo</a></p>
<p><a href="https://github.com/mischavandenburg/static-quote-app-gitops">GitOps repo</a></p>
<p><a href="https://minikube.sigs.k8s.io/docs/">minikube</a></p>
<p><a href="https://kind.sigs.k8s.io/">kind</a></p>
<p><a href="https://argo-cd.readthedocs.io/">ArgoCD</a></p>
<p><a href="https://redhat-scholars.github.io/argocd-tutorial/argocd-tutorial/02-getting_started.html">tutorial to run ArgoCD in minikube</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Tokens and Identity on the Internet</title>
      <link>https://mischavandenburg.com/zet/articles/identity/</link>
      <pubDate>Sun, 18 Dec 2022 00:55:47 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/identity/</guid>
      <description>Introduction Have you ever thought about your identity on the internet? How does LinkedIn know it is you when you log in to LinkedIn? And when you allow LinkedIn to post to your Twitter, how does LinkedIn access your account and not your kindergarten teacher&amp;rsquo;s account?
This is a &amp;ldquo;Mischa Explains&amp;rdquo; article where I attempt to explain a concept in my own words as simply as possible. I use the Feynman technique and pretend to explain it to a 12-year-old.</description>
      <content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>
<p>Have you ever thought about your identity on the internet? How does LinkedIn know it is you when you log in to LinkedIn? And when you allow LinkedIn to post to your Twitter, how does LinkedIn access your account and not your kindergarten teacher&rsquo;s account?</p>
<p>This is a &ldquo;Mischa Explains&rdquo; article where I attempt to explain a concept in my own words as simply as possible. I use the Feynman technique and pretend to explain it to a 12-year-old.</p>
<h1 id="identity">Identity</h1>
<p>The first step in this process is identity. You need a starting point; for many of us, this can be our Google account. You signed up for this account and probably verified this with your phone number.</p>
<p>This relates to <strong>authentication</strong>. Authentication is the process of verifying identity. You&rsquo;ll need to provide the correct password when you log in to your Google account. You must give a valid password to log in to your account and access the resources. Google uses your password to <strong>authenticate</strong> that it is you.</p>
<h1 id="authorization">Authorization</h1>
<p>Then we have <strong>authorization</strong>. Authorization means granting access to particular resources. For example, let&rsquo;s say you are working in the science classroom at school. In the classroom is a bookcase that everybody can use: it is not dangerous, and every student can take the books they need without asking for permission. In the back of the science classroom is a cabinet that contains chemicals. It would be very dangerous if everybody could go into the cabinet and take out the sulphuric acid. Not everybody might know how dangerous sulphuric acid is. That&rsquo;s why the cabinet is locked.</p>
<p>If you need something from the chemicals cabinet, you need to ask permission from the teacher. You need to be <strong>authorized</strong> by the teacher to take out the sulphuric acid. When you make your request, the teacher may ask you questions to ensure you know what you are doing. He might even ask you for your school ID card because he has not seen you before. The teacher <strong>authenticates</strong> you by asking for your school ID, and then he <strong>authorizes</strong> you to take out the sulphuric acid.</p>
<h1 id="tokens">Tokens</h1>
<p>How do we accomplish this on the internet?</p>
<p>To verify identities on the internet, we have identity providers. Google is an identity provider. Azure AD is also an identity provider. An open-source identity provider is Keycloak.</p>
<p>Identity providers use <em>tokens</em> to verify identity and authorize access to resources. There are two types of tokens: ID tokens and access tokens. And for each token, there is an associated protocol.</p>
<h2 id="id-tokens">ID tokens</h2>
<p>OpenID Connect, also known as OIDC, is an open standard for authentication. Identity providers have agreed with each other that they will use this standard. When you go through an OpenID workflow, the result is an ID token, proving that the user has been authenticated.</p>
<p>Your school ID card is the ID token in our science class example. When you started at your school, you went through a registration process. Your parents probably handled this. Your name was written down, and the school verified that it was you by looking at your passport and talking to your parents. The result of this process was your school ID card, which you use to borrow books from the library. The school ID card proves that you are a student of that school and that you can use the facilities at the school.</p>
<h2 id="access-tokens">Access tokens</h2>
<p>These are specifically designed to allow access to a resource. For example, this resource could be a file on a server or a database.</p>
<p>Access tokens are strictly for authorization and use the OAuth 2.0 standard.</p>
<p>In our science class example, the token would be the key to the chemicals cabinet. The teacher authorizes you to access the cabinet and gives you the key to the cabinet.</p>
<h2 id="putting-it-together">Putting it Together</h2>
<p>Now let&rsquo;s put it together with an example.</p>
<p>You just created a new Facebook account and want to add all your friends. However, you have a Google account, and Facebook can use the contacts in your Google account to automatically add all of your friends.</p>
<p>Your Google account can only be accessed by you, and your contacts are locked away behind a password. But it is possible to grant Facebook access to this.</p>
<p>On Facebook, you select the &ldquo;import contacts from Google&rdquo; function. Facebook sends you to Google, and Google will ask you to log in. Google is the teacher in our science class example. Google needs you to <strong>authenticate</strong> to prove that it is you. When this is done, Google generates an ID token using OIDC for Facebook: Google gives Facebook a school ID that it can use.</p>
<p>Next, Facebook needs access to the contacts in your Google account. In our example, Facebook asks to take the sulphuric acid from the chemicals cabinet. You will see a menu that specifies what Facebook wants to do, and you need to give your permission. When you give your permission, Google generates an OAuth 2.0 token for Facebook. In other words, Google gives the key to the chemicals cabinet to Facebook, and Facebook is now authorized to take the sulphuric acid.</p>
<p>When both of these tokens are generated, Facebook contacts Google and asks if it can take the sulphuric acid from the chemicals cabinet.</p>
<p>Google, the teacher, asks Facebook for the school ID, and Facebook shows the ID card it received earlier. When Google is satisfied with the ID and successfully authenticates Facebook, it gives Facebook the key to the chemicals cabinet. Facebook is now authorized to take out the sulphuric acid. Facebook is now authorized to access the contacts in your Google account.</p>
<h1 id="links">Links</h1>
<p>You can use these resources to learn more about this topic:</p>
<p><a href="https://www.youtube.com/watch?v=t18YB3xDfXI">An Illustrated Guide to OAuth and OpenID connect</a></p>
<p><a href="https://www.youtube.com/watch?v=M4JIvUIE17c">ID Tokens vs Access Tokens - Do you know the difference?</a></p>
<p><a href="https://learn.microsoft.com/en-us/azure/active-directory/develop/id-tokens">Microsoft Learn: ID Tokens</a></p>
<p><a href="https://learn.microsoft.com/en-us/azure/active-directory/develop/security-tokens">Microsoft Learn: Security Tokens</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Podcast Tip: The system that runs Norway&#39;s welfare payments</title>
      <link>https://mischavandenburg.com/zet/articles/nav-podcast/</link>
      <pubDate>Fri, 02 Dec 2022 20:45:46 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/nav-podcast/</guid>
      <description>An interesting podcast episode describing the system that runs Norway&amp;rsquo;s welfare payments. It was interesting to hear that they focus on open source and that everything runs one one big kubernetes cluster.
It was also very inspiring to hear that they went from 4 nightly deployments a year to 1700 deployments a week.
Link to the podcast episode:
The system that runs Norway&amp;rsquo;s welfare payments</description>
      <content:encoded><![CDATA[<p>An interesting podcast episode describing the system that runs Norway&rsquo;s welfare payments. It was interesting to hear that they focus on open source and that everything runs one one big kubernetes cluster.</p>
<p>It was also very inspiring to hear that they went from 4 nightly deployments a year to 1700 deployments a week.</p>
<p>Link to the podcast episode:</p>
<p><a href="https://changelog.com/shipit/78">The system that runs Norway&rsquo;s welfare payments</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Book Notes: The Unicorn Project</title>
      <link>https://mischavandenburg.com/zet/articles/unicorn-project/</link>
      <pubDate>Tue, 11 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/unicorn-project/</guid>
      <description>This book is the sequel to the Phoenix project. Both books are set at Parts Unlimited, a fictitious company that supplies car parts to DIY mechanics and repair shops. Phoenix is a new system that Parts Unlimited has worked on for multiple years. It is supposed to handle order processing and communication between manufacturing, stores, and clients. Phoenix will also play a role in sales and marketing. The company has been gathering customer data for years, but it cannot use any of this data yet.</description>
      <content:encoded><![CDATA[<p>
  <img loading="lazy" src="https://m.media-amazon.com/images/W/IMAGERENDERING_521856-T1/images/I/91UM5i4nirL.jpg" alt="Unicorn Project"  /></p>
<p>This book is the sequel to <a href="/zet/articles/phoenix-project">the Phoenix project</a>. Both books are set at Parts Unlimited, a fictitious company that supplies car parts to DIY mechanics and repair shops. Phoenix is a new system that Parts Unlimited has worked on for multiple years. It is supposed to handle order processing and communication between manufacturing, stores, and clients. Phoenix will also play a role in sales and marketing. The company has been gathering customer data for years, but it cannot use any of this data yet. Phoenix will enable it to generate targeted marketing campaigns from the data when it&rsquo;s finished.</p>
<p>But as we saw in the previous book, it is far from finished, and things go wrong all the time. The company is not doing well, the stock prices are falling, and they need an edge over the competition. Phoenix will be their edge, but they&rsquo;ve been working on it for years. Eventually, management decided that Phoenix needed to be deployed in two weeks. But it is far from ready.</p>
<p>The main character in the Unicorn project is Maxine, a senior developer who temporarily transferred to a different department. She had to work on the Phoenix project against her will because of an unfortunate situation that needed a scapegoat. However, Maxine decides to make the best out of it, and she&rsquo;d like to begin with one thing: to get a Phoenix build going on her laptop.</p>
<p>Very quickly, she finds it impossible to run a full build of the Phoenix project due to missing files and other elements. She is appalled and makes it her mission to get the build going, but she meets another hurdle every step of the way. Missing credentials. Missing binaries and libraries. And for each of these hurdles, she must submit a ticket with a different department. Very soon, she has over 20 tickets running with long waiting times. Just to get a build going on her machine so she can work! Dozens of developers were hired to work on the Phoenix project. But when she asks them if they&rsquo;ve managed to get a build going yet, Maxine is horrified to discover that they&rsquo;ve tried for several months but haven&rsquo;t made any progress. Maxine has made more progress in a week.</p>
<blockquote>
<hr>
<p>&ldquo;Everyone around here thinks features are important because they can see them in their app, on the web page, or in the API. But no one seems to realize how important the build process is. Developers cannot be productive without a great build, integration, and test process.&rdquo;</p>
<ul>
<li>Unicorn Project</li>
</ul>
<hr>
</blockquote>
<p>After a few weeks, Maxine receives an invitation to have a drink with a group of people who are very interested in her. When she arrives at the bar, she meets the Rebellion: a group of developers, managers, and people from Operations, who are tired of the old organizational structure and want to make real changes. They think out of the box and experiment with new technologies, even though they are not authorized to do so.</p>
<p>With the Rebellion, Maxine significantly improves the build and deployment process. They recognized that Phoenix actually never was being built in its entirety. Developers were always working on parts of the application. However, after a lot of struggle, they create a build process that enables each developer to become operational on his first day.</p>
<p>This is the first step of a long series of exciting events that lead to Phoenix becoming a success. By the end of the book, they have a completely new development and testing process, and they can deploy changes to production without needing to take the entire application down. This allows them to create targeted marketing campaigns and respond to changes in the market. The first campaign was a huge success and generated the highest sales ever.</p>
<p>Maxine&rsquo;s struggle with the build process was an eye-opening experience for me. It gave me a very practical example of the need for DevOps principles to enable delivering value to customers. It is also something I recognize in my current organization. For example, projects can get stuck on a firewall change that needs to be approved by an external party. By implementing DevOps principles and arranging teams according to the &ldquo;you build it, you run it&rdquo; principle, teams can be responsible for the entire process from idea to production and therefore have a very short release cycle for their application.</p>
<p>I thoroughly enjoyed the first part of the book. However, the second part was less engaging to me. It became long-winded and felt like butter spread over too much bread. The author demonstrates a high level of technical experience and knowledge through his descriptions of processes, deployments, and fictional applications. Although I understand the intention of making Parts Unlimited a believable company, I think it could have been accomplished with much less detail and words.</p>
<p>The second part has more corporate drama, such as temporarily suspended managers without any clear reason. The focus shifts from a development and operations perspective to a managerial perspective. Maybe I will reread the book in a few years and this part will make a lot more sense to me then. The same happened when I reread the Phoenix project. I could not understand some aspects of the book, which became much clearer to me when I revisited it after gaining experience in the field.</p>
<p>I highly recommend this book to anyone working as a developer, DevOps Engineer, or in operations, especially if you are starting your career. The book gave me a lot of insights into &ldquo;the old way of working&rdquo; and a better understanding of the need for DevOps principles in the modern IT landscape. However, make sure to read the Phoenix project first.</p>
<h2 id="the-unicorn-project-a-novel-about-developers-digital-disruption-and-thriving-in-the-age-of-data-by-gene-kim">The Unicorn Project: A Novel about Developers, Digital Disruption, and Thriving in the Age of Data by Gene Kim</h2>
]]></content:encoded>
    </item>
    
    <item>
      <title>Book Notes: The Phoenix Project</title>
      <link>https://mischavandenburg.com/zet/articles/phoenix-project/</link>
      <pubDate>Fri, 16 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/phoenix-project/</guid>
      <description>When I started my DevOps traineeship, I borrowed this book from my boss and read it from cover to cover. I loved the story and the characters; it helped me understand &amp;ldquo;the old way&amp;rdquo; of doing things and the merits of implementing DevOps principles.
I reread the book ten months later. In the meantime, I&amp;rsquo;ve learned many new skills and technologies and started working full-time as a DevOps engineer.</description>
      <content:encoded><![CDATA[<p>
  <img loading="lazy" src="https://m.media-amazon.com/images/W/IMAGERENDERING_521856-T1/images/I/914-sUgELZL.jpg" alt="Phoenix Project"  /></p>
<p>When I started my DevOps traineeship, I borrowed this book from my boss and read it from cover to cover. I loved the story and the characters; it helped me understand &ldquo;the old way&rdquo; of doing things and the merits of implementing DevOps principles.</p>
<p>I reread the book ten months later. In the meantime, I&rsquo;ve learned many new skills and technologies and started working full-time as a DevOps engineer.</p>
<p>Not surprisingly, it made a lot more sense to me this time, and I&rsquo;m sure it will be even better when I reread it a few years later. In this article, I&rsquo;ll share my thoughts and notes on the book.</p>
<h2 id="the-story">The Story</h2>
<p>The main character is Bill Palmer, a mid-level IT manager in a manufacturing company called Parts Unlimited. Within a few pages, he is called into the CEO&rsquo;s office, and he is promoted to the VP of Operations, putting him in charge of IT, much against his own will or desire.</p>
<p>The situation Bill enters is a humorously chaotic one. We are thrown straight into a Sev 1 incident where managers point fingers and shout at each other. We quickly get the impression that this is a dysfunctional department that only performs tasks for the manager who shouts the loudest while fighting off crippling outages.</p>
<blockquote>
<p>It’s like the Wild West out here. We’re mostly shooting from the hip.”<br>
<em>The Phoenix Project</em></p>
</blockquote>
<p>The bulk of the story revolves around how Bill, together with his team of managers, Wes, Patty, and John, manage to turn this chaos into a department that does work according to a streamlined plan in a much more predictable manner.</p>
<p>To achieve this goal, Bill is introduced to Erik, a prospective board member of the company. Erik becomes Bill&rsquo;s mentor and guides Bill through the process of creating order in the chaos. Their interaction reminds me of Zen masters training their disciples by asking deep questions which don&rsquo;t have an immediately apparent answer.</p>
<h2 id="master--disciple">Master &amp; Disciple</h2>
<p>Erik takes Bill to the manufacturing plant of Parts Unlimited and tries to impress upon Bill that manufacturing planning principles from Lean can be applied to IT work. Erik argues that an IT department could be structured like a factory production line, but Bill is not ready to accept this.</p>
<p>A fundamental notion from manufacturing principles is that work should always be moving forwards along the production line, never backward. But unfortunately, this is very often the case in the &ldquo;old&rdquo; way of working: the development team works on an application for several months, and when they are finished with it, they throw it over the fence to the Operations people, whose job it is to deploy the application.</p>
<blockquote>
<p>One of the developers had actually walked in a couple of minutes ago and said, “Look, it’s running on my laptop. How hard can it be?”<br>
<em>The Phoenix Project</em></p>
</blockquote>
<p>However, as we see happening time after time in the book, usually the application is incompatible with the infrastructure it is deployed to. As a result, the application needs to go back to development. According to manufacturing theory, this is a situation where work goes backward through the production line, which we must avoid at all costs.</p>
<h2 id="implementation">Implementation</h2>
<p>Erik challenges Bill to start doing ten deployments a day instead of one deployment every nine months. Understandably, this is a ridiculous notion to Bill. The last few deployments were disastrous events that required his entire department to pull all-nighters through the weekend, and still, the stores were not managing to process all orders and payments.</p>
<p>However, Bill takes his mentor&rsquo;s advice and figures out a way to do it together with his team. One of the main problems they uncovered was the inconsistent deployment and production environments.</p>
<p>The solution to this problem was to involve the operations people in the development stage right from the beginning, so the development environment matched the production environment exactly. The environments were standardized and put in code with version control, and things started progressing quickly.</p>
<blockquote>
<p>As Wes talks, I think about Erik challenging me to think like a plant manager as opposed to a work center supervisor. I suddenly realize that he probably meant that I needed to span the departmental boundaries of Development and IT Operations.
<em>The Phoenix Project</em></p>
</blockquote>
<p>This is just one of the problems addressed by melting away the fence between Development and Operations. By the end of the book, the two camps started to work together much better. They come closer to the target of 10 deployments a day, and the DevOps way of working was born.</p>
<h2 id="closing-thoughts">Closing Thoughts</h2>
<p>I think this book is a must-read for anyone considering entering the DevOps field or anyone already working with DevOps.</p>
<p>As a nerd who loves structure and organization, the theme of the story is incredibly entertaining and satisfying to me. The authors excellently capture the transition from an utterly disorganized situation to a predictable environment with happy co-workers. Actually, I&rsquo;m a little embarrassed by how much joy this transition brings me.</p>
<p>Especially the second time around, it helped me better understand the underlying principles that enable the DevOps way of working in an organization. Moreover, it paints a great picture of how an organization can change for the better by embracing DevOps principles and how these changes express themselves in the improved quality and speed of software development and deployment. All of these advantages lead to delivering better value to the customer, which is the core focus of any productive and creative endeavor involving customers and end users.</p>
<h2 id="the-phoenix-project-written-by-gene-kim-kevin-behr-george-spafford">The Phoenix Project, written by Gene Kim, Kevin Behr, George Spafford</h2>
]]></content:encoded>
    </item>
    
    <item>
      <title>Certified Kubernetes Administrator (CKA) Exam Tips</title>
      <link>https://mischavandenburg.com/zet/articles/cka-tips/</link>
      <pubDate>Tue, 13 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/cka-tips/</guid>
      <description>I recently obtained my CKA certification. I started this certification journey with zero knowledge of Kubernetes. However, I was already working as a DevOps Engineer, and I know a fair bit of Linux. I daily drive Arch Linux and have LPIC-1 certification. It was handy to know where files are located on Linux systems and how to interact with systemd services. I also knew yaml quite well because I work with Ansible daily.</description>
      <content:encoded><![CDATA[<p>I recently obtained my CKA certification. I started this certification journey with zero knowledge of Kubernetes. However, I was already working as a DevOps Engineer, and I know a fair bit of Linux. I daily drive Arch Linux and have LPIC-1 certification. It was handy to know where files are located on Linux systems and how to interact with systemd services. I also knew yaml quite well because I work with Ansible daily. I passed on my first try, and I did one session of killer.sh.</p>
<h3 id="my-preparation">My preparation</h3>
<ul>
<li><a href="https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests/">KodeKloud CKA Course</a></li>
<li><a href="https://killer.sh/">Killer.sh Mock Exam</a></li>
<li><a href="https://killercoda.com/killer-shell-cka">Killercoda</a></li>
</ul>
<p>I kept track of the time I spent on this certification. In total, I spent 80 hours on study and practice.</p>
<h3 id="in-hindsight">In Hindsight</h3>
<p>I spent too much time repeating things during the KodeKloud course. This is the one thing I would do differently if I could start over. I went over some modules multiple times and kept meticulous notes. However, I have hardly used any of those notes. But they will be nice to have for the future.</p>
<p>I learned most from the killer.sh exams. So I would advise you to go through the KodeKloud course and do all the exercises, but don&rsquo;t spend too much time repeating stuff. If you don&rsquo;t understand the topic at all, it is, of course, necessary to repeat it. But you don&rsquo;t need to know all the details.</p>
<h3 id="killersh">Killer.sh</h3>
<p>After I finished the KodeKloud course, I purchased the exam voucher and started the killer.sh on Saturday morning. I wanted to simulate the exam experience as much as possible, so I set the timer and did not allow myself to stand up for two hours. My first round was humiliating. I only managed to get 24 out of 125 points. A little shocked by the experience, I spent the whole Saturday going through all the solutions of the exercises that killer.sh provides. The explanations they give are extensive, and I found them helpful. Saturday evening, I went out for dinner with friends, and on Sunday morning, I passed killer.sh. I spent the whole Sunday studying the solutions more and more, and on my last try on Sunday evening, I scored 115 out of 125.</p>
<h2 id="tips">Tips:</h2>
<ul>
<li>
<p>I know tmux quite well and used it extensively during the killer.sh, but it was not necessary during the exam. No need to learn it if you don&rsquo;t know it already.</p>
</li>
<li>
<p>Knowing vim well will save you a lot of time at the exam. For example, dG to delete all lines until the end of the file from your current location. Run &ldquo;vimtutor&rdquo; on a Linux system to learn the basics.</p>
</li>
<li>
<p>You cannot use bookmarks. Learn how to search the docs efficiently. One handy one I figured out was to control + F and enter &ldquo;kind: Pod&rdquo; or &ldquo;kind: PersistentVolume&rdquo; to immediately go to the example YAML.</p>
</li>
<li>
<p>my exam environment did not need much extra configuration. All I added to my .bashrc was alias v=vim and export do=&quot;&ndash;dry-run=client -o yaml&quot; so you can use &ldquo;k run Nginx $do &gt; Nginx.YAML&rdquo;</p>
</li>
<li>
<p>The exam environment is not as bad as people make it out to be on the internet. There is a little delay while scrolling through the docs in the browser, but working in the terminal didn&rsquo;t give me any problems. Get used to the environment on killer.sh, and there should not be any surprises in the real exam environment.</p>
</li>
<li>
<p>Skip questions you cannot solve immediately. But don&rsquo;t spend time reviewing all the questions, sorting by the highest % and doing those first. You will lose a lot of time evaluating all of these questions. It is much better to solve the questions during your first pass through and skip the ones you cannot immediately solve.</p>
</li>
<li>
<p>When the 120-minute timer ran out, I was presented with a screen that said &ldquo;quit&rdquo; or &ldquo;request more time.&rdquo; I was pretty sure I could not get more time for this exam, so I just pressed &ldquo;quit.&rdquo; After I pressed quit, the application closed immediately, and there was no confirmation whatsoever that they received my exam results or anything. This was extremely disorienting, and I was left doubting if I had done it correctly. Eventually, I could see in the Linux Foundation portal that my exam was in Grading status.</p>
</li>
<li>
<p>Speed is of the essence. An hour before my exam, I used killercoda to get into the mood and get things up to speed. Learn to solve things quickly and don&rsquo;t spend time having to arrange terminal windows on your screen or stumbling around in vim. You cannot afford to lose time on these things.</p>
</li>
<li>
<p>Finally, this video is an excellent summary of all the necessary tips and information: <a href="https://www.youtube.com/watch?v=8VK9NJ3pObU"></a></p>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>How to Upgrade Java and Jenkins on Ubuntu 18.04</title>
      <link>https://mischavandenburg.com/how-to-upgrade-java-and-jenkins-on-ubuntu-18-04/</link>
      <pubDate>Tue, 19 Jul 2022 18:01:26 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/how-to-upgrade-java-and-jenkins-on-ubuntu-18-04/</guid>
      <description>Last week I upgraded Jenkins to the latest version on the server infrastructure at work. Starting with the Jenkins 2.357 release, Java 11 or Java 17 will be required to run Jenkins. Also, the upcoming LTS release will require Java 11.
This means that I also needed to update Java on our Jenkins servers. Here are the steps that I did to perform the Jenkins and Java upgrade.
SSH into the server and stop the service.</description>
      <content:encoded><![CDATA[<p>Last week I upgraded Jenkins to the latest version on the server infrastructure at work. Starting with the Jenkins 2.357 release, Java 11 or Java 17 <a href="https://www.jenkins.io/blog/2022/06/28/require-java-11/">will be required to run Jenkins</a>. Also, the upcoming LTS release will require Java 11.</p>
<p>This means that I also needed to update Java on our Jenkins servers. Here are the steps that I did to perform the Jenkins and Java upgrade.</p>
<p>SSH into the server and stop the service. Then get the latest upgrades for your server, which is good practice:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">service jenkins stop
</span></span><span class="line"><span class="cl">apt-get update
</span></span><span class="line"><span class="cl">apt-get upgrade
</span></span></code></pre></div><p>Depending on your setup, the apt-get upgrade command might upgrade Jenkins to the latest version that does not require Java 11+. In my case, that was 3.346.</p>
<p><strong>When you get a question about updating your current config file, take the default option. This option keeps your current configuration.</strong></p>
<p>However, if your Jenkins is installed from a binary or another source, you might need to upgrade Jenkins to 3.346 using the Jenkins.war file:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> /usr/share/jenkins
</span></span><span class="line"><span class="cl">mv jenkins.war jenkins.war.old
</span></span><span class="line"><span class="cl">wget https://updates.jenkins-ci.org/latest/jenkins.war
</span></span><span class="line"><span class="cl">service jenkins start
</span></span></code></pre></div><p>When you start Jenkins, it will be updated to the latest version that does not require Java 11 or higher. You will notice that there will be a new folder called migrate in /usr/share/jenkins , and the jenkins.war is now located in /usr/share/java</p>
<p>This is where I got confused because it did not patch to the latest version, only up to 3.346 and the jenkins.war file was no longer being updated from the /usr/share/jenkins folder.</p>
<p>The reason is that this update moves the .war file to the /usr/share/java directory.</p>
<h1 id="java">java</h1>
<p>To get Jenkins to the latest version, we need to install or update Java and check if it has worked:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">apt-get install default-jre
</span></span><span class="line"><span class="cl">java -version
</span></span></code></pre></div><p>Now that you have updated the java version, you are ready to update Jenkins to the latest version.</p>
<p>Notice that we use the /usr/share/java folder now, instead of /usr/share/jenkins</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">service jenkins stop
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> /usr/share/java
</span></span><span class="line"><span class="cl">mv jenkins.war jenkins.war.old
</span></span><span class="line"><span class="cl">wget https://updates.jenkins-ci.org/latest/jenkins.war
</span></span><span class="line"><span class="cl">service jenkins start
</span></span></code></pre></div><h1 id="nodes">nodes</h1>
<p>When I accessed the Jenkins GUI, everything seemed fine, and my version was up to 3.358.</p>
<p>However, I noticed that the build nodes were all offline. When inspecting the logs, I saw the following error:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">java.io.EOFException
</span></span><span class="line"><span class="cl">	at java.base/java.io.ObjectInputStream<span class="nv">$PeekInputStream</span>.readFully<span class="o">(</span>ObjectInputStream.java:2905<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at java.base/java.io.ObjectInputStream<span class="nv">$BlockDataInputStream</span>.readShort<span class="o">(</span>ObjectInputStream.java:3400<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at java.base/java.io.ObjectInputStream.readStreamHeader<span class="o">(</span>ObjectInputStream.java:936<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at java.base/java.io.ObjectInputStream.&lt;init&gt;<span class="o">(</span>ObjectInputStream.java:379<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at hudson.remoting.ObjectInputStreamEx.&lt;init&gt;<span class="o">(</span>ObjectInputStreamEx.java:49<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at hudson.remoting.Command.readFrom<span class="o">(</span>Command.java:142<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at hudson.remoting.Command.readFrom<span class="o">(</span>Command.java:128<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at hudson.remoting.AbstractSynchronousByteArrayCommandTransport.read<span class="o">(</span>AbstractSynchronousByteArrayCommandTransport.java:35<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at hudson.remoting.SynchronousCommandTransport<span class="nv">$ReaderThread</span>.run<span class="o">(</span>SynchronousCommandTransport.java:61<span class="o">)</span>
</span></span><span class="line"><span class="cl">Caused: java.io.IOException: Unexpected termination of the channel
</span></span><span class="line"><span class="cl">	at hudson.remoting.SynchronousCommandTransport<span class="nv">$ReaderThread</span>.run<span class="o">(</span>SynchronousCommandTransport.java:75<span class="o">)</span>
</span></span></code></pre></div><p>Observing that the error had something to do with Java, I ssh’d into the build nodes and updated Java there as well with the same command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">apt-get install default-jre
</span></span></code></pre></div><p>After updating Java on the build node, head back to the GUI on the master node and restart the build node.</p>
<p>It should now be online again.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>5 Reasons Why I Changed my Career to IT in my Thirties</title>
      <link>https://mischavandenburg.com/5-reasons-why-i-changed-my-career-to-it-in-my-thirties/</link>
      <pubDate>Mon, 04 Jul 2022 20:04:33 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/5-reasons-why-i-changed-my-career-to-it-in-my-thirties/</guid>
      <description>In this post, I’d like to share the five main reasons why I changed my career to IT in my thirties. Making a career change can be daunting, especially when you are past your twenties, and employers can get more skeptical of hiring and training you. However, when I passed my twenties and became more serious and intentional about my life and career, I decided to take the plunge and hope for the best.</description>
      <content:encoded><![CDATA[<p>In this post, I’d like to share the five main reasons why I changed my career to IT in my thirties. Making a career change can be daunting, especially when you are past your twenties, and employers can get more skeptical of hiring and training you. However, when I passed my twenties and became more serious and intentional about my life and career, I decided to take the plunge and hope for the best. These are listed in no particular order.</p>
<h2 id="1-job-opportunities">#1 job opportunities</h2>
<p>IT has always been a field with many job opportunities, and with the current movements of digitalization and automation, there is no sign that it will slow down.</p>
<p>According to <a href="https://www.bls.gov/ooh/computer-and-information-technology/home.htm">the U.S. Bureau of Labor Statistics</a>, “Employment in computer and information technology occupations is projected to grow 13 percent from 2020 to 2030, faster than the average for all occupations. These occupations are projected to add about 667,600 new jobs. Demand for these workers will stem from greater emphasis on cloud computing, the collection and storage of big data, and information security.”</p>
<p>The situation is no different here in the Netherlands. Currently, there is a shortage of people in IT, and employers are much more willing to provide training to motivated individuals to make a change.</p>
<h2 id="2-remote-work">#2 remote work</h2>
<p>I think remote work is one of the best parts of living in post-pandemic 2022. I am an introvert, and having a quiet, stable space without distractions, which is the same from day to day, is a huge boost to my productivity.</p>
<p>Secondly, I think it is crucial to be mindful of your posture and body while working at a desk. For example, I am dependent on having a standing desk which I adjust more than ten times a day. I also need a chair suitable for my body type to avoid getting stiff and getting a sore back. Although some offices take care of providing these facilities to their employees, I think it is beneficial to invest in your own setup, which you can tailor to your own needs.</p>
<p>Thirdly, working from anywhere in the world is a massive advantage. I am not very interested in living a digital nomad lifestyle, working from a MacBook in coffee shops, but I think it’s great that you can spend some time abroad while working from that location.</p>
<h2 id="3-personal-interest">#3 personal interest</h2>
<p>This is a big one. You should not change your career to IT just because it earns well or because you think you can work from the beach in Thailand. I have been tinkering with computers and programming languages since I was a kid and have always enjoyed it. I always found myself “the computer guy” in groups of friends or colleagues.</p>
<p>However, for some reason, I never managed to make my career out of it until now, and I get a lot of satisfaction from my work every day after I made the change.</p>
<h2 id="4-high-income">#4 high income</h2>
<p>It is no secret that tech jobs are some of the best paying jobs in the U.S., having<a href="https://www.bls.gov/oes/current/oes_nat.htm#15-0000"> a mean wage of $99,860</a>. And if you work your way up into management, there are even higher salaries. Here <a href="https://www.nationaleberoepengids.nl/salaris/ict">in the Netherlands</a>, it is also a financially sound choice, with a mean wage of €47.200</p>
<h2 id="5-fast-changing-field">#5 fast-changing field</h2>
<p>IT is a broad field with many little niches you can get into, and every niche is constantly developing. Being in IT means you will need to stay on board by continuing to learn the new technologies and languages to keep on track.</p>
<p>This is also what makes it exciting to me, being a life-long learner. There is always more to learn and some cutting-edge technology to become familiar with, which can improve your workflow and your deployments.</p>
<h2 id="links">links:</h2>
<p><a href="https://www.bls.gov/ooh/computer-and-information-technology/home.htm">Computer and Information Technology Occupations – US Bureau of Labor Statistics</a></p>
<p><a href="https://www.bls.gov/oes/current/oes_nat.htm#15-0000">May 2021 National Occupational Employment and Wage Estimates</a></p>
<p><a href="https://www.nationaleberoepengids.nl/salaris/ict">Salaries in IT - Dutch</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Docker LEMP Stack deployed with Ansible</title>
      <link>https://mischavandenburg.com/docker-lemp-stack-deployed-with-ansible/</link>
      <pubDate>Sun, 30 Jan 2022 16:20:24 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/docker-lemp-stack-deployed-with-ansible/</guid>
      <description>In order to learn more about Docker and Ansible I am working on an assignment to take an existing application and to break it down into containers. However, in order to be able to understand this process properly, I first needed to understand more about Docker and containerisation.
I wrote a playbook that installs Docker and deploys a fully containerised LEMP stack on a virtual machine.
You can have a look at the Github repo with the result here.</description>
      <content:encoded><![CDATA[<p>In order to learn more about Docker and Ansible I am working on an assignment to take an existing application and to break it down into containers. However, in order to be able to understand this process properly, I first needed to understand more about Docker and containerisation.</p>
<p>I wrote a playbook that installs Docker and deploys a fully containerised LEMP stack on a virtual machine.</p>
<p><a href="https://github.com/mischavandenburg/lemp_docker_ansible">You can have a look at the Github repo with the result here. </a></p>
<p>The repo is using the “ansible-galaxy init” role structure. You will find the playbooks as follows: roles/your_choice/tasks/main.yml</p>
<h2 id="docker">Docker</h2>
<p>I was very excited to learn more about Docker and containerisation. I was familiar with the concept of virtualisation, which is creating virtual versions of fully functional machines on a host operating system. But the concept of containerisation was new to me.</p>
<p>As I understand it, containerisation differs drastically from virtualisation because containers are able to use resources from host directly. They do not need an entire operating system to run, and therefore they are a much more lightweight.</p>
<p>This means that resources can be used much more efficiently which eventually can mean cost reduction in your cloud infrastructure.</p>
<p>Docker is a very popular platform for building and running containers. It seemed like the best option to get started with deploying my own containers.</p>
<h2 id="lemp-stack">LEMP Stack</h2>
<p>My colleague recommended me <a href="https://tech.osteel.me/posts/docker-for-local-web-development-part-1-a-basic-lemp-stack">this tutorial</a> to become more familiar with Docker. It uses a LEMP stack as an example application. When I told friends about the fact that I was building a LEMP stack, they corrected me and said it was a LAMP stack.</p>
<p>The LAMP stack is a collection of software built out of these elements:</p>
<p>L – Linux: the operating system</p>
<p>A – Apace: webserver</p>
<p>M – MySQL: database</p>
<p>P – PHP: server scripting language</p>
<p>However, in a LEMP stack, we use NGINX as a webserver, which is pronounced “Engine X”, hence the E in LEMP stack. Therefore, LEMP is the correct way to spell it, and it is used in all the tutorials that I have been using.</p>
<p>I highly recommend <a href="https://tech.osteel.me/posts/docker-for-local-web-development-part-1-a-basic-lemp-stack">the tutorial</a> in order to learn how to deploy your first collection of containers. Deploying one container is relatively easy with Docker, but it gets a little more complicated when deploying several containers and making them communicate with each other in order to combine them into one application. But this tutorial does a great job at showing you how it’s done and it is especially good at explaining the steps along the way.</p>
<h2 id="docker-compose-vs-ansible">Docker Compose vs. Ansible</h2>
<p>Docker Compose is a tool you can use to run multi-container applications. With the help of the tutorial, it was fairly easy to understand and hit the ground running by deploying multiple containers into one network.</p>
<p>Now let’s have a look at how we actually set up the containers. In the Docker Compose file, the NGINX container was defined like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;3.8&#39;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="c">#Services</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">services</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="c">#Nginx Service</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">nginx</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">nginx:1.19</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="m">80</span><span class="p">:</span><span class="m">80</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="l">./src:/var/www/php</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="l">./.docker/nginx/conf.d:/etc/nginx/conf.d</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">depends_on</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="l">php</span><span class="w">
</span></span></span></code></pre></div><p>It looks pretty straightforward, right? Almost like pseudocode. We tell Docker which image to pull from the Docker Hub, and we tell it to route container’s port 80 to our host’s port 80. This ensures that the web server can be accessed from the outside, provided you have opened this port in the firewall.</p>
<p>Next there is the volumes section: this mounts certain directories on the host into the container so it is accessible. In this case this was necessary to transfer the web server configuration and the index.php which we wanted to serve to the outside.</p>
<p>Having successfully deployed my LEMP using Docker Compose, the next step was to automate this process by using Ansible. Ansible is a very powerful tool which enables you to automate configuration management and application deployment by writing scripts called playbooks.</p>
<h5 id="why-was-it-necessary-to-introduce-ansible">Why was it necessary to introduce Ansible?</h5>
<p>By using Docker Compose, you would need to have Docker and Docker Compose installed on the virtual machine before you could start running the containers.</p>
<p>However, Ansible gives you the power to take a completely fresh virtual machine, configure it from scratch, and install Docker and its necessary dependencies, followed by deploying the containers.</p>
<h4 id="now-lets-take-a-look-at-the-same-container-defined-in-ansible">Now let’s take a look at the same container defined in Ansible:</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl">- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">start nginx </span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">docker_container</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">nginx:1.19</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">detach</span><span class="p">:</span><span class="w"> </span><span class="kc">yes</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="m">80</span><span class="p">:</span><span class="m">80</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">networks</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">network_one</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="l">/src:/var/www/php</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="l">/.docker/nginx/conf.d:/etc/nginx/conf.d</span><span class="w">
</span></span></span></code></pre></div><p>Although there are some differences, they look very similar. Converting my Docker Compose file to an Ansible playbook was quite a natural and easy experience. It also helps that both are written in YAML and therefore use the same indentation conventions.</p>
<p>A few differences we can observe:</p>
<p>In the Ansible playbook we invoke the docker_container module, whereas they are defined as services in the Docker Compose file. Another difference is that we need to set up the network ourselves. In the Docker Compose file, we just specified the containers and Docker Compose created a network automatically and made sure that all containers were connected to it.</p>
<p>However, it isn’t very complicated in Ansible either:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl">- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">setup network</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">docker_network</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">network_one</span><span class="w">
</span></span></span></code></pre></div><p>We simply call the docker_network module and tell it to make a network called network_one. All we need to do then is make sure to set the networks: parameter to network_one in the docker_container module as we saw above.</p>
<p>The last point to note is the detach parameter. This means that the container will keep running in the background after it is started.</p>
<h3 id="result">Result</h3>
<p>After some debugging here and there and making sure all of the elements were in place, eventually we get the satisfying message that everything went according to plan:</p>
<p><img loading="lazy" src="/success.png" type="" alt="Successful play from Ansible"  /></p>
<p>The result is a webpage being served on the server ip:</p>
<p><img loading="lazy" src="/webpage.png" type="" alt="The final web page served by Nginx "  /></p>
<p>I know, it is not the prettiest or most intricate design. But remember that I am working towards becoming a DevOps Engineer, not a Front End Developer 😉</p>
<p>We can also enter the phpMyAdmin dashboard by adding port 8080 to our ip in the browser:</p>
<p><img loading="lazy" src="/php.png" type="" alt="The PHP page."  /></p>
<h2 id="conclusion">Conclusion</h2>
<p>The assignment of deploying a LEMP stack in separate containers has been very useful and I learned a lot from the process. There were a few more modules that needed to be configured in Ansible as opposed to the Docker Compose method, but the tradeoff is that Ansible is much more powerful and enables you to configure the server from scratch. You can have a look at the code in the GitHub repo to see all of the changes I needed to do.</p>
<p>The only part that I needed to do by hand is to create the VM in the Microsoft Azure portal, open the ports and configure the SSH keys. The next step in my learning process will be to learn how I can automate this step as well. This means that I will need to learn Terraform.</p>
<p>By using Terraform I will be truly deploying this stack as Infrastructure as Code, but doing all of these steps with Ansible has given me a much better understanding of Infrastructure as Code already.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>My Journey Into DevOps So Far</title>
      <link>https://mischavandenburg.com/my-journey-into-devops-so-far/</link>
      <pubDate>Fri, 28 Jan 2022 08:56:38 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/my-journey-into-devops-so-far/</guid>
      <description>In 2021 I had reached a certain stage in my life where I had the liberty to make a choice. I was 31 years old and had just finished an important chapter of my life, and was ready to begin a new one. After thinking about where I wanted to steer my professional career, I decided to follow my passion for programming and automation and attempt to make a career out of my interests in tech and IT.</description>
      <content:encoded><![CDATA[<p>In 2021 I had reached a certain stage in my life where I had the liberty to make a choice. I was 31 years old and had just finished an important chapter of my life, and was ready to begin a new one. After thinking about where I wanted to steer my professional career, I decided to follow my passion for programming and automation and attempt to make a career out of my interests in tech and IT.</p>
<p>I was in a fortunate situation, because there were large shortages of people on the IT job market in The Netherlands. Consequently, employers were much more willing to train their employees to perform the roles that they required to fulfill.</p>
<h2 id="bots-and-scripts">Bots and Scripts</h2>
<p>I have always loved messing around with computers, and I learned to write small and simple programs at a very young age. At the same time I was an avid gamer, and spent many hours slaying monsters in online RPG’s. One day I came across the concept of a bot: a program that plays the computer game for you, and I was hooked. This is where I developed my ‘fetish’ for automation.</p>
<p>I was very lucky that I had a friend who shared my interests, and together we built our own automation projects (called ‘botfarms’) in which we ran large amounts of bots that played a certain game for us. This army of bots generated in-game currency which we could sell for actual money. They weren’t huge profits, but it was an amazingly satisfying feeling to be the overlords of an army of automations that actually generated some income for us.</p>
<h2 id="from-bots-to-devops">From Bots to DevOps</h2>
<p>These botfarms were hosted on servers which set up ourselves. In order to save costs we rented Linux servers, and I spent many evenings figuring out how configure them via the command line. Often I would suddenly snap out of my flow at 3am and realise I had to go to work at 7 in the morning again.</p>
<p>Although I did not manage to make these personal interests into a personal career, my friend eventually became a Data Engineer. After making the decision to make a career switch to IT I needed to figure out which direction I wanted to go in, because IT covers a very broad range of topics and skills. Based on on our shared interests and previous projects, he recommended me to become a DevOps Engineer.</p>
<p>I very quickly realised that he was right on the money with his suggestion and I started to become very enthusiastic to learn how to make a living by working with computers and automation.</p>
<h2 id="traineeship">Traineeship</h2>
<p>As I mentioned before, employers in The Netherlands are now willing to train potential candidates, and I used my hobby projects as a way to demonstrate my genuine interest and affinity with IT and automation. I was offered a traineeship to become a DevOps Engineer in 2021. After a period of training I started working for the City of Amsterdam and I&rsquo;ve been part of an IT4IT operations team since.</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
