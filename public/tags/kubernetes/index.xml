<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Kubernetes on Mischa van den Burg</title>
    <link>https://mischavandenburg.com/tags/kubernetes/</link>
    <description>Recent content in Kubernetes on Mischa van den Burg</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 18 Jan 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://mischavandenburg.com/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>PersistentVolumeClaims Lifecycle in Kubernetes</title>
      <link>https://mischavandenburg.com/zet/kubernetes-storage-pvc-pv/</link>
      <pubDate>Thu, 18 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/kubernetes-storage-pvc-pv/</guid>
      <description>I always thought that Persistent Volume Claims where deleted when you delete the pod which they are associated with. I was wrong. The lifecycle of PVCs is independent of Pods, and their behavior is largely governed by the Reclaim Policy set on the PVs. Here&amp;rsquo;s what you need to know:
PVCs: These are requests for storage, akin to how Pods request resources like CPU and memory. They exist independently and can be bound to Pods when needed.</description>
      <content:encoded><![CDATA[<p>I always thought that Persistent Volume Claims where deleted when you delete the pod which they are associated with. I was wrong. The lifecycle of PVCs is independent of Pods, and their behavior is largely governed by the Reclaim Policy set on the PVs. Here&rsquo;s what you need to know:</p>
<ul>
<li><strong>PVCs</strong>: These are requests for storage, akin to how Pods request resources like CPU and memory. They exist independently and can be bound to Pods when needed.</li>
<li><strong>PVs</strong>: Provisioned by administrators or dynamically through Storage Classes, PVs provide the actual storage resources. Their lifecycle is not tied to any specific Pod.</li>
</ul>
<h1 id="understanding-reclaim-policies">Understanding Reclaim Policies</h1>
<p>The Reclaim Policy on a PV dictates its fate after a PVC is released. There are three policies to be aware of:</p>
<ol>
<li><strong>Delete</strong>: The PV and its underlying storage are deleted when the PVC is deleted. Importantly, if the PVC is not deleted, the PV remains.</li>
<li><strong>Retain</strong>: When a PVC is deleted, the PV is not. Instead, it transitions to a <code>Released</code> state, but the data remains intact until manually deleted or repurposed.</li>
<li><strong>Recycle (Deprecated)</strong>: The volume is scrubbed clean and made available for a new claim. Note: this policy is no longer recommended.</li>
</ol>
<h1 id="practical-implications">Practical Implications</h1>
<ul>
<li>PVCs won&rsquo;t cause PV deletion unless they themselves are deleted. This is crucial to understand, especially with a <code>Delete</code> Reclaim Policy, as it leads to the deletion of both the PV and the underlying storage, resulting in data loss.</li>
<li>Carefully manage PVCs, especially in production environments. Ensure you have backups and fully understand the implications of deleting PVCs, especially when using a <code>Delete</code> Reclaim Policy.</li>
</ul>
<h2 id="links">Links:</h2>
<p>202401180901</p>
<p>[[kubernetes]]</p>
<p>[[storage]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: Learning Flux and Installing To Homelab</title>
      <link>https://mischavandenburg.com/zet/video-homelab-learning-flux/</link>
      <pubDate>Wed, 17 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/video-homelab-learning-flux/</guid>
      <description>In this video I set up Flux running in a local cluster on my MacBook by following the getting started guide. Then I learn about how to structure the repo according to Flux methodology. I implement this structure in my homelab repo and deploy flux to my homelab cluster. Then I manage to configure Grafana and the Weave UI to be accessbible via ingress using a custom fake domain.</description>
      <content:encoded><![CDATA[
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/BtuqzsyztBc" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<p>In this video I set up Flux running in a local cluster on my MacBook by following the getting started guide. Then I learn about how to structure the repo according to Flux methodology. I implement this structure in my homelab repo and deploy flux to my homelab cluster. Then I manage to configure Grafana and the Weave UI to be accessbible via ingress using a custom fake domain.</p>
<p>Notes:</p>
<ul>
<li>
<p>kustomization resources live in the cluster</p>
</li>
<li>
<p>source resources also live in the cluster</p>
</li>
<li>
<p>To suspend updates for a kustomization, run the command flux suspend kustomization <!-- raw HTML omitted -->.</p>
</li>
<li>
<p>To resume updates run the command flux resume kustomization <!-- raw HTML omitted -->.</p>
</li>
<li>
<p>use <code>flux reconcile source git podinfo</code> to force a sync, nn waiting</p>
</li>
<li>
<p>love the fact that helm releases are still accessible and visible on the cluster</p>
</li>
<li>
<p>flux is just managing helm for you based on code</p>
</li>
<li>
<p>love the fact that weave dashboard is also part of the cluster manifests</p>
</li>
<li>
<p>learned about Kubernetes controllers</p>
</li>
<li>
<p>an operator is a controller</p>
</li>
<li>
<p>basically everything that enables CRD&rsquo;s to run is a controller</p>
</li>
</ul>
<p>&ldquo;A kubernetes controller is a control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state&rdquo;</p>
<h2 id="sources">Sources</h2>
<blockquote>
<p>A Source defines the origin of a repository containing the desired state of the system and the requirements to obtain it (e.g. credentials, version selectors). For example, the latest 1.x tag available from a Git repository over SSH.</p>
</blockquote>
<blockquote>
<p>All sources are specified as Custom Resources in a Kubernetes cluster, examples of sources are GitRepository, OCIRepository, HelmRepository and Bucket resources.</p>
</blockquote>
<p><a href="https://fluxcd.io/flux/concepts/#sources">https://fluxcd.io/flux/concepts/#sources</a></p>
<h2 id="weave-ui">Weave UI</h2>
<p>Follow this guide</p>
<p><a href="https://docs.gitops.weave.works/docs/installation/weave-gitops/">https://docs.gitops.weave.works/docs/installation/weave-gitops/</a></p>
<ul>
<li>sources are located in the clusters directory in a monorepo structure</li>
</ul>
<h2 id="repo">Repo</h2>
<p>Decided to fully commit to Flux and their practices.</p>
<p>Set up the repo according to this guide:</p>
<p><a href="https://fluxcd.io/flux/guides/repository-structure/">https://fluxcd.io/flux/guides/repository-structure/</a></p>
<p>And following this example:</p>
<p><a href="https://github.com/fluxcd/flux2-kustomize-helm-example">https://github.com/fluxcd/flux2-kustomize-helm-example</a></p>
<h2 id="links">Links:</h2>
<p>202312261612</p>
<p><a href="https://kubernetes.io/docs/concepts/architecture/controller/">https://kubernetes.io/docs/concepts/architecture/controller/</a></p>
<p><a href="https://youtu.be/BtuqzsyztBc">https://youtu.be/BtuqzsyztBc</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Learned Cilium Network Policies</title>
      <link>https://mischavandenburg.com/zet/cilium-network-policies/</link>
      <pubDate>Sun, 14 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/cilium-network-policies/</guid>
      <description>Today I learned about Cilium network policies. These are much easier to implement than normal network policies because there are some tools available when creating the cilium policies. Network policies were probably my weakest Kubernetes skill and I tended to avoid them.
But now I&amp;rsquo;m exposing some apps to the internet in my homelab and I&amp;rsquo;m forced to think about security and what would happen if a hacker managed to get root privileges in a container even though I implemented strict security contests and enabled privilege escalation.</description>
      <content:encoded><![CDATA[<p>Today I learned about Cilium network policies. These are much easier to implement than normal network policies because there are some tools available when creating the cilium policies. Network policies were probably my weakest Kubernetes skill and I tended to avoid them.</p>
<p>But now I&rsquo;m exposing some apps to the internet in my homelab and I&rsquo;m forced to think about security and what would happen if a hacker managed to get root privileges in a container even though I implemented strict security contests and enabled privilege escalation.</p>
<p>I installed Cilium as the Container Networking Interface in my homelab cluster. This also allows me to use the Hubble tool. This has proven to be of immmense value to see the traffic that is actually flowing around in the namespace you&rsquo;re working on.</p>
<p><img loading="lazy" src="/npolicy1.png" type="" alt=""  /></p>
<p>Even with a small application of a few microservices, as the image shows, the traffic that flows between them and which needs to come from other namespaces can get pretty complex. But in the Hubble UI you get a clear view of what&rsquo;s happening, and you can see exactly when packets are being dropped.</p>
<p>I also found the <a href="https://editor.networkpolicy.io/">Network Policy Editor</a> extremely valuable. It is such a relief to be able to compose the policy from a graphical interface, even though I&rsquo;m a CLI guy and want to do everything in and from code, this has been a gamechanger.</p>
<p>Now I have created the following policy for my linkding app, which allows the EDB operator to do its thing on my databases, Prometheus to monitor my application, but a hacker could not do anything outside of the namespace if they managed to break out.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">cilium.io/v2</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">CiliumNetworkPolicy</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">linkding-app</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">linkding</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">endpointSelector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">policy-type</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;app&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">ingress</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">fromEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- {}<span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">fromEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">io.kubernetes.pod.namespace</span><span class="p">:</span><span class="w"> </span><span class="l">monitoring</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">fromEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">io.kubernetes.pod.namespace</span><span class="p">:</span><span class="w"> </span><span class="l">postgresql-operator-system</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">egress</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">toEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">io.kubernetes.pod.namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kube-system</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">k8s-app</span><span class="p">:</span><span class="w"> </span><span class="l">kube-dns</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">toPorts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;53&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">              </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">UDP</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">rules</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">dns</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">              </span>- <span class="nt">matchPattern</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;*&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">toEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- {}<span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">toEntities</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="l">world</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">toPorts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;443&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;80&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;7844&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">cilium.io/v2</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">CiliumNetworkPolicy</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">linkding-database</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">linkding</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">endpointSelector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">policy-type</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;database&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">ingress</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">fromEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- {}<span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">fromEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">io.kubernetes.pod.namespace</span><span class="p">:</span><span class="w"> </span><span class="l">monitoring</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">fromEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">io.kubernetes.pod.namespace</span><span class="p">:</span><span class="w"> </span><span class="l">postgresql-operator-system</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">egress</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">toEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- {}<span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">toEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">io.kubernetes.pod.namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kube-system</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">k8s-app</span><span class="p">:</span><span class="w"> </span><span class="l">kube-dns</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">toPorts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;53&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">              </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">UDP</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">rules</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">dns</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">              </span>- <span class="nt">matchPattern</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;*&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">toEntities</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="l">kube-apiserver</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">toPorts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;6443&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">toEntities</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="l">world</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">toPorts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;443&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">cilium.io/v2</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">CiliumNetworkPolicy</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">deny-all</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">linkding</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">endpointSelector</span><span class="p">:</span><span class="w"> </span>{}<span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">ingress</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- {}<span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">egress</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- {}<span class="w">
</span></span></span></code></pre></div><h2 id="links">Links:</h2>
<p>202401142001</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video notes - Application Gateway for Containers</title>
      <link>https://mischavandenburg.com/zet/application-gateway-for-containers/</link>
      <pubDate>Fri, 05 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/application-gateway-for-containers/</guid>
      <description>App Gateway It has App Gateway in the name, but it is an entirely new solution. The App Gateway is the only thing it has in common with Azure Application Gateway.
Resources Two types of resources. Azure resources and k8s resources.
The App Gateway for Container is an azure resource which listens to changes in k8s resources through the ALB controller. AGWFC is the control plane.
Frontend Azure Frontend is a public IP and fqdn.</description>
      <content:encoded><![CDATA[
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/slCjHV8z9Wk" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<h1 id="app-gateway">App Gateway</h1>
<p>It has App Gateway in the name, but it is an entirely new solution. The App Gateway is the only thing it has in common with Azure Application Gateway.</p>
<h1 id="resources">Resources</h1>
<p>Two types of resources. Azure resources and k8s resources.</p>
<p>The App Gateway for Container is an azure resource which listens to changes in k8s resources through the ALB controller. AGWFC is the control plane.</p>
<h1 id="frontend">Frontend</h1>
<h2 id="azure">Azure</h2>
<p>Frontend is a public IP and fqdn. Both are managed resources, you don&rsquo;t see them in your subscription.</p>
<p>You can have multiple frontends in one gateway to save money. Teams could share the app gateway but use different frontend IP addresses or FQDNs.</p>
<p>Control plane: Azure App Gateway for Containers</p>
<p>Data plane: association with kubernetes pods.</p>
<p>The association is made to the subnet in the Azure VNet.</p>
<p>Each association is in one subnet, and the subnet should at least have /24 or 256 addresses.</p>
<h1 id="kubernetes">Kubernetes</h1>
<p>ALB controller consists of two pods. Controller pod and a bootstrap pod.</p>
<p>Controller communicates to the Azure gateway resource. It talks directly to the App Gateway, not to the Azure Resource Manager, which is why you&rsquo;re able to have sub-second updates.</p>
<p>The bootstrap contains the CRDs etc, it does not do very much.</p>
<h2 id="creating-resources">Creating resources</h2>
<p>There is a managed option that will talk to ARM and create the resources for you. Or you can choose to deploy them yourself. It depends whether you want to control everything from Kubernetes. If you have all your Azure resources in Infrastructure as Code it probably makes more sense to create the App Gateway resources from there instead of from Kubernetes.</p>
<h1 id="association">Association</h1>
<p>Links frontend with a subnet in a VNet. This will typically be the same vnet that the AKS cluster is in. Could technically be a peered VNet but probably uncommon.</p>
<p>This is an Azure resource. It lives in the VNet and handles TLS and makes the connections to and from the pods and frontend IP. This is the data path.</p>
<p>The traffic is not routed within the cluster, but in Azure by the association.</p>
<p>Client talks to the front end, passes to the association, the association is doing the work, and then routing it to the cluster.</p>
<h1 id="support">Support</h1>
<p>Azure CNI. Does not support kubenet or Azure CNI overlay yet, but it will support in the future.</p>
<h1 id="kubernetes-resources">Kubernetes Resources</h1>
<p>Application Load Balancer: this name was chosen because k8s people don&rsquo;t know the concept of app gateway.</p>
<h1 id="benefits">Benefits</h1>
<ul>
<li>ssl offloading</li>
<li>traffic splitting</li>
<li>clearer separation between platform team and app team</li>
<li>platform team manages the gateway itself</li>
<li>automatic default health probes</li>
</ul>
<h1 id="benefits-of-azure">Benefits of Azure</h1>
<ul>
<li>not suing cluster resources for load balancing. this happens in azure</li>
<li>native azure metrics available</li>
</ul>
<h1 id="misc">misc</h1>
<ul>
<li>AGIC does not support ingress</li>
</ul>
<h2 id="links">Links:</h2>
<p>202309301009</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: Homelab E2 - Setting Up Monitoring &#43; Studying k3s Networking  &amp; Configuring Ingress</title>
      <link>https://mischavandenburg.com/zet/video-homelab-3-k3s-networking/</link>
      <pubDate>Thu, 04 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/video-homelab-3-k3s-networking/</guid>
      <description>In this video I installed Prometheus and Grafana using helm and studied k3s networking.
My goal was to make Grafana approachable via ingress using a fake domain and after a bit of tinkering it worked.
installed prometheus and grafana with kube-prometheus-stack helm chart reflected on why I use k3s gained understanding of k3s loadbalancing solution configured /etc/hosts file to resolve to fake domain configured k3s ingress to use fake local domain struggled with ingress but figured it out in the end successfully made grafana UI available on fake local domain grafana.</description>
      <content:encoded><![CDATA[
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/JjIB65CVXAo" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<p>In this video I installed Prometheus and Grafana using helm and studied k3s networking.</p>
<p>My goal was to make Grafana approachable via ingress using a fake domain and after a bit of tinkering it worked.</p>
<ul>
<li>installed prometheus and grafana with kube-prometheus-stack helm chart</li>
<li>reflected on why I use k3s</li>
<li>gained understanding of k3s loadbalancing solution</li>
<li>configured /etc/hosts file to resolve to fake domain</li>
<li>configured k3s ingress to use fake local domain</li>
<li>struggled with ingress but figured it out in the end</li>
<li>successfully made grafana UI available on fake local domain grafana.homelab.nl</li>
</ul>
<h2 id="links">Links:</h2>
<p>202312261012</p>
<p><a href="https://youtu.be/JjIB65CVXAo">https://youtu.be/JjIB65CVXAo</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: Homelab Episode 1</title>
      <link>https://mischavandenburg.com/zet/homelab-episode-1/</link>
      <pubDate>Sun, 31 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/homelab-episode-1/</guid>
      <description>This is the first video of my homelab series.
I set up the repo for my project and do the initial installation of k3s on an old laptop I had lying around. I had a k8s cluster installed on there with kubeadm which I needed to clean up first.
Links: 202312301912
https://youtu.be/X40gNPZ2xP4
[[homelab]]</description>
      <content:encoded><![CDATA[
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/X40gNPZ2xP4" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<p>This is the first video of my homelab series.</p>
<p>I set up the repo for my project and do the initial installation of k3s on an old laptop I had lying around. I had a k8s cluster installed on there with kubeadm which I needed to clean up first.</p>
<h2 id="links">Links:</h2>
<p>202312301912</p>
<p><a href="https://youtu.be/X40gNPZ2xP4">https://youtu.be/X40gNPZ2xP4</a></p>
<p>[[homelab]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Discovered a simple CLI to backup and upload Grafana dashboards</title>
      <link>https://mischavandenburg.com/zet/grafana-gdg-backup-cli/</link>
      <pubDate>Sat, 30 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/grafana-gdg-backup-cli/</guid>
      <description>This is a super useful tool to list, download and upload Grafana dashboards as json.
Currently using this to occasionally take extra-extra backups of my Grafana creations
https://software.es.net/gdg/
Links: 202312301612</description>
      <content:encoded><![CDATA[<p>This is a super useful tool to list, download and upload Grafana dashboards as json.</p>
<p>Currently using this to occasionally take extra-extra backups of my Grafana creations</p>
<p><a href="https://software.es.net/gdg/">https://software.es.net/gdg/</a></p>
<h2 id="links">Links:</h2>
<p>202312301612</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Homelab Secret Management With GitOps and Azure Key Vault</title>
      <link>https://mischavandenburg.com/zet/handling-secrets-kubernetes/</link>
      <pubDate>Fri, 29 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/handling-secrets-kubernetes/</guid>
      <description>In this blog post, I want to share with you how I set up secrets management for my home lab. I use my home lab to explore new technologies, but I also try to keep it in line with the practices I would use when setting up environments for clients. I focus on Microsoft Azure and the ecosystem they provide for cloud native applications. Secrets management is an important aspect of any cloud-native application, as it allows you to securely store and access sensitive information such as passwords, tokens and certificates.</description>
      <content:encoded><![CDATA[<p>In this blog post, I want to share with you how I set up secrets management for my <a href="https://github.com/mischavandenburg/homelab/">home lab</a>. I use my home lab to explore new technologies, but I also try to keep it in line with the practices I would use when setting up environments for clients. I focus on Microsoft Azure and the ecosystem they provide for cloud native applications. Secrets management is an important aspect of any cloud-native application, as it allows you to securely store and access sensitive information such as passwords, tokens and certificates.</p>
<p>Since Flux is the integrated solution for GitOps in Azure Kubernetes Service I also adopted it for my home lab. Flux supports and recommends encrypting secrets in git using SOPS, which is a tool that uses asymmetric encryption to protect secrets. This means that you can store encrypted secrets in your git repository and only decrypt them when they are applied to the cluster. This sounds like a convenient and secure way to manage secrets, right?</p>
<p>Well, not quite. After doing some research, I decided to use a different approach: storing my secrets in Azure Key Vault and syncing them to my cluster with the Azure Key Vault Provider. This is the recommended best practice by Microsoft, as explained in this article:</p>
<blockquote>
<p>TLDR: Referencing secrets in an external key vault is the recommended approach. It is easier to orchestrate secret rotation and more scalable with multiple clusters and/or teams.</p>
</blockquote>
<p><a href="https://microsoft.github.io/code-with-engineering-playbook/continuous-delivery/gitops/secret-management/secret-management-gitops/">https://microsoft.github.io/code-with-engineering-playbook/continuous-delivery/gitops/secret-management/secret-management-gitops/</a></p>
<p>SOPS has some drawbacks that made me choose the Azure Key Vault route for my home lab. First of all, SOPS requires you to encrypt each secret value manually in the command line, which can be tedious and error-prone. Secondly, SOPS relies on a single encryption key that is stored in your local machine or in a cloud KMS. If you lose this key, you will lose access to all your encrypted secrets in your Git repo. Thirdly, SOPS does not provide a backup or recovery mechanism for your secrets, nor does it offer fine-grained access control or auditing capabilities.</p>
<p>Here are some of the reasons why I chose this approach over SOPS:</p>
<ul>
<li>Secrets are stored in an external source, which means that they are not exposed in git, even if encrypted.</li>
<li>Azure Key Vault provides backup and recovery features, such as soft delete and restore, which can help in case of accidental deletion or corruption of secrets.</li>
<li>Azure Key Vault allows fine-grained authentication and authorization to the vaults, using Entra ID identities and policies. This means that I can control who can access or modify my secrets, and audit their actions.</li>
<li>I don&rsquo;t need to encrypt or decrypt values in the command line, which can be error-prone or leak information. I can use the Azure CLI or the Azure Portal to manage my secrets in the vault.</li>
<li>If I lose my encryption key, all my encrypted secrets in git are useless with SOPS. With Azure Key Vault, I can still access my secrets using my Entra ID or recover them from backup.</li>
<li>This approach mimics a solution that I would use for an enterprise production environment</li>
</ul>
<p>However, this does not mean that I will not use SOPS at all. Sometimes Helm charts or other configurations need values to be hardcoded due to incorrect implementation. This is where SOPS truly shines. You can safely check in your secret values to source code. Additionally, the SOPS keys can also be stored in Azure Key Vault. Tools should be used where they are most useful, so I&rsquo;m not saying I won&rsquo;t use SOPS at all, but I&rsquo;m choosing Azure Key Vaults and the accompanying CSI Store provider for my main solution.</p>
<h1 id="azure-key-vault-provider">Azure Key Vault Provider</h1>
<p>To sync my secrets from Azure Key Vault to my AKS cluster, I used the Azure Key Vault Provider for Secrets Store CSI Driver. This is an open source project that enables you to mount secrets from external sources (such as Azure Key Vault) as volumes in your pods using the Container Storage Interface (CSI) specification.</p>
<p>I followed <a href="https://azure.github.io/secrets-store-csi-driver-provider-azure/docs/">the provider documentation</a> to install the provider in my local cluster using a Flux HelmRelease.</p>
<p>With the provider installed I can use the following configuration to sync a secret named grafana-admin-password from my Key Vault and make it available as a volume and environment variable in an example busybox pod:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">secrets-store.csi.x-k8s.io/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">SecretProviderClass</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">azure-kv-secrets</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">monitoring</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">provider</span><span class="p">:</span><span class="w"> </span><span class="l">azure</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">parameters</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">keyvaultName</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;mischa-homelab-k8s&#34;</span><span class="w"> </span><span class="c"># the name of the KeyVault</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">objects</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">      array:
</span></span></span><span class="line"><span class="cl"><span class="sd">        - |
</span></span></span><span class="line"><span class="cl"><span class="sd">          objectName: grafana-admin-password
</span></span></span><span class="line"><span class="cl"><span class="sd">          objectType: secret              # object types: secret, key, or cert</span><span class="w">      
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">tenantId</span><span class="p">:</span><span class="w"> </span><span class="l">6ddecc48-41b1-48de-bfde-2efd29fae9c7</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">secretObjects</span><span class="p">:</span><span class="w"> </span><span class="c"># [OPTIONAL] SecretObjects defines the desired state of synced Kubernetes secret objects</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">data</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">password</span><span class="w"> </span><span class="c"># data field to populate</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">objectName</span><span class="p">:</span><span class="w"> </span><span class="l">grafana-admin-password</span><span class="w"> </span><span class="c"># name of the mounted content to sync; this could be the object name or the object alias</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">secretName</span><span class="p">:</span><span class="w"> </span><span class="l">grafana-custom-secret</span><span class="w"> </span><span class="c"># name of the Kubernetes secret object</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Opaque</span><span class="w"> </span><span class="c"># type of Kubernetes secret object (for example, Opaque, kubernetes.io/tls)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">busybox-secrets-store-inline</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">monitoring</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">busybox</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">registry.k8s.io/e2e-test-images/busybox:1.29-1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">command</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="s2">&#34;/bin/sleep&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="s2">&#34;10000&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">secrets-store01-inline</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;/mnt/secrets-store&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">readOnly</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">env</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">SECRET_USERNAME</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">valueFrom</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">secretKeyRef</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">              </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">grafana-custom-secret</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">              </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">password</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">secrets-store01-inline</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">csi</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">driver</span><span class="p">:</span><span class="w"> </span><span class="l">secrets-store.csi.k8s.io</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">readOnly</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">volumeAttributes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">secretProviderClass</span><span class="p">:</span><span class="w"> </span><span class="l">azure-kv-secrets</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">nodePublishSecretRef</span><span class="p">:</span><span class="w"> </span><span class="c"># Only required when using service principal mode</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">secrets-store-creds</span><span class="w"> </span><span class="c"># Only required when using service principal mode. The name of the Kubernetes secret that contains the service principal credentials to access keyvault.</span><span class="w">
</span></span></span></code></pre></div><p>I hope you found this blog post useful and informative. If you have any questions or feedback, please feel free to leave a comment below or contact me on Twitter @mischa_vdburg</p>
<h2 id="links">Links:</h2>
<p>202312290912</p>
<p><a href="https://microsoft.github.io/code-with-engineering-playbook/continuous-delivery/gitops/secret-management/secret-management-gitops/">https://microsoft.github.io/code-with-engineering-playbook/continuous-delivery/gitops/secret-management/secret-management-gitops/</a></p>
<p><a href="https://github.com/mischavandenburg/homelab/">https://github.com/mischavandenburg/homelab/</a></p>
<ul>
<li><a href="https://docs.microsoft.com/en-us/azure/key-vault/">Azure Key Vault documentation</a></li>
<li><a href="https://fluxcd.io/docs/">Flux documentation</a></li>
<li><a href="https://learn.microsoft.com/en-us/azure/aks/csi-secrets-store-driver">Azure Key Vault Provider for Secrets Store CSI Driver documentation</a></li>
</ul>
<p><a href="https://azure.github.io/secrets-store-csi-driver-provider-azure/docs/">https://azure.github.io/secrets-store-csi-driver-provider-azure/docs/</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: Viewing Strava Data In Grafana Dashboards</title>
      <link>https://mischavandenburg.com/zet/video-strava-grafana/</link>
      <pubDate>Wed, 27 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/video-strava-grafana/</guid>
      <description>In this video I&amp;rsquo;ll show you how to view your Strava data in Grafana by doing the following steps:
creating an application in Strava installing the Strava plugin in Grafana adding the Strava data source importing dashboards exploring data and creating custom dashboard Links: 202312250812
https://youtu.be/CgP9hs9UDzA</description>
      <content:encoded><![CDATA[
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/CgP9hs9UDzA" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<p>In this video I&rsquo;ll show you how to view your Strava data in Grafana by doing the following steps:</p>
<ul>
<li>creating an application in Strava</li>
<li>installing the Strava plugin in Grafana</li>
<li>adding the Strava data source</li>
<li>importing dashboards</li>
<li>exploring data and creating custom dashboard</li>
</ul>
<h2 id="links">Links:</h2>
<p>202312250812</p>
<p><a href="https://youtu.be/CgP9hs9UDzA">https://youtu.be/CgP9hs9UDzA</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: How To Install Prometheus &amp; Grafana In Your Homelab</title>
      <link>https://mischavandenburg.com/zet/video-install-prometheus-grafana-homelab/</link>
      <pubDate>Mon, 25 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/video-install-prometheus-grafana-homelab/</guid>
      <description>In this video I&amp;rsquo;ll be installing Prometheus and Grafana in a Kubernetes cluster running in Rancher Desktop on my MacBook.
There are many options available out there but this is the easiest one I found to get up and running quickly.
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install prometheus-stack prometheus-community/kube-prometheus-stack --namespace=prometheus-stack --create-namespace
helm show values prometheus-community/kube-prometheus-stack &amp;gt; prometheus-default-values.yaml
Opening the Grafana UI k port-forward svc/prometheus-stack-grafana 3000:80</description>
      <content:encoded><![CDATA[
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/3AINqaBwOYs" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<p>In this video I&rsquo;ll be installing Prometheus and Grafana in a Kubernetes cluster running in Rancher Desktop on my MacBook.</p>
<p>There are many options available out there but this is the easiest one I found to get up and running quickly.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
</span></span><span class="line"><span class="cl">helm repo update
</span></span></code></pre></div><p><code>helm install prometheus-stack prometheus-community/kube-prometheus-stack --namespace=prometheus-stack --create-namespace</code></p>
<p><code>helm show values prometheus-community/kube-prometheus-stack &gt; prometheus-default-values.yaml</code></p>
<h2 id="opening-the-grafana-ui">Opening the Grafana UI</h2>
<p><code>k port-forward svc/prometheus-stack-grafana 3000:80</code></p>
<p>Then you can open it by entering <code>localhost:3000</code> in your browser.</p>
<p>The default credentials are admin:prom-operator</p>
<h2 id="links">Links:</h2>
<p>202312250812</p>
<p><a href="https://youtu.be/3AINqaBwOYs?si=maN1rfbg4pzBc3xI">https://youtu.be/3AINqaBwOYs?si=maN1rfbg4pzBc3xI</a></p>
<p><a href="https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack#configuration">https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack#configuration</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Talk: Avoiding Microservice Megadisasters by Jimmy Bogard</title>
      <link>https://mischavandenburg.com/zet/talk-avoiding-microservice-disasters/</link>
      <pubDate>Sat, 23 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/talk-avoiding-microservice-disasters/</guid>
      <description>Watched this very insightful talk on microservice architecture.
Some things I learned:
microservices should be autonomous: they should have minimal dependencies on other microservices if they have dependencies they should only be 1 layer deep a microservice should not be calling another microservice which calls another microservice dependencies can be reversed by pushing data towards the service for example: a pricing database can be dumped and pushed to a catalog service once a day data duplication is not a sin Another powerful point he made is that the architecture is a reflection from the organization&amp;rsquo;s structure.</description>
      <content:encoded><![CDATA[<p>Watched this very insightful talk on microservice architecture.</p>
<p>Some things I learned:</p>
<ul>
<li>microservices should be <strong>autonomous</strong>: they should have minimal dependencies on other microservices</li>
<li>if they have dependencies they should only be 1 layer deep
<ul>
<li>a microservice should not be calling another microservice which calls another microservice</li>
</ul>
</li>
<li>dependencies can be reversed by pushing data towards the service
<ul>
<li>for example: a pricing database can be dumped and pushed to a catalog service once a day</li>
<li>data duplication is not a sin</li>
</ul>
</li>
</ul>
<p>Another powerful point he made is that the architecture is a reflection from the organization&rsquo;s structure. He explained that the company was organized in teams and that managers of teams were promoted based on the amount of people they managed. This meant that managers were making up services which needed people to build them, which led to a proliferation of services and dependencies.</p>
<p>In order to have a correct software strucutre, the organization must be organized correctly first.</p>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/gfh-VCTwMw8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<h2 id="links">Links:</h2>
<p>202312231912</p>
<p><a href="https://youtu.be/gfh-VCTwMw8?si=QqHcbG_5ezx5qJX2">https://youtu.be/gfh-VCTwMw8?si=QqHcbG_5ezx5qJX2</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Ensure Cgroupsv2 compatiblity when containerizing old apps</title>
      <link>https://mischavandenburg.com/zet/ensure-cgroupsv2-compatibility/</link>
      <pubDate>Tue, 12 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/ensure-cgroupsv2-compatibility/</guid>
      <description>Are you currently working with containerizing older Java or .NET applications? From Kubernetes 1.29, the default cgroups implementation on Azure Linux AKS nodes will be cgroupsv2. Older versions of Java, .NET and NodeJS do not support memory querying v2 memory constraints and this will lead to out of memory (OOM) issues for workloads.
Please make sure that your older containerized applications are compatible with cgroupsv2 or you might be in quite some pain in the future.</description>
      <content:encoded><![CDATA[<p>Are you currently working with containerizing older Java or .NET applications? From Kubernetes 1.29, the default cgroups implementation on Azure Linux AKS nodes will be cgroupsv2. Older versions of Java, .NET and NodeJS do not support memory querying v2 memory constraints and this will lead to out of memory (OOM) issues for workloads.</p>
<p>Please make sure that your older containerized applications are compatible with cgroupsv2 or you might be in quite some pain in the future.</p>
<p><a href="https://github.com/Azure/AKS/releases/tag/2023-11-28">https://github.com/Azure/AKS/releases/tag/2023-11-28</a></p>
<h2 id="links">Links:</h2>
<p>202312120812</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>What&#39;s so hard in Kubernetes?</title>
      <link>https://mischavandenburg.com/zet/whats-so-hard-about-kubernetes/</link>
      <pubDate>Sun, 10 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/whats-so-hard-about-kubernetes/</guid>
      <description>Read an interesting Reddit thread today where commenters explained the complexities of running Kubernetes. Indeed it is easy to set up, but that is only the beginning.
The real art is to build distributed systems on top of it, and it is dauntingly complex. That&amp;rsquo;s why I&amp;rsquo;m so attracted to it.
It does make clear just how great Azure&amp;rsquo;s AKS offering actually is. So much of the administrative overhead is automated, and so far I&amp;rsquo;m under the impression it is done really well.</description>
      <content:encoded><![CDATA[<p>Read an interesting Reddit thread today where commenters explained the complexities of running Kubernetes. Indeed it is easy to set up, but that is only the beginning.</p>
<p>The real art is to build distributed systems on top of it, and it is dauntingly complex. That&rsquo;s why I&rsquo;m so attracted to it.</p>
<p>It does make clear just how great Azure&rsquo;s AKS offering actually is. So much of the administrative overhead is automated, and so far I&rsquo;m under the impression it is done really well. Which is why I&rsquo;m on a mission to become an AKS expert.</p>
<p><a href="https://www.reddit.com/r/devops/comments/18e4nuw/whats_so_hard_in_kubernetes/?utm_source=share&amp;utm_medium=web2x&amp;context=3">https://www.reddit.com/r/devops/comments/18e4nuw/whats_so_hard_in_kubernetes/?utm_source=share&amp;utm_medium=web2x&amp;context=3</a></p>
<h2 id="links">Links:</h2>
<p>202312101012</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Deploying Grafana Agent With Custom Secrets From Azure Key Vault Using Akv2k8s And K8s-Monitoring Helm Chart</title>
      <link>https://mischavandenburg.com/zet/grafana-agent-with-custom-secrets-akv2k8s/</link>
      <pubDate>Tue, 28 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/grafana-agent-with-custom-secrets-akv2k8s/</guid>
      <description>Grafana has developed a Helm chart which greatly simplifies the deployment of a monitoring stack to your Kubernetes clusters.
It contains:
kube-state-metrics, which gathers metrics about Kubernetes objects Node exporter, which gathers metrics about Kubernetes nodes OpenCost, which interprets the above to create cost metrics for the cluster, and Grafana Agent, which scrapes the above services to forward metrics to Prometheus and logs to Loki The Prometheus and Loki services may be hosted on the same cluster, or remotely (e.</description>
      <content:encoded><![CDATA[<p>Grafana has developed a Helm chart which greatly simplifies the deployment of a monitoring stack to your Kubernetes clusters.</p>
<p>It contains:</p>
<ul>
<li>kube-state-metrics, which gathers metrics about Kubernetes objects</li>
<li>Node exporter, which gathers metrics about Kubernetes nodes</li>
<li>OpenCost, which interprets the above to create cost metrics for the cluster, and</li>
<li>Grafana Agent, which scrapes the above services to forward metrics to Prometheus and logs to Loki</li>
</ul>
<p>The Prometheus and Loki services may be hosted on the same cluster, or remotely (e.g. on Grafana Cloud).</p>
<p>For my current project I&rsquo;m setting it up to a Grafana Cloud stack, but as stated above it can also be used with a local or other remote Prometheus instance.</p>
<h1 id="akv2k8s">akv2k8s</h1>
<p>We use akv2k8s on our clusters to synch secrets from Azure Key Vaults to Kubernetes secrets or injecting them as environment variables. This way we can prevent checking in secrets to our code.</p>
<p>However, when I first tried to deploy this chart, external secrets were not supported. They have now been fixed by Pete Wall and I managed to deploy the k8s-monitoring by writing a wrapper Helm chart with the external secret objects.</p>
<h1 id="code">code</h1>
<p>All code is available in my <a href="https://github.com/mischavandenburg/lab/tree/main/kubernetes/k8smonitoring-secrets">lab repo</a>. I wrote the following Chart.yaml:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v2</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">k8s-monitoring</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="m">1.0.0</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="l">This chart deploys the k8s-monitoring chart with custom secrets</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">dependencies</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">k8s-monitoring</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="m">0.5.1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">repository</span><span class="p">:</span><span class="w"> </span><span class="l">https://grafana.github.io/helm-charts/</span><span class="w">
</span></span></span></code></pre></div><p>Then I created a templates directory and added secrets.yaml:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">spv.no/v1alpha1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">AzureKeyVaultSecret</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-user</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">vault</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span>{{<span class="w"> </span><span class="l">.Values.keyVault }}</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">object</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-user</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">secret</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">output</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">secret</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">grafana-agent-credentials-akv2k8s</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">dataKey</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-user</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">spv.no/v1alpha1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">AzureKeyVaultSecret</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-password</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">vault</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span>{{<span class="w"> </span><span class="l">.Values.keyVault }}</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">object</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-password</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">secret</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">output</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">secret</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">grafana-agent-credentials-akv2k8s</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">dataKey</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-password</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">spv.no/v1alpha1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">AzureKeyVaultSecret</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-host</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">vault</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span>{{<span class="w"> </span><span class="l">.Values.keyVault }}</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">object</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-host</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">secret</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">output</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">secret</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">grafana-agent-credentials-akv2k8s</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">dataKey</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-host</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span></code></pre></div><p>Then, to use the external secrets in the values file, I used the following configuration:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">keyVault</span><span class="p">:</span><span class="w"> </span><span class="l">kv-123-hello</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">k8s-monitoring</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">cluster</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">aks-123-hello</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">externalServices</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">prometheus</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">hostKey</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-host</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">basicAuth</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">usernameKey</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-user</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">passwordKey</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-password</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">secret</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">create</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">grafana-agent-credentials-akv2k8s</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">k8s-monitoring</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">extraConfig</span><span class="p">:</span><span class="w"> </span><span class="p">|-</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">    logging {
</span></span></span><span class="line"><span class="cl"><span class="sd">      level  = &#34;info&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">      format = &#34;logfmt&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">    }</span><span class="w">    
</span></span></span></code></pre></div><p>This chart can be deployed using <code>helm install</code> but we are using GitOps like the gods intended. When this is configured in ArgoCD, the chart will be deployed with the secrets templates. These templates will retreive the secrets from the Azure Key Vault and synch them to the <code>grafana-agent-credentials-akv2k8s</code> secret.</p>
<h1 id="bug">bug</h1>
<p>Althoug this might have been a slight bug during the first implementation of this chart, I did run into the following error in my first attempts. The logs of the Grafana Agent showed:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">unsupported protocol scheme <span class="se">\&#34;\&#34;</span><span class="s2">&#34;
</span></span></span></code></pre></div><p>After a lot of debugging I decided to completely rebuild the deployment from scratch, and then I found out that I had put the externalServices.prometheus and externalServices.secret objects in a different order which messed up the rest of the values file. <strong>It is important to keep the order that is given in the values.yaml in the k8s-monitoring repo.</strong></p>
<h2 id="links">Links:</h2>
<p>Helm chart:</p>
<p><a href="https://github.com/grafana/k8s-monitoring-helm/tree/main/charts/k8s-monitoring">https://github.com/grafana/k8s-monitoring-helm/tree/main/charts/k8s-monitoring</a></p>
<p>GitHub issue:</p>
<p><a href="https://github.com/grafana/k8s-monitoring-helm/issues/81#issuecomment-1828771508">https://github.com/grafana/k8s-monitoring-helm/issues/81#issuecomment-1828771508</a></p>
<p>Link to example code in my lab repo:</p>
<p><a href="https://github.com/mischavandenburg/lab/tree/main/kubernetes/k8smonitoring-secrets">https://github.com/mischavandenburg/lab/tree/main/kubernetes/k8smonitoring-secrets</a></p>
<p>202311281011</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Implementing Pod Disruption Budgets</title>
      <link>https://mischavandenburg.com/zet/implementing-poddisruptionbudgets/</link>
      <pubDate>Mon, 27 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/implementing-poddisruptionbudgets/</guid>
      <description>When I was doing the first round of AKS cluster upgrades at my current client, I noticed we were running a lot of pods with only 1 replica. I always try to lift my clients to the next level by leveraging Cloud Native technologies as much as possible. I&amp;rsquo;m therefore starting a project to always run applications with multiple replicas.
However, running multiple replicas is not the only necessary improvement here.</description>
      <content:encoded><![CDATA[<p>When I was doing the first round of AKS cluster upgrades at my current client, I noticed we were running a lot of pods with only 1 replica. I always try to lift my clients to the next level by leveraging Cloud Native technologies as much as possible. I&rsquo;m therefore starting a project to always run applications with multiple replicas.</p>
<p>However, running multiple replicas is not the only necessary improvement here. Even though a pod is running with multiple replicas, that does not mean that Kubernetes will always keep them alive. When you do an AKS cluster upgrade, nodes are drained one by one and the pods are moved to a node with the higher k8s version. Technically, when draining a node, Kubernetes could kill both of the pods at the same time if they are running on the same node.</p>
<p>To prevent this we use <a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/">pod disruption budgets</a></p>
<h1 id="helm">Helm</h1>
<p>We&rsquo;re using a Helm chart to deploy with ArgoCD. I&rsquo;m adding this to the templates directory:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">policy/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">PodDisruptionBudget</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">pyramid-backend-pdb</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">minAvailable</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">tier</span><span class="p">:</span><span class="w"> </span><span class="l">backend</span><span class="w">
</span></span></span></code></pre></div><p>I&rsquo;m selecing all the pods that have the label &ldquo;tier: backend&rdquo;.</p>
<p>To verify that I&rsquo;m targeting the right pods, I run:</p>
<p><code>kubectl get pods -l tier=backend</code></p>
<p>Currently my deployment has only two replicas:</p>
<pre tabindex="0"><code>(ins)$ k get deploy
NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
pyramid-deployment-backend   1/1     1            1           297d
</code></pre><p>So when I check my PDB it will show no allowed disruptions.</p>
<pre tabindex="0"><code>(ins)$ k get pdb
NAME                  MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
pyramid-backend-pdb   1               N/A               0                     13m
</code></pre><p>This is good, this is the expected behaviour. Kubernetes will now prevent me from draining the node because there are no allowed disruptions.</p>
<p>Then I scale up the deployment:</p>
<pre tabindex="0"><code>(ins)$ k scale deploy --replicas=2 pyramid-deployment-backend
deployment.apps/pyramid-deployment-backend scaled
</code></pre><p>And then I get an allowed disruption of 1:</p>
<pre tabindex="0"><code>(ins)$ k get pdb
NAME                  MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
pyramid-backend-pdb   1               N/A               1                     15m
</code></pre><p>With this configuration, Kubernetes will never kill all of the pods simultaneously in case a node needs to be drained. It will make sure that one of the pods keeps running when it is rescheduling pods for an upgrade.</p>
<h2 id="links">Links:</h2>
<p><a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/">https://kubernetes.io/docs/tasks/run-application/configure-pdb/</a></p>
<p>202311271411</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>KEDA went GA for AKS!</title>
      <link>https://mischavandenburg.com/zet/keda-ga/</link>
      <pubDate>Thu, 09 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/keda-ga/</guid>
      <description>Great news! KEDA finally went GA for AKS.
https://learn.microsoft.com/en-us/azure/aks/keda-about
Links: 202311081711</description>
      <content:encoded><![CDATA[<p>Great news! KEDA finally went GA for AKS.</p>
<p><a href="https://learn.microsoft.com/en-us/azure/aks/keda-about">https://learn.microsoft.com/en-us/azure/aks/keda-about</a></p>
<h2 id="links">Links:</h2>
<p>202311081711</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Some notes on upgrading AKS clusters running EnterpriseDB</title>
      <link>https://mischavandenburg.com/zet/edb-cluster-notes/</link>
      <pubDate>Tue, 17 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/edb-cluster-notes/</guid>
      <description>Upgrading clusters with databases When upgrading this type of cluster it is better to upgrade the control plane first and then upgrade the node pools one by one.
The EDB operator prevents you to drain nodes with databases on them with PodDisruptionBudgets.
In order to be able to run the AKS upgrade you will need to set the following:
k cnp maintenance set --all-namespaces --reusePVC
cnp is a kubectl plugin which you need to install to manage the EDB operator --reusePVC can only be used if all the nodes are in the same availability zone use this command to list all the zones: kubectl describe nodes | grep -e &amp;quot;Name:&amp;quot; -e &amp;quot;topology.</description>
      <content:encoded><![CDATA[<h1 id="upgrading-clusters-with-databases">Upgrading clusters with databases</h1>
<p>When upgrading this type of cluster it is better to upgrade the control plane first and then upgrade the node pools one by one.</p>
<p>The EDB operator prevents you to drain nodes with databases on them with PodDisruptionBudgets.</p>
<p>In order to be able to run the AKS upgrade you will need to set the following:</p>
<p><code>k cnp maintenance set --all-namespaces --reusePVC</code></p>
<ul>
<li>cnp is a kubectl plugin which you need to install to manage the EDB operator</li>
<li><code>--reusePVC</code> can only be used if all the nodes are in the same availability zone</li>
<li>use this command to list all the zones: <code>kubectl describe nodes | grep -e &quot;Name:&quot; -e &quot;topology.kubernetes.io/zone&quot;</code></li>
</ul>
<p>When the upgrade is complete, run <code>k cnp maintenance set --all-namespaces</code></p>
<h2 id="getting-the-status">Getting the status</h2>
<p>You can get all information about the database cluster:</p>
<p><code>k cnp status pyramid-cluster-database -n mycommodity</code></p>
<p>Most deployments have the same name pyramid-cluster-database.</p>
<h2 id="problems">Problems</h2>
<p>Sometimes a database replica will not come up properly and you can get the following error message:</p>
<p><code>(FATAL: the database system is starting up (SQLSTATE 57P03))</code></p>
<p>To fix this, first disable reusing PVC by running:</p>
<p><code>k cnp maintenance set -n problemdatabase</code></p>
<p>&ldquo;problemdatabase&rdquo; is the namespace where the database cluster is deployed.</p>
<p>Then delete the PVC of the problematic pod. This should trigger a new replica to be created from scratch. Depending on the size of the DB it can take a lot of time.</p>
<h2 id="links">Links:</h2>
<p>202310171410</p>
<p>[[kubernetes]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Backing up AKS Clusters with Azure Backup is now in preview</title>
      <link>https://mischavandenburg.com/zet/aks-azure-backup-preview/</link>
      <pubDate>Fri, 13 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/aks-azure-backup-preview/</guid>
      <description>You can back up your AKS clusters using Azure Backup in preview! Preferably you have your clusters stateless and you can redeploy everything from code when sh*t hits the fan. However, I can think of a few enterprise use cases that will be relevant for this new feature.
https://learn.microsoft.com/en-us/azure/backup/azure-kubernetes-service-cluster-backup
Links: 202310130610
[[aks]]
[[kubernetes]]
[[azure]]</description>
      <content:encoded><![CDATA[<p>You can back up your AKS clusters using Azure Backup in preview! Preferably you have your clusters stateless and you can redeploy everything from code when sh*t hits the fan. However, I can think of a few enterprise use cases that will be relevant for this new feature.</p>
<p><a href="https://learn.microsoft.com/en-us/azure/backup/azure-kubernetes-service-cluster-backup">https://learn.microsoft.com/en-us/azure/backup/azure-kubernetes-service-cluster-backup</a></p>
<h2 id="links">Links:</h2>
<p>202310130610</p>
<p>[[aks]]</p>
<p>[[kubernetes]]</p>
<p>[[azure]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Fixed an issue with akv2k8s overriding security context</title>
      <link>https://mischavandenburg.com/zet/akv2k8s-edb-initcontainer-securitycontext/</link>
      <pubDate>Fri, 13 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/akv2k8s-edb-initcontainer-securitycontext/</guid>
      <description>This week I&amp;rsquo;ve been tackling an issue at work with akv2k8s.
We are running CloudnativePG, similar to EnterpriseDB. After upgrading akv2k8s to v2.5.0 our database pods were not coming up anymore due to an error with the initcontainer:
Error: container has runAsNonRoot and image has non-numeric user (nonroot), cannot verify user is non-root (pod: &amp;#34;vcs-pooler-rw-858bf7c954-vjzr4_vcs(e19bfa4c-26d8-4a6d-b4ad-bba8b52c01e6)&amp;#34;, container: bootstrap-controller) Pods have a security context where you specify how the containers in the pod should be run.</description>
      <content:encoded><![CDATA[<p>This week I&rsquo;ve been tackling an issue at work with akv2k8s.</p>
<p>We are running CloudnativePG, similar to EnterpriseDB. After upgrading akv2k8s to v2.5.0 our database pods were not coming up anymore due to an error with the initcontainer:</p>
<pre tabindex="0"><code>Error: container has runAsNonRoot and image has non-numeric user (nonroot), cannot verify user is non-root (pod: &#34;vcs-pooler-rw-858bf7c954-vjzr4_vcs(e19bfa4c-26d8-4a6d-b4ad-bba8b52c01e6)&#34;, container: bootstrap-controller)
</code></pre><p>Pods have a security context where you specify how the containers in the pod should be run.</p>
<pre tabindex="0"><code>securityContext:
    runAsUser: 998
    runAsGroup: 996
    runAsNonRoot: true
    fsGroup: 996
</code></pre><p>Here we are telling the pod that we may not run as the root user and that we will run as user 998.</p>
<p>My error states that it cannot verify whether it is running as root or not because we are using a non-numeric user.</p>
<p>We were dumbfounded by this because the deployment and ReplicaSet neatly had their securitycontexts configured and the pods should inherit them. After a lot of searching I figured out that the securitycontext of the pods was actually empty, so somehow it must be overridden somewhere.</p>
<p>The culprit was the akv2k8s envinjector. This tool injects the secrets into the environment of the pods. It took a lot of experimentation but we managed to narrow it down to the akv2k8s upgrade and eventually we found this GitHub issue:</p>
<p><a href="https://github.com/SparebankenVest/azure-key-vault-to-kubernetes/issues/605">https://github.com/SparebankenVest/azure-key-vault-to-kubernetes/issues/605</a></p>
<p>For some reason the envinjector is overriding the securitycontext of the pods. The issue was fixed by downgrading the helm chart back to 2.4.2</p>
<p>I did learn a lot in the process though!</p>
<h2 id="links">Links:</h2>
<p>202310130610</p>
<p><a href="https://www.enterprisedb.com/docs/postgres_for_kubernetes/latest/installation_upgrade/">https://www.enterprisedb.com/docs/postgres_for_kubernetes/latest/installation_upgrade/</a></p>
<p><a href="https://github.com/SparebankenVest/azure-key-vault-to-kubernetes/issues/605">https://github.com/SparebankenVest/azure-key-vault-to-kubernetes/issues/605</a></p>
<p><a href="https://stackoverflow.com/questions/53949329/kubernetes-runasnonroot-failing-createcontainerconfigerror">https://stackoverflow.com/questions/53949329/kubernetes-runasnonroot-failing-createcontainerconfigerror</a></p>
<p><a href="https://github.com/cloudnative-pg/cloudnative-pg">https://github.com/cloudnative-pg/cloudnative-pg</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Forcing a new cert from letsencrypt when too many have been issued</title>
      <link>https://mischavandenburg.com/zet/forcing-new-cert-letsencrypt-timeout/</link>
      <pubDate>Thu, 12 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/forcing-new-cert-letsencrypt-timeout/</guid>
      <description>Learned a new trick to force a new cert from letsencrypt. Had this error message:
The certificate request has failed to complete and will be retried: Failed to wait for order resource &amp;#34;vcs-secret-j8gnj-3121219202&amp;#34; to become ready: order is in &amp;#34;errored&amp;#34; state: Failed to create Order: 429 urn:ietf:params:acme:error:rateLimited: Error creating new order :: too many certificates (5) already issued for this exact set of domains in the last 168 hours: sadfsadf.com, retry after 2023-10-12T18:54:27Z: see https://letsencrypt.</description>
      <content:encoded><![CDATA[<p>Learned a new trick to force a new cert from letsencrypt. Had this error message:</p>
<pre tabindex="0"><code>The certificate request has failed to complete and will be retried: Failed to wait for order resource &#34;vcs-secret-j8gnj-3121219202&#34; to become ready: order is in &#34;errored&#34; state: Failed to create Order: 429 urn:ietf:params:acme:error:rateLimited: Error creating new order :: too many certificates (5) already issued for this exact set of domains in the last 168 hours: sadfsadf.com, retry after 2023-10-12T18:54:27Z: see https://letsencrypt.org/docs/duplicate-certificate-limit/
</code></pre><p>I asked for too many certificates during a 168 hour period. However, there is a really easy fix. Just add a new subdomain and LetsEncrypt will treat it as an entirely new certificate!</p>
<p>For example, when using cert-manager:</p>
<pre tabindex="0"><code>apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  creationTimestamp: &#39;2023-10-12T11:09:36Z&#39;
  generation: 2
  labels:
    argocd.argoproj.io/instance: vcs-dev
  name: vcs-secret
  namespace: vcs
  ownerReferences:
    - apiVersion: networking.k8s.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: Ingress
      name: vcs-ingress
      uid: xxxxx
      resourceVersion: &#39;226558404&#39;
  uid: xxxx
spec:
  dnsNames:
    - vcs.mischavandenburg.com
    - test.mischavandenburg.com
  issuerRef:
    group: cert-manager.io
    kind: ClusterIssuer
    name: letsencrypt-prod
  secretName: vcs-secret
  usages:
    - digital signature
    - key encipherment
</code></pre><p>Just add another domain under spec.dnsNames and it will treat it as a new cert.</p>
<pre tabindex="0"><code>spec:
  dnsNames:
    - vcs.mischavandenburg.com
    - test.mischavandenburg.com
    - hello.mischavandenburg.com
</code></pre><h2 id="links">Links:</h2>
<p>202310121310</p>
<p>[[kubernetes]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Some Interesting Features went GA on Azure</title>
      <link>https://mischavandenburg.com/zet/azure-updates-sept-23/</link>
      <pubDate>Thu, 28 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/azure-updates-sept-23/</guid>
      <description>Yesterday a few interesting AKS related features became Generally Available on Azure.
KEDA add-on makes it easier to scale your applications on AKS cluster.
https://azure.microsoft.com/en-us/updates/generally-available-keda-addon-for-aks/
You can have a flexible and customized strategy for node-level OS security updates.
https://azure.microsoft.com/en-us/updates/ga-node-os-patching-nodeimage-feature-in-aks/
Use Vertical Pod Autoscaling add-on for AKS to improve cost-efficiency, and cluster utilization for your workloads
https://azure.microsoft.com/en-us/updates/ga-vertical-pod-autoscaling-addon-for-aks/
Preview Public preview: AKS support for Kubernetes version 1.28
https://azure.microsoft.com/en-us/updates/public-preview-aks-support-for-kubernetes-version-128/
Links: 202309281009
[[azure]]
[[kubernetes]]</description>
      <content:encoded><![CDATA[<p>Yesterday a few interesting AKS related features became Generally Available on Azure.</p>
<p>KEDA add-on makes it easier to scale your applications on AKS cluster.</p>
<p><a href="https://azure.microsoft.com/en-us/updates/generally-available-keda-addon-for-aks/">https://azure.microsoft.com/en-us/updates/generally-available-keda-addon-for-aks/</a></p>
<p>You can have a flexible and customized strategy for node-level OS security updates.</p>
<p><a href="https://azure.microsoft.com/en-us/updates/ga-node-os-patching-nodeimage-feature-in-aks/">https://azure.microsoft.com/en-us/updates/ga-node-os-patching-nodeimage-feature-in-aks/</a></p>
<p>Use Vertical Pod Autoscaling add-on for AKS to improve cost-efficiency, and cluster utilization for your workloads</p>
<p><a href="https://azure.microsoft.com/en-us/updates/ga-vertical-pod-autoscaling-addon-for-aks/">https://azure.microsoft.com/en-us/updates/ga-vertical-pod-autoscaling-addon-for-aks/</a></p>
<h1 id="preview">Preview</h1>
<p>Public preview: AKS support for Kubernetes version 1.28</p>
<p><a href="https://azure.microsoft.com/en-us/updates/public-preview-aks-support-for-kubernetes-version-128/">https://azure.microsoft.com/en-us/updates/public-preview-aks-support-for-kubernetes-version-128/</a></p>
<h2 id="links">Links:</h2>
<p>202309281009</p>
<p>[[azure]]</p>
<p>[[kubernetes]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Pod Disruption Budgets Can Mess With Your AKS Updates</title>
      <link>https://mischavandenburg.com/zet/pod-disruption-budget-aks/</link>
      <pubDate>Wed, 27 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/pod-disruption-budget-aks/</guid>
      <description>Past week we&amp;rsquo;ve been struggling a bit with poorly configured pod disruption budgets. When you do an AKS upgrade, a new node is created and one of the old nodes is drained.
If a deployment has a pod disruption budget which is incorrectly configured, it might show up as ALLOWED DISRUPTIONS: 0. When this happens, the node cannot be drained and you will get an error message in your events.</description>
      <content:encoded><![CDATA[<p>Past week we&rsquo;ve been struggling a bit with poorly configured pod disruption budgets. When you do an AKS upgrade, a new node is created and one of the old nodes is drained.</p>
<p>If a deployment has a pod disruption budget which is incorrectly configured, it might show up as <code>ALLOWED DISRUPTIONS: 0</code>. When this happens, the node cannot be drained and you will get an error message in your events.</p>
<p><code>k get poddisruptionbudgets.policy</code></p>
<p><code>k get events</code></p>
<p>The error message will say something like &ldquo;Too man eviction attempts, usually a pdb&rdquo; (I lost the shell output so can&rsquo;t copy atm).</p>
<p>Kubernetes is in a situation where it needs to schedule the pod on another node, but it is unable to do do so because we are telling Kubernetes that it is not allowed to have any disruptions on the deployment.</p>
<p>Kubernetes is logical, it&rsquo;s doing like it&rsquo;s told. But it&rsquo;s frustrating because you can be sitting there waiting for 20 minutes wondering why your node isn&rsquo;t draining.</p>
<p>At my current gig we are not responsible for the content on the clusters and we should not meddle with the application teams&rsquo; namespaces.</p>
<p>However, one solution is to either kill the pod manually or scale up the deployment to more replicas so there will be a higher amount of allowed disruptions.</p>
<h2 id="links">Links:</h2>
<p>202309271809</p>
<p>[[kubernetes]]</p>
<p>[[azure]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: Deploying AKS Cluster With Azure CNI Using Bicep</title>
      <link>https://mischavandenburg.com/zet/video-deploy-aks-with-azure-cni/</link>
      <pubDate>Sun, 09 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/video-deploy-aks-with-azure-cni/</guid>
      <description>In this video, I will show you how to use Bicep to deploy a Kubernetes cluster with custom network settings using the Azure CNI.
Azure CNI allows pods to be assigned IP addresses from Azure VNets which allows them to communicate with Azure resources directly through peered networks.
I use Neovim and the Azure CLI for my coding and deployment.
You will learn how to:
Implement dev/test prefix to create multiple clusters with the same template Plan a VNet range for an Azure CNI cluster and be mindful of overlaps Deploy a VNet and subnet for the cluster using Bicep Deploy a cluster with Azure CNI enabled and configure the maximum number of pods per node Validate your Bicep template and troubleshoot errors Explore the results of your deployment in the Azure portal Understand the limitations of Azure CNI and why VNet peering is not supported in my configuration due to overlaps This video is suitable for anyone who wants to learn more about Azure CNI and how to use it in their Kubernetes deployments.</description>
      <content:encoded><![CDATA[
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/_U3HichIJ0Q" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<p>In this video, I will show you how to use Bicep to deploy a Kubernetes cluster with custom network settings using the Azure CNI.</p>
<p>Azure CNI allows pods to be assigned IP addresses from Azure VNets which allows them to communicate with Azure resources directly through peered networks.</p>
<p>I use Neovim and the Azure CLI for my coding and deployment.</p>
<p>You will learn how to:</p>
<ul>
<li>Implement dev/test prefix to create multiple clusters with the same template</li>
<li>Plan a VNet range for an Azure CNI cluster and be mindful of overlaps</li>
<li>Deploy a VNet and subnet for the cluster using Bicep</li>
<li>Deploy a cluster with Azure CNI enabled and configure the maximum number of pods per node</li>
<li>Validate your Bicep template and troubleshoot errors</li>
<li>Explore the results of your deployment in the Azure portal</li>
<li>Understand the limitations of Azure CNI and why VNet peering is not supported in my configuration due to overlaps</li>
</ul>
<p>This video is suitable for anyone who wants to learn more about Azure CNI and how to use it in their Kubernetes deployments.</p>
<h1 id="excalidraw">Excalidraw</h1>
<p><img loading="lazy" src="/excni.png" type="" alt=""  /></p>
<h1 id="bullet-points">Bullet Points</h1>
<ul>
<li>Introduction to Azure CNI</li>
<li>Implement dev/test prefix</li>
<li>Deploy VNET and one subnet for the cluster</li>
<li>Deploy cluster with Azure CNI enabled</li>
</ul>
<h1 id="vnet-planning">VNet planning</h1>
<p>VNet cidr: 10.108.0.0/16</p>
<p>Subnet cidr:</p>
<p>10.108.0.0/16 - 65,536 addresses</p>
<p>Service cidr 10.0.0.0/16
DNS service ip addres 10.0.0.10</p>
<h2 id="links">Links:</h2>
<p>202307071807</p>
<p><a href="https://youtu.be/_U3HichIJ0Q">https://youtu.be/_U3HichIJ0Q</a></p>
<p><a href="https://learn.microsoft.com/en-us/azure/aks/configure-azure-cni">https://learn.microsoft.com/en-us/azure/aks/configure-azure-cni</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: Finishing Pipeline Setup  &amp; Working on KeyVault Template - Azure Kubernetes Lab Series</title>
      <link>https://mischavandenburg.com/zet/video-finish-pipeline-setup-aks-series/</link>
      <pubDate>Fri, 30 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/video-finish-pipeline-setup-aks-series/</guid>
      <description>Finish deploying keyvault using pipeline Get the random name generation to work Lessons Learned Subscriptions need to be registered with resource providers, apparently https://learn.microsoft.com/en-us/azure/azure-resource-manager/troubleshooting/error-register-resource-provider?tabs=azure-cli
acccesPolicies are mandatory on KeyVaults, but not when RBAC is enabled Assign contributor role to Azure DevOps service connection to be able to create resource groups from pipeline Achieved Setting up connection between pipeline and Azure subscription Assign correct rights to the service connection so it is allowed to deploy new resource groups (and other resources) Learned about provider registrations Made progress on creating unique names for resources Successfully deployed new resource group and key vault from the pipeline Next time: Look into random string creation with utcNow</description>
      <content:encoded><![CDATA[
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/eooZ3OHl5Mc" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<ul>
<li>Finish deploying keyvault using pipeline</li>
<li>Get the random name generation to work</li>
</ul>
<h1 id="lessons-learned">Lessons Learned</h1>
<ul>
<li>Subscriptions need to be registered with resource providers, apparently</li>
</ul>
<p><a href="https://learn.microsoft.com/en-us/azure/azure-resource-manager/troubleshooting/error-register-resource-provider?tabs=azure-cli">https://learn.microsoft.com/en-us/azure/azure-resource-manager/troubleshooting/error-register-resource-provider?tabs=azure-cli</a></p>
<ul>
<li>acccesPolicies are mandatory on KeyVaults, but not when RBAC is enabled</li>
<li>Assign contributor role to Azure DevOps service connection to be able to create resource groups from pipeline</li>
</ul>
<h1 id="achieved">Achieved</h1>
<ul>
<li>Setting up connection between pipeline and Azure subscription</li>
<li>Assign correct rights to the service connection so it is allowed to deploy new resource groups (and other resources)</li>
<li>Learned about provider registrations</li>
<li>Made progress on creating unique names for resources</li>
<li>Successfully deployed new resource group and key vault from the pipeline</li>
</ul>
<h1 id="next-time">Next time:</h1>
<p>Look into random string creation with utcNow</p>
<p><a href="https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-functions-date#utcnow">https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-functions-date#utcnow</a></p>
<p>or newGuid</p>
<p><a href="https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-functions-string#newguid">https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-functions-string#newguid</a></p>
<p>Links:</p>
<p>202306281806</p>
<h2 id="links">Links:</h2>
<p>202306302206</p>
<p><a href="https://youtu.be/eooZ3OHl5Mc">https://youtu.be/eooZ3OHl5Mc</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: Setting Up A Simple Azure Pipeline To Deploy A Keyvault</title>
      <link>https://mischavandenburg.com/zet/video-aks-lab-pipeline-first-setup/</link>
      <pubDate>Fri, 30 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/video-aks-lab-pipeline-first-setup/</guid>
      <description>Write KeyVault template Write pipeline code set up Azure DevOps pipeline Lessons Learned Always make sure to use az deployment group instead of az group deployment Because it has older Bicep version and will be deprecated Make sure to be in correct Directory to be able to sync subscriptions for service connection Links: 202306302206
https://youtu.be/WnA8V3uq7P8</description>
      <content:encoded><![CDATA[
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/WnA8V3uq7P8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<ul>
<li>Write KeyVault template</li>
<li>Write pipeline code</li>
<li>set up Azure DevOps pipeline</li>
</ul>
<h1 id="lessons-learned">Lessons Learned</h1>
<ul>
<li>Always make sure to use <code>az deployment group</code> instead of <code>az group deployment</code></li>
<li>Because it has older Bicep version and will be deprecated</li>
<li>Make sure to be in correct Directory to be able to sync subscriptions for service connection</li>
</ul>
<h2 id="links">Links:</h2>
<p>202306302206</p>
<p><a href="https://youtu.be/WnA8V3uq7P8">https://youtu.be/WnA8V3uq7P8</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: Deploying an AKS Cluster with Bicep, GitHub Copilot and Neovim</title>
      <link>https://mischavandenburg.com/zet/video-deploying-aks-cluster-bicep-github-copilot/</link>
      <pubDate>Tue, 27 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/video-deploying-aks-cluster-bicep-github-copilot/</guid>
      <description>Inspired by a GitHub Copilot demonstration I witnessed at Microsoft, I wanted to see how quickly I could deploy an AKS cluster from Neovim with Bicep using Copilot. I wasn&amp;rsquo;t disappointed!
Links: 202306271706
https://www.youtube.com/watch?v=l0B65FUfNBU
[[aks]] [[kubernetes]] [[neovim]] [[bicep]] [[coding]]</description>
      <content:encoded><![CDATA[<p>Inspired by a GitHub Copilot demonstration I witnessed at Microsoft, I wanted to see how quickly I could deploy an AKS cluster from Neovim with Bicep using Copilot. I wasn&rsquo;t disappointed!</p>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/l0B65FUfNBU" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<h2 id="links">Links:</h2>
<p>202306271706</p>
<p><a href="https://www.youtube.com/watch?v=l0B65FUfNBU">https://www.youtube.com/watch?v=l0B65FUfNBU</a></p>
<p>[[aks]]
[[kubernetes]]
[[neovim]]
[[bicep]]
[[coding]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: Introducing New Bicep Parameter Files - .bicepparam - No more JSON!</title>
      <link>https://mischavandenburg.com/zet/video-bicep-bicepparam/</link>
      <pubDate>Tue, 27 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/video-bicep-bicepparam/</guid>
      <description>The new parameter files use bicep style formatting instead of JSON, and they will make the lives of Cloud Engineers a lot easier. They have the following advantages:
More readable and editor friendly Cleaner and less lines of code VSCode integration Quickly convert from JSON or template file using VSCode In this video I introduce these new files. I go over the new formatting, and I also introduce the new features in VSCode for the .</description>
      <content:encoded><![CDATA[<p>The new parameter files use bicep style formatting instead of JSON, and they will make the lives of Cloud Engineers a lot easier. They have the following advantages:</p>
<ul>
<li>More readable and editor friendly</li>
<li>Cleaner and less lines of code</li>
<li>VSCode integration</li>
<li>Quickly convert from JSON or template file using VSCode</li>
</ul>
<p>In this video I introduce these new files. I go over the new formatting, and I also introduce the new features in VSCode for the .bicepparam files.</p>
<p>Yes, you read that right, you&rsquo;ll be seeing a hardcore vim user switch to VSCode for this particular task!</p>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/6Gav1JpGAzo" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<h2 id="links">Links:</h2>
<p>202306271706</p>
<p><a href="https://youtu.be/6Gav1JpGAzo">https://youtu.be/6Gav1JpGAzo</a></p>
<p><a href="https://github.com/Azure/bicep/releases/tag/v0.18.4">https://github.com/Azure/bicep/releases/tag/v0.18.4</a></p>
<p><a href="https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/parameter-files?tabs=Bicep">https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/parameter-files?tabs=Bicep</a></p>
<p><a href="https://github.com/mischavandenburg/lab/tree/main/bicep/keyvault-parameters">https://github.com/mischavandenburg/lab/tree/main/bicep/keyvault-parameters</a></p>
<p>[[AKS]]
[[bicep]]
[[coding]]
[[kubernetes]]
[[azure]]
[[neovim]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>What is Azure CNI Overlay for AKS?</title>
      <link>https://mischavandenburg.com/zet/azure-aks-cni-overlay/</link>
      <pubDate>Wed, 14 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/azure-aks-cni-overlay/</guid>
      <description>CNI? CNI stands for Container Network Interface. It allows communication between pods and services.
Current Azure CNI limitations Let&amp;rsquo;s take a practical example. We have an enterprise environment where a large network is utilized, spanning multiple clouds and on-prem infrastructure hubs. To enable seamless communication across these sections, they must belong to the same network. As a result, specific IP address ranges are assigned to each section, with AWS, On-Prem A, and Azure each having their respective ranges.</description>
      <content:encoded><![CDATA[<h1 id="cni">CNI?</h1>
<p>CNI stands for Container Network Interface. It allows communication between pods and services.</p>
<h2 id="current-azure-cni-limitations">Current Azure CNI limitations</h2>
<p>Let&rsquo;s take a practical example. We have an enterprise environment where a large network is utilized, spanning multiple clouds and on-prem infrastructure hubs. To enable seamless communication across these sections, they must belong to the same network. As a result, specific IP address ranges are assigned to each section, with AWS, On-Prem A, and Azure each having their respective ranges.</p>
<p>Let&rsquo;s say Azure is assigned the following ranges:</p>
<p>10.60.0.0/16</p>
<p>10.61.0.0/16</p>
<p>10.62.0.0/16</p>
<p>This means that the networks in each of these ranges would have a maximum possible amount of 65534 addresses per range.</p>
<p>With the current Azure CNI (i.e. the non-overlay version), all pods are assigned an IP address from one of these ranges. It also uses direct VNet routing.  Since the pods use VNet IP&rsquo;s, there is a maximum of 65.000 pods per cluster. In other words, there is a risk for IP exhaustion, which limits the scalability of your workloads. Moreover, pod subnets cannot be shared across clusters.</p>
<p>It is crucial to carefully plan the number of pods you expect to deploy. If the required number of IP addresses exceeds the available addresses in the subnet, you will not be able to run your pods.</p>
<p>Now, these ranges are large and you can anticipate the growth of your resources. For now we are fine. But to design an infrastructure which is truly scalable and extendable, you will need to look into different options. This is where the Azure CNI Overlay comes in.</p>
<h2 id="benefits-of-azure-cni-overlay">Benefits of Azure CNI Overlay</h2>
<p>An Overlay network is an abstracted, virtual network which is put on top of your current network infrastructure. Nodes are assigned IP addresses from the VNets that they are deployed in, but pods get assigned IP addresses from the Overlay network.</p>
<p>Pods are assigned addresses from a private CIDR which is logically separate from the VNet hosting the nodes. They do not use up the IP addressess of the VNets, which means that your workloads become nearly infinitely scalable within your assigned IP address ranges when you are operating in this type of corporate networking infrastructure with IP range limitations. You can scale up to literally thousands of nodes without worrying about IP exhaustion.</p>
<p><img loading="lazy" src="/cnioverlay.png" type="" alt=""  /></p>
<p>Additionally, the Overlay network can also span across multiple AKS clusters. This opens up a whole world of possibilities where pods from separate workloads on separate clusters could communicate with each other directly using the high speed native direct routing of the Azure network.</p>
<h2 id="limitations">Limitations</h2>
<p>Azure CNI Overlay also comes with some limitations. A big one is that you cannot use Application Gateway as an Ingress Controller (AGIC) for an Overlay cluster.</p>
<p>Other notable limitations:</p>
<ul>
<li>Windows support is still in Preview</li>
<li>Virtual Machine Availability Sets (VMAS) are not supported for Overlay</li>
<li>Dualstack networking is not supported in Overlay</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>In conclusion, the Azure CNI Overlay provides a powerful solution to address the challenges of IP exhaustion and scalability in Azure AKS. By implementing the overlay network, organizations can overcome the limitations of the non-overlay version of Azure CNI and achieve a truly scalable and manageable infrastructure.</p>
<p>Azure CNI Overlay is currently in preview for Windows and GA for Linux nodes, but I&rsquo;m very excited about the developments. I&rsquo;ll be following them closely and I hope to be a part of its implementation at my current contract.</p>
<h1 id="links">Links:</h1>
<p><a href="https://learn.microsoft.com/en-us/azure/aks/azure-cni-overlay">https://learn.microsoft.com/en-us/azure/aks/azure-cni-overlay</a></p>
<p><a href="https://www.youtube.com/watch?v=kLBLaCC_dNs">https://www.youtube.com/watch?v=kLBLaCC_dNs</a></p>
<p><a href="https://azure.microsoft.com/en-us/updates/generally-available-azure-cni-overlay-for-linux/">https://azure.microsoft.com/en-us/updates/generally-available-azure-cni-overlay-for-linux/</a></p>
<p>202306131506</p>
<p>[[aks-networking-essentials]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Deploying Simple Applications to AKS with Draft</title>
      <link>https://mischavandenburg.com/zet/deploy-draft-azure/</link>
      <pubDate>Fri, 09 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/deploy-draft-azure/</guid>
      <description>Not sure how this will play out with more complex applications, but I can definitely see how this would accellerate the process for developers to get their first versions deployed without the toil of setting up manifests and pipelines. Will definitely play around with this soon and I&amp;rsquo;m curious to see how far Microsoft will take this!
Links: 202306092006</description>
      <content:encoded><![CDATA[<p>Not sure how this will play out with more complex applications, but I can definitely see how this would accellerate the process for developers to get their first versions deployed without the toil of setting up manifests and pipelines. Will definitely play around with this soon and I&rsquo;m curious to see how far Microsoft will take this!</p>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/PqhdX8-SZYw" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<h2 id="links">Links:</h2>
<p>202306092006</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>You Can Abort Operations on AKS Clusters Now</title>
      <link>https://mischavandenburg.com/zet/aks-abort-operation/</link>
      <pubDate>Thu, 27 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/aks-abort-operation/</guid>
      <description>It&amp;rsquo;s now possible to abort long running operations on AKS clusters. It was released as Generally Available.
For example:
az aks operation-abort --name myAKSCluster --resource-group myResourceGroup
Links: 202304270704
https://azure.microsoft.com/en-us/updates/generally-available-operation-abort-in-aks/
https://learn.microsoft.com/en-us/azure/aks/manage-abort-operations?tabs=azure-cli</description>
      <content:encoded><![CDATA[<p>It&rsquo;s now possible to abort long running operations on AKS clusters. It was released as Generally Available.</p>
<p>For example:</p>
<p><code>az aks operation-abort --name myAKSCluster --resource-group myResourceGroup</code></p>
<h2 id="links">Links:</h2>
<p>202304270704</p>
<p><a href="https://azure.microsoft.com/en-us/updates/generally-available-operation-abort-in-aks/">https://azure.microsoft.com/en-us/updates/generally-available-operation-abort-in-aks/</a></p>
<p><a href="https://learn.microsoft.com/en-us/azure/aks/manage-abort-operations?tabs=azure-cli">https://learn.microsoft.com/en-us/azure/aks/manage-abort-operations?tabs=azure-cli</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Starting My Homelab</title>
      <link>https://mischavandenburg.com/zet/starting-my-homelab/</link>
      <pubDate>Wed, 12 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/starting-my-homelab/</guid>
      <description>This week I started a project which I&amp;rsquo;ve been putting off for too long. I finally started my homelab. Over the past year I&amp;rsquo;ve been collecting hardware here and there, and I&amp;rsquo;ve had the intention to start up a proper Kubernetes cluster at home. I got inspired by Rob Muhlenstein&amp;rsquo;s Homelab Init playlist on YouTube which I&amp;rsquo;m working on.
There are a few reasons why I haven&amp;rsquo;t started up until now:</description>
      <content:encoded><![CDATA[<p>This week I started a project which I&rsquo;ve been putting off for too long. I finally started my homelab. Over the past year I&rsquo;ve been collecting hardware here and there, and I&rsquo;ve had the intention to start up a proper Kubernetes cluster at home. I got inspired by Rob Muhlenstein&rsquo;s <a href="https://www.youtube.com/playlist?list=PLrK9UeDMcQLpjUGg5z9Z6Un-axVx06-2J">Homelab Init</a> playlist on YouTube which I&rsquo;m working on.</p>
<p>There are a few reasons why I haven&rsquo;t started up until now:</p>
<ul>
<li>Focused on switching jobs and certifications</li>
<li>Not knowing what to run on the cluster</li>
<li>High electricity costs</li>
</ul>
<p>Now that I found a new job with a great employer, I&rsquo;ve changed my focus towards doing more hands-on learning in my free time by learning Go and diving deeper into Cloud Native technology. Energy prices have come down in the meantime as well.</p>
<p>I&rsquo;ve reached a stage in my Go learning journey where I&rsquo;m actually able to start making small deployable applications, and I want an environment where I can do that without high costs or without fearing to break something. I want to learn more about databases on Kubernetes, and I want to start writing small microservices and API&rsquo;s that are able to query these databases.</p>
<p>My lab is going to be my playground, where I can deploy whatever I want and learn the technologies that interest me at that particular moment.</p>
<h1 id="hardware-parts">Hardware: Parts</h1>
<p>For some reason the Raspberry Pi has become synonymous with homelabs. I get that it&rsquo;s fun to run a cluster on something that is not much larger than a 1kg pack of sugar. But I never really caught on to that whole scene yet. Maybe it&rsquo;s because I&rsquo;m a bit late to the party and the Pi&rsquo;s have been scarce and very expensive lately?</p>
<p>In any case, I&rsquo;ve been thankfully accepting old computers that friends were going to get rid of, and I&rsquo;ve been keeping some of my own old hardware as well. I have enough motherboards and other parts to assemble around 3 nodes, which will probably have around 8GB RAM each, but possibilities to attach storage.</p>
<p>This is also what has been keeping me back for a while I think. There is quite a bit of work that I need to do to get these machines going, and probably I&rsquo;ll have to purchase a couple of other parts. However, I also have some functional hardware.</p>
<h2 id="gaming-desktop">Gaming Desktop</h2>
<p>I have an old gaming desktop with 16GB RAM, an Intel 6700K Skylake, 1070 video card and a couple of TB of storage.</p>
<p>This has been my Arch Linux desktop for the past year, but now that I switched to my new MacBook, I&rsquo;m not using it as much. I want to keep it as it is right now, but I could run a few Virtual Machines on there, and maybe consider turning it into a ProxMox server.</p>
<h2 id="old-laptops">Old Laptops</h2>
<p>I have two old laptops. One Asus with 4GB of RAM and a Thinkpad T430 with 8GB RAM. The Thinkpad is actually surprisingly powerful. As a weekend project I installed Arch on it and I fitted it with a refurbished keyboard, and it is a very pleasant machine to work with. However, now that I have a very powerful laptop that I use as a desktop and portable device, it has become redundant.</p>
<h1 id="old-laptops-as-raspberri-pis">Old Laptops as Raspberri Pi&rsquo;s</h1>
<p>Having these two old laptops lying around, it occurred to me that these machines were basically Raspberri Pi&rsquo;s with a large form factor and a higher power usage. Why would I need to spend hundreds of euros on these smaller computers if I could just use these laptops as a starting point for my lab?</p>
<p>Using laptops has the following advantages:</p>
<ul>
<li>No additional costs</li>
<li>No building needed</li>
<li>Easy to install Linux on them</li>
<li>Built-in screen and keyboard for quick access when SSH does not work out</li>
<li>Built-in batteries to handle short power disruptions (rare but possible)</li>
<li>Can get going very quickly</li>
</ul>
<h1 id="choices-and-goals">Choices and Goals</h1>
<h2 id="kubernetes">Kubernetes</h2>
<p>The first goal is to get a Kubernetes cluster running. I will do bare metal kubeadm installs, and later I want to learn more about Talos. Fortunately I feel very comfortable installing Kubernetes with kubeadm. I did plenty of practice for my CKA, and I recently installed it on free Oracle VM&rsquo;s. I&rsquo;ve experimented a bit with K9S earlier, but I want to learn how to maintain on-prem Kubernetes.</p>
<p>My goals is to learn to maintain production-grade clusters properly.</p>
<h2 id="linux">Linux</h2>
<p>Naturally I&rsquo;ll be using Linux as my base OS. After some consideration I chose to use Ubuntu Server 22. Some notes on that choice:</p>
<ul>
<li>I already have years of Ubuntu Server experience</li>
<li>Good to keep building on what I have</li>
<li>Working with managed Kubernetes on my day job requires me to keep Linux admin skills fresh</li>
<li>Still the <a href="https://www.enterpriseappstoday.com/stats/linux-statistics.html">most popular Linux distro</a></li>
<li>Google uses Ubuntu Server</li>
<li>Well documented and plenty of questions on StackOverflow</li>
</ul>
<h2 id="infrastructure-as-code--gitops">Infrastructure as Code &amp; GitOps</h2>
<p>Initially I&rsquo;ll configure the servers by hand, but I want to have the server configuration as code as Ansible playbooks eventually. However, I&rsquo;ll be using ArgoCD for all of my deployments on Kubernetes itself, so the server configuration is only a very small part of the setup. Just get Kubernetes running and do the rest with ArgoCD.</p>
<p>Perhaps I will expand with larger servers that run multiple VM&rsquo;s. Then it will be very relevant to start provisioning these with Ansible.</p>
<h2 id="networking">Networking</h2>
<p>I want to learn more about networking and use static IP addresses for my servers. I need to figure out how my home network works exactly. Surprisingly, I&rsquo;ve never taken the effort to actually know how the devices on my network get their IP addresses and how they communicate, even though I&rsquo;ve learned plenty about it for my day job and do networking in an enterprise environment daily.</p>
<p>For Kubernetes I&rsquo;ll use Flannel to start out with, but I want to learn more about Cilium, Istio and other service mesh implementations.</p>
<p>Another goal is to host my own DNS server for internal name resolution, probably CoreDNS.</p>
<h2 id="deployment">Deployment</h2>
<p>I want to host my own container registry (Harbor) and use Tekton pipelines to for CI/CD, and I&rsquo;m playing with the thought to host my own GitLab instance as well.</p>
<h1 id="lets-go">Let&rsquo;s Go!</h1>
<p>Another realization was that I don&rsquo;t need to have everything figured out before I begin. The beauty of cluster computing is that you can add to it as you go. I can start with a small cluster of two nodes and build it out as my needs grow. I don&rsquo;t expect to need more than a few GB of RAM in the foreseeable future, so these two laptops will be plenty to get going.</p>
<p><img loading="lazy" src="/cluster-laptops.png" type="" alt=""  /></p>
<h2 id="links">Links:</h2>
<p>2023041213</p>
<p>[[homelab]]</p>
<p>[[homelab-network]]</p>
<p>[[linux]]</p>
<p>[[homelab-ubuntu-server]]</p>
<p>[[kubernetes]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Outlining My First Go Project</title>
      <link>https://mischavandenburg.com/zet/go-twitter-cli-project/</link>
      <pubDate>Sat, 08 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/go-twitter-cli-project/</guid>
      <description>When learning a programming language it is important to start building things quickly and to begin applying the theory. I have a tendency to dive into the books and lose myself in the theory, where I should be getting hands on experience.
Over the past few months I&amp;rsquo;ve generated a bunch of ideas for projects that I want to write, and I selected my first project today.
https://github.com/mischavandenburg/twitter-cli
https://twitter.com/mischa_vdburg
Twitter CLI Programs should solve a problem.</description>
      <content:encoded><![CDATA[<p>When learning a programming language it is important to start building things quickly and to begin applying the theory. I have a tendency to dive into the books and lose myself in the theory, where I should be getting hands on experience.</p>
<p>Over the past few months I&rsquo;ve generated a bunch of ideas for projects that I want to write, and I selected my first project today.</p>
<p><a href="https://github.com/mischavandenburg/twitter-cli">https://github.com/mischavandenburg/twitter-cli</a></p>
<p><a href="https://twitter.com/mischa_vdburg">https://twitter.com/mischa_vdburg</a></p>
<h1 id="twitter-cli">Twitter CLI</h1>
<p>Programs should solve a problem. My problem has to do with Twitter. I recently created a Twitter account, and I want to make a tweet whenever I publish something new on my website. I&rsquo;m currently doing this by hand, and that needs to stop, obviously.</p>
<p>There are bots out there for this, but I want to build it myself. I&rsquo;ve created the following user stories for my project.</p>
<h2 id="user-story-1">User Story 1</h2>
<blockquote>
<p>As a user, I need a command that I can run from a bash shell that will post the standard input to my Twitter account</p>
</blockquote>
<h2 id="user-story-2">User Story 2</h2>
<blockquote>
<p>As a user, I need a command that I can run from a bash shell that will take the latest post from the RSS feed generated by my blog and post it to Twitter</p>
</blockquote>
<h2 id="concepts">Concepts</h2>
<p>By writing this program I&rsquo;ll need to figure out the following problems in Go:</p>
<ul>
<li>Taking input from the command line</li>
<li>Authenticating to the Twitter API</li>
<li>Making a POST request to the Twitter API</li>
<li>Curling an RSS feed</li>
<li>Looping over / reading XML / HTML data</li>
<li>Transforming that data to a suitable format to post to Twitter</li>
</ul>
<h1 id="expansion">Expansion</h1>
<p>This will be a good start for my project and will keep me busy for a while. When I solved the previous problems I can use the result and expand further. Some thoughts about further expansion:</p>
<h2 id="rss-feeds">RSS Feeds</h2>
<p>I can use the skills I learn to start crawling Reddit feeds and filter them for keywords. I can automatically generate a curated selection from Reddit which will be easier to consume and will save me time by only serving me content that I might think is interesting to me, based on keywords.</p>
<h2 id="database">Database</h2>
<p>I want to learn more about using databases on Kubernetes and how to interact with databases using Go. For this I&rsquo;d like to store my RSS feed into a database and keep track of information in the database. I could track whether an article has been posted to Twitter and when.</p>
<h2 id="bot">Bot</h2>
<p>Rather than posting my latest blog post to Twitter by manually running a command, I should have a bot scanning my blog and posting to Twitter when it detects a new article. Or I could trigger the bot whenever I make a push to my blog repo.</p>
<p>In any case, I want to have an application running on a server. I&rsquo;m making plans to start up a proper home lab and this will be a perfect use case to start running on my home Kubernetes cluster.</p>
<h2 id="links">Links:</h2>
<p>202304081304</p>
<p><a href="https://github.com/mischavandenburg/twitter-cli">https://github.com/mischavandenburg/twitter-cli</a></p>
<p><a href="https://twitter.com/mischa_vdburg">https://twitter.com/mischa_vdburg</a></p>
<p>[[go]]</p>
<p>[[go-twitter-cli-project]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Kubernetes Resource Management for Pods and Containers - CPU and Memory</title>
      <link>https://mischavandenburg.com/zet/kubernetes-resource-management-pods-containers/</link>
      <pubDate>Tue, 28 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/kubernetes-resource-management-pods-containers/</guid>
      <description>Pods have containers, and limits can be set on those containers.
Requests used by the kube-scheduler to determine where the Pod will be placed containers can use more than requested resources if it is available on node If a limit is specified, but no request, Kubernetes will use the limit value as the request value.
Limits containers may never use more than the set limit
enforced by kubelet and container</description>
      <content:encoded><![CDATA[<p>Pods have containers, and limits can be set on those containers.</p>
<h1 id="requests">Requests</h1>
<ul>
<li>used by the kube-scheduler to determine where the Pod will be placed</li>
<li>containers can use more than requested resources if it is available on node</li>
</ul>
<p>If a limit is specified, but no request, Kubernetes will use the limit value as the request value.</p>
<h1 id="limits">Limits</h1>
<ul>
<li>
<p>containers may never use more than the set limit</p>
</li>
<li>
<p>enforced by kubelet and container</p>
</li>
<li>
<p>host kernel will kill processes that attempt to allocate more than limit (OOM error)</p>
</li>
<li>
<p>reactively: killed when exceeded</p>
</li>
<li>
<p>enforcement: system prevents container to ever exceed limit</p>
</li>
<li>
<p>If the node runs out of memory and the container exceeds its memory request, the pod will be evicted</p>
</li>
<li>
<p>container runtimes don&rsquo;t terminate Pods or containers for excessive CPU usage</p>
</li>
</ul>
<h1 id="pods">Pods</h1>
<p>The pod resource request and limit is the sum of the resource requests of the containers in the pod.</p>
<h1 id="cpu-units">CPU Units</h1>
<p>Defined as an absolute amount of resource. 1000m = 1 CPU.</p>
<p>This is always the same unit, regardless whether the host has 4 or 48 CPU&rsquo;s.</p>
<p>500m CPU = 0.5 CPU</p>
<h1 id="memory-units">Memory Units</h1>
<p>Can use P, T, G, M etc.</p>
<p>Note that &ldquo;m&rdquo; is not megabyte. 0.8m = 0.8 bytes.</p>
<p>Use mebibytes Mi or megabytes M.</p>
<h1 id="definiton-example">Definiton Example</h1>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">requests</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">100Mi</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l">250m</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">limits</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">200Mi</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l">500m</span><span class="w">
</span></span></span></code></pre></div><h1 id="scheduling">Scheduling</h1>
<p>The scheduler ensures that the sum of requests of the pods on the node does not exceed the available resources.</p>
<p>Even if a node has low resource usage, it will not accept pods that have requests which exceed the available resources.</p>
<h1 id="nodes">Nodes</h1>
<p>use <code>k describe node</code> to see the resource status of the node.</p>
<h2 id="links">Links:</h2>
<p>202303281903</p>
<p><a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>The Difference Between DevOps, Cloud and Cloud Native</title>
      <link>https://mischavandenburg.com/zet/cloud-cloudnative-devops/</link>
      <pubDate>Sun, 26 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/cloud-cloudnative-devops/</guid>
      <description>I found an excellent video by Rob Muhlenstein explaining the differences between Cloud, Cloud Native and DevOps. Here are the notes I wrote.
Cloud These are primarily cloud services. The external cloud.
&amp;ldquo;Something as a Service&amp;rdquo;.
Amazon Azure GCP Cloud Native This is Cloud Native: The CNCF Landscape
Cloud Native is the technology that makes the cloud possible, and all the technology dependent on those services.
Computing Edge Computing
High Performance Computing</description>
      <content:encoded><![CDATA[<p>I found <a href="https://youtu.be/gyjRriOyw-k">an excellent video</a> by Rob Muhlenstein explaining the differences between Cloud, Cloud Native and DevOps. Here are the notes I wrote.</p>
<h1 id="cloud">Cloud</h1>
<p>These are primarily <em>cloud services</em>. The external cloud.</p>
<p>&ldquo;Something as a Service&rdquo;.</p>
<ul>
<li>Amazon</li>
<li>Azure</li>
<li>GCP</li>
</ul>
<h1 id="cloud-native">Cloud Native</h1>
<p>This is Cloud Native: <a href="https://landscape.cncf.io/">The CNCF Landscape</a></p>
<p>Cloud Native is the technology that makes the cloud possible, and all the technology dependent on those services.</p>
<h2 id="computing">Computing</h2>
<ul>
<li>
<p>Edge Computing</p>
</li>
<li>
<p>High Performance Computing</p>
</li>
<li>
<p>Encapsulates all of the technologies that are involved with containerization of work, jobs and nodes</p>
</li>
<li>
<p>Deployment of compute resources as nodes</p>
</li>
<li>
<p>This is why Google&rsquo;s Borg was called Borg</p>
</li>
<li>
<p>Computers are drones of a larger collective</p>
</li>
<li>
<p>Every node puts all the resources into the collective.</p>
</li>
</ul>
<p>The collective is all the nodes combined, and Kubernetes is the Borg that orchestrates everything. It sees available resources and allocates the work that needs to be done.</p>
<p>Borg is the internal system developed at Google to run their infrastructure. You can read about it in the <a href="https://sre.google/books/">Site Reliability Engineering</a> books and I highly recommend them.</p>
<blockquote>
<p>Kubernetes is /proc for the cloud</p>
</blockquote>
<blockquote>
<p>Rob Muhlenstein</p>
</blockquote>
<h2 id="most-important-technologies">Most Important Technologies</h2>
<ul>
<li>
<p>Docker, Dockerfiles</p>
</li>
<li>
<p>Kubernetes</p>
</li>
<li>
<p>Helm</p>
</li>
<li>
<p>Harbor</p>
</li>
<li>
<p>Different registries, harbor, quay</p>
</li>
<li>
<p>It is a lot of Python and POSIX shell</p>
</li>
<li>
<p>Go for infrastructure application development</p>
</li>
<li>
<p>Kubernetes and Helm have won the game</p>
</li>
</ul>
<h2 id="containers-size-matters">Containers: Size Matters</h2>
<ul>
<li>Size matters (again) in the cloud</li>
<li>The smaller your container the better, because it takes less resources and less costs</li>
</ul>
<h1 id="devops">DevOps</h1>
<p>DevOps is not the same as Cloud Native. It is one piece of it, a specific set of practices and actions that can be done within Cloud Native.</p>
<ul>
<li>How you write software and release it</li>
<li>CI/CD</li>
<li>Focused on getting the software out</li>
<li>GitLab has become the one stop shop</li>
<li>Purpose is to write software and get it published fast</li>
<li>GitOps</li>
</ul>
<h1 id="summary">Summary</h1>
<p>In summary, &ldquo;cloud&rdquo; stands for the services offered by cloud providers such as AWS, Azure and GCP. Cloud Native stands for all of the technology that makes these cloud services possible. DevOps is part of Cloud Native, but definitely not the same thing. DevOps is concerned with how software is written and released.</p>
<h1 id="links">Links:</h1>
<p>202303262003</p>
<p><a href="https://youtu.be/gyjRriOyw-k">https://youtu.be/gyjRriOyw-k</a></p>
<p><a href="https://landscape.cncf.io/">https://landscape.cncf.io/</a></p>
<p><a href="https://sre.google/books/">https://sre.google/books/</a></p>
<p>[[rwxrob]]</p>
<p>[[devops]]</p>
<p>[[kubernetes]]</p>
<p>[[cloud-native]]</p>
<p>[[cncf]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Running Docker and Kubernetes on Mac M2</title>
      <link>https://mischavandenburg.com/zet/docker-kubernetes-on-mac-m2/</link>
      <pubDate>Sat, 18 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/docker-kubernetes-on-mac-m2/</guid>
      <description>The past few days I&amp;rsquo;ve been trying out a few options to run Docker containers and a Kubernetes clusters on my new MacBook Pro M2.
Unfortunately you can&amp;rsquo;t just run brew install docker and expect it to work. Additionally, Docker desktop requires that you purchase a license if you use it for work purposes.
Minikube works fine as well, but the networking driver for qemu is not fully supported yet, and I haven&amp;rsquo;t tried any of the other alternatives because I found something better.</description>
      <content:encoded><![CDATA[<p>The past few days I&rsquo;ve been trying out a few options to run Docker containers and a Kubernetes clusters on my new MacBook Pro M2.</p>
<p>Unfortunately you can&rsquo;t just run <code>brew install docker</code> and expect it to work. Additionally, Docker desktop requires that you purchase a license if you use it for work purposes.</p>
<p>Minikube works fine as well, but the networking driver for qemu is not fully supported yet, and I haven&rsquo;t tried any of the other alternatives because I found something better.</p>
<p>Rancher Desktop provides everything that you need. It sets up a local VM where it will run a Kubernetes cluster using k3s. It will configure the containerd container engine for you which you can interact with using <code>nerdctl</code>.</p>
<p>To install:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">brew install rancher
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#after installing rancher, start it up and wait for it to boot the VM.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">alias</span> <span class="nv">docker</span><span class="o">=</span>nerdctl
</span></span><span class="line"><span class="cl">docker run hello-world
</span></span></code></pre></div><p>And you&rsquo;re good to go. Rancher will add the rancher-desktop to your kube context.</p>
<p>To test your Kubernetes cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">k get pods
</span></span><span class="line"><span class="cl">k get nodes
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># test running a pod</span>
</span></span><span class="line"><span class="cl">k run nginx --image<span class="o">=</span>nginx
</span></span><span class="line"><span class="cl">k expose pod nginx --port<span class="o">=</span><span class="m">80</span> --type<span class="o">=</span>NodePort
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># inspect your services and look for 80:31066/TCP under PORT(S)</span>
</span></span><span class="line"><span class="cl">k get svc
</span></span><span class="line"><span class="cl">curl localhost:31066
</span></span></code></pre></div><p>Or visit localhost:31066 in your browser. Replace 31066 with the port you found listed under your services.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Lab VM project - Install ArgoCD to your Kubernetes cluster</title>
      <link>https://mischavandenburg.com/zet/articles/lab-vm-install-argocd/</link>
      <pubDate>Sun, 05 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/lab-vm-install-argocd/</guid>
      <description>This guide uses the official getting started guide with a few modifications. This installation is only for lab purposes. Running ArgoCD in a production environment requires more configuration.
Install argocd and argocd cli kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml My VM is running on arm architecture, so I need these commands to install the argocd cli on ubuntu.
curl -sSL -o argocd-linux-arm64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-arm64 sudo install -m 555 argocd-linux-arm64 /usr/local/bin/argocd rm argocd-linux-arm64 Change the service type to LoadBalancer</description>
      <content:encoded><![CDATA[<p>This guide uses the official getting started guide with a few modifications. This installation is only for lab purposes. Running ArgoCD in a production environment requires more configuration.</p>
<h2 id="install-argocd-and-argocd-cli">Install argocd and argocd cli</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl create namespace argocd
</span></span><span class="line"><span class="cl">kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
</span></span></code></pre></div><p>My VM is running on arm architecture, so I need these commands to install the argocd cli on ubuntu.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">curl -sSL -o argocd-linux-arm64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-arm64
</span></span><span class="line"><span class="cl">sudo install -m <span class="m">555</span> argocd-linux-arm64 /usr/local/bin/argocd
</span></span><span class="line"><span class="cl">rm argocd-linux-arm64
</span></span></code></pre></div><p>Change the service type to LoadBalancer</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl patch svc argocd-server -n argocd -p <span class="s1">&#39;{&#34;spec&#34;: {&#34;type&#34;: &#34;LoadBalancer&#34;}}&#39;</span>
</span></span></code></pre></div><p>Retrieve your passsword</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl -n argocd get secret argocd-initial-admin-secret -o <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">&#34;{.data.password}&#34;</span> <span class="p">|</span> base64 -d<span class="p">;</span> <span class="nb">echo</span>
</span></span></code></pre></div><p>Find out which port argocd-server is running on</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">k get svc -A
</span></span></code></pre></div><p>Look for the argocd-server and see where port 80 is mapped to. In my case, it is 80:31372.</p>
<p>Open this port in your network security group for your VM, and you should be able to log in on ArgoCD in the browser by entering the VM ip followed by the port:</p>
<p><code>http://143.44.179.11:31372</code></p>
<h2 id="links">Links</h2>
<p><a href="https://argo-cd.readthedocs.io/en/stable/getting_started/">https://argo-cd.readthedocs.io/en/stable/getting_started/</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Setting up a Kubernetes cluster on an Ubuntu 20.04 VM with containerd and flannel</title>
      <link>https://mischavandenburg.com/zet/articles/simple-cluster-on-ubuntu-vm/</link>
      <pubDate>Sun, 05 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/simple-cluster-on-ubuntu-vm/</guid>
      <description>You can get a free 24GB ram VM from Oracle. What better place for your own Kubernetes lab that is always available? See this article to create your VM.
Here are the steps I took to install a single node kubernetes cluster on the Ubuntu VM.
Installation sudo apt-get update sudo apt install apt-transport-https curl Install containerd
sudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg echo &amp;#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.</description>
      <content:encoded><![CDATA[<p>You can get a free 24GB ram VM from Oracle. What better place for your own Kubernetes lab that is always available? See <a href="/zet/free-oracle-vm.md">this article</a> to create your VM.</p>
<p>Here are the steps I took to install a single node kubernetes cluster on the Ubuntu VM.</p>
<h2 id="installation">Installation</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo apt-get update
</span></span><span class="line"><span class="cl">sudo apt install apt-transport-https curl
</span></span></code></pre></div><p>Install containerd</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo mkdir -p /etc/apt/keyrings
</span></span><span class="line"><span class="cl">curl -fsSL https://download.docker.com/linux/ubuntu/gpg <span class="p">|</span> sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;deb [arch=</span><span class="k">$(</span>dpkg --print-architecture<span class="k">)</span><span class="s2"> signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu </span><span class="k">$(</span>lsb_release -cs<span class="k">)</span><span class="s2"> stable&#34;</span> <span class="p">|</span> sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null
</span></span><span class="line"><span class="cl">sudo apt-get update
</span></span><span class="line"><span class="cl">sudo apt-get install containerd.io
</span></span></code></pre></div><p>Remove the default containerd configuration, because it creates errors when running kubeadm init.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo rm -f /etc/containerd/config.toml
</span></span><span class="line"><span class="cl">sudo systemctl status containerd.service
</span></span></code></pre></div><p>Install Kubernetes</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo curl -fsSLo /etc/apt/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main&#34;</span> <span class="p">|</span> sudo tee /etc/apt/sources.list.d/kubernetes.list
</span></span><span class="line"><span class="cl">sudo apt install kubeadm kubelet kubectl kubernetes-cni
</span></span></code></pre></div><p>Avoid the error &ldquo;/proc/sys/net/bridge/bridge-nf-call-iptables does not exist&rdquo; on kubeinit (reference <a href="https://github.com/kubernetes/kubeadm/issues/1062)">https://github.com/kubernetes/kubeadm/issues/1062)</a>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo modprobe br_netfilter
</span></span><span class="line"><span class="cl">sudo <span class="nb">echo</span> <span class="m">1</span> &gt; /proc/sys/net/ipv4/ip_forward
</span></span></code></pre></div><h2 id="start-the-cluster">Start the cluster</h2>
<p>Initialize the Kubernetes cluster for use with Flannel</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo kubeadm init --pod-network-cidr<span class="o">=</span>10.244.0.0/16
</span></span></code></pre></div><p>Copy to config as kubadm command says</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">mkdir -p <span class="nv">$HOME</span>/.kube
</span></span><span class="line"><span class="cl">sudo cp -i /etc/kubernetes/admin.conf <span class="nv">$HOME</span>/.kube/config
</span></span><span class="line"><span class="cl">sudo chown <span class="k">$(</span>id -u<span class="k">)</span>:<span class="k">$(</span>id -g<span class="k">)</span> <span class="nv">$HOME</span>/.kube/config
</span></span></code></pre></div><p>Usually you wouldn&rsquo;t run pods on your control-plane node. However, since we are running a lab environment on a single VM, it&rsquo;s ok. To be able to schedule pods on the control-plane node, we need to remove the NoSchedule taint:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl taint node instance-20230205-0909 node-role.kubernetes.io/control-plane:NoSchedule-
</span></span></code></pre></div><h2 id="add-a-container-networking-interface">Add a Container Networking Interface</h2>
<p>Install Flannel to the cluster (reference <a href="https://github.com/flannel-io/flannel">https://github.com/flannel-io/flannel</a>)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
</span></span></code></pre></div><h2 id="configure-the-server-firewall">Configure the server firewall</h2>
<p>We use Uncomplicated Firewall. Run these commands:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo ufw allow <span class="m">22</span>
</span></span><span class="line"><span class="cl">sudo ufw allow 6443/tcp
</span></span><span class="line"><span class="cl">sudo ufw allow 2379:2380/tcp
</span></span><span class="line"><span class="cl">sudo ufw allow 10250/tcp
</span></span><span class="line"><span class="cl">sudo ufw allow 10259/tcp
</span></span><span class="line"><span class="cl">sudo ufw allow 10257/tcp
</span></span><span class="line"><span class="cl">sudo ufw <span class="nb">enable</span>
</span></span><span class="line"><span class="cl">sudo ufw status
</span></span></code></pre></div><h2 id="set-up-bashrc">Set up bashrc</h2>
<p>Next, edit your bashrc with <code>vim ~/.bashrc</code> and add these lines:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">source</span> &lt;<span class="o">(</span>kubectl completion bash<span class="o">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">alias</span> <span class="nv">k</span><span class="o">=</span>kubectl
</span></span><span class="line"><span class="cl"><span class="nb">complete</span> -o default -F __start_kubectl k
</span></span></code></pre></div><p>Then run <code>source ~/.bashrc</code></p>
<p>This configures autocompletion for kubectl, and sets up &ldquo;k&rdquo; as an alias for kubectl.</p>
<h2 id="lets-run-a-pod">Let&rsquo;s run a pod!</h2>
<p>To see all pods running on your cluster:</p>
<p><code>k get pods -A</code></p>
<p>Now let&rsquo;s run a simple nginx pod and expose it:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">k run nginx --image<span class="o">=</span>nginx
</span></span><span class="line"><span class="cl">k expose pod nginx --port<span class="o">=</span><span class="m">80</span> --type<span class="o">=</span>NodePort
</span></span></code></pre></div><p>To find out which port it&rsquo;s running on, run <code>k get service</code>. In the PORT(S) column, there will be an nginx service exposing port 80 to a random port on the node in the range of 30000-32767.</p>
<p>In my case, it says &ldquo;80:31878/TCP&rdquo;</p>
<p>To see if we can reach the container, run:</p>
<p><code>curl localhost:31878</code></p>
<p>If everything went well, you will get back the HTML of the default index page served by NGINX:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ubuntu@instance-20230205-0909:~$ curl localhost:31878
</span></span><span class="line"><span class="cl">&lt;!DOCTYPE html&gt;
</span></span><span class="line"><span class="cl">&lt;html&gt;
</span></span><span class="line"><span class="cl">&lt;head&gt;
</span></span><span class="line"><span class="cl">&lt;title&gt;Welcome to nginx!&lt;/title&gt;
</span></span><span class="line"><span class="cl">&lt;style&gt;
</span></span><span class="line"><span class="cl">html <span class="o">{</span> color-scheme: light dark<span class="p">;</span> <span class="o">}</span>
</span></span><span class="line"><span class="cl">body <span class="o">{</span> width: 35em<span class="p">;</span> margin: <span class="m">0</span> auto<span class="p">;</span>
</span></span><span class="line"><span class="cl">font-family: Tahoma, Verdana, Arial, sans-serif<span class="p">;</span> <span class="o">}</span>
</span></span><span class="line"><span class="cl">&lt;/style&gt;
</span></span><span class="line"><span class="cl">&lt;/head&gt;
</span></span><span class="line"><span class="cl">&lt;body&gt;
</span></span><span class="line"><span class="cl">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
</span></span><span class="line"><span class="cl">&lt;p&gt;If you see this page, the nginx web server is successfully installed and
</span></span><span class="line"><span class="cl">working. Further configuration is required.&lt;/p&gt;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">&lt;p&gt;For online documentation and support please refer to
</span></span><span class="line"><span class="cl">&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&#34;http://nginx.org/&#34;</span>&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
</span></span><span class="line"><span class="cl">Commercial support is available at
</span></span><span class="line"><span class="cl">&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&#34;http://nginx.com/&#34;</span>&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">&lt;p&gt;&lt;em&gt;Thank you <span class="k">for</span> using nginx.&lt;/em&gt;&lt;/p&gt;
</span></span><span class="line"><span class="cl">&lt;/body&gt;
</span></span><span class="line"><span class="cl">&lt;/html&gt;
</span></span></code></pre></div><p>To reach the pod from the browser, open your port in the security group configured for the subnet of your VM.</p>
<p>Good luck with your new lab environment!</p>
<h2 id="links">Links</h2>
<p><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/</a></p>
<p><a href="https://kubernetes.io/docs/concepts/services-networking/service/">https://kubernetes.io/docs/concepts/services-networking/service/</a></p>
<p><a href="https://github.com/flannel-io/flannel">https://github.com/flannel-io/flannel</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Get a free 4 CPU 24GB Ram VM on from Oracle</title>
      <link>https://mischavandenburg.com/zet/free-oracle-vm/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/free-oracle-vm/</guid>
      <description>A few weeks ago someone gave me a tip. Oracle actually has a really good free tier offering.
You can host a 4CPU 24GB VM for free!
This is perfect for a lab environment.
I spent my evening creating the VM and setting up a kubernetes cluster from scratch.
Use this video to claim your free vm:
https://www.youtube.com/watch?v=NKc3k7xceT8</description>
      <content:encoded><![CDATA[<p>A few weeks ago someone gave me a tip. Oracle actually has a really good free tier offering.</p>
<p>You can host a 4CPU 24GB VM for free!</p>
<p>This is perfect for a lab environment.</p>
<p>I spent my evening creating the VM and setting up a kubernetes cluster from scratch.</p>
<p>Use this video to claim your free vm:</p>
<p><a href="https://www.youtube.com/watch?v=NKc3k7xceT8">https://www.youtube.com/watch?v=NKc3k7xceT8</a></p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
