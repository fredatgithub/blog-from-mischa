<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Mischa van den Burg</title>
    <link>https://mischavandenburg.com/</link>
    <description>Recent content on Mischa van den Burg</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 05 Feb 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://mischavandenburg.com/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Setting up a simple cluster on an Ubuntu 20.04 VM with containerd and flannel</title>
      <link>https://mischavandenburg.com/zet/articles/simple-cluster-on-ubuntu-vm/</link>
      <pubDate>Sun, 05 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/simple-cluster-on-ubuntu-vm/</guid>
      <description>You can get a free 24GB ram VM from Oracle. What better place for your own Kubernetes lab that is always available? See this article to create your VM.
Here are the steps I took to install a single node kubernetes cluster on the Ubuntu VM.
Installation sudo apt-get update sudo apt install apt-transport-https curl Install containerd
sudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg echo &amp;#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.</description>
      <content:encoded><![CDATA[<p>You can get a free 24GB ram VM from Oracle. What better place for your own Kubernetes lab that is always available? See <a href="/zet/free-oracle-vm.md">this article</a> to create your VM.</p>
<p>Here are the steps I took to install a single node kubernetes cluster on the Ubuntu VM.</p>
<h2 id="installation">Installation</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo apt-get update
</span></span><span class="line"><span class="cl">sudo apt install apt-transport-https curl
</span></span></code></pre></div><p>Install containerd</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo mkdir -p /etc/apt/keyrings
</span></span><span class="line"><span class="cl">curl -fsSL https://download.docker.com/linux/ubuntu/gpg <span class="p">|</span> sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;deb [arch=</span><span class="k">$(</span>dpkg --print-architecture<span class="k">)</span><span class="s2"> signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu </span><span class="k">$(</span>lsb_release -cs<span class="k">)</span><span class="s2"> stable&#34;</span> <span class="p">|</span> sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null
</span></span><span class="line"><span class="cl">sudo apt-get update
</span></span><span class="line"><span class="cl">sudo apt-get install containerd.io
</span></span></code></pre></div><p>Remove the default containerd configuration, because it creates errors when running kubeadm init.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo rm -f /etc/containerd/config.toml
</span></span><span class="line"><span class="cl">sudo systemctl status containerd.service
</span></span></code></pre></div><p>Install Kubernetes</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo curl -fsSLo /etc/apt/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main&#34;</span> <span class="p">|</span> sudo tee /etc/apt/sources.list.d/kubernetes.list
</span></span><span class="line"><span class="cl">sudo apt install kubeadm kubelet kubectl kubernetes-cni
</span></span></code></pre></div><p>Avoid the error &ldquo;/proc/sys/net/bridge/bridge-nf-call-iptables does not exist&rdquo; on kubeinit (reference <a href="https://github.com/kubernetes/kubeadm/issues/1062)">https://github.com/kubernetes/kubeadm/issues/1062)</a>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo modprobe br_netfilter
</span></span><span class="line"><span class="cl">sudo <span class="nb">echo</span> <span class="m">1</span> &gt; /proc/sys/net/ipv4/ip_forward
</span></span></code></pre></div><h2 id="start-the-cluster">Start the cluster</h2>
<p>Initialize the Kubernetes cluster for use with Flannel</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo kubeadm init --pod-network-cidr<span class="o">=</span>10.244.0.0/16
</span></span></code></pre></div><p>Copy to config as kubadm command says</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">mkdir -p <span class="nv">$HOME</span>/.kube
</span></span><span class="line"><span class="cl">sudo cp -i /etc/kubernetes/admin.conf <span class="nv">$HOME</span>/.kube/config
</span></span><span class="line"><span class="cl">sudo chown <span class="k">$(</span>id -u<span class="k">)</span>:<span class="k">$(</span>id -g<span class="k">)</span> <span class="nv">$HOME</span>/.kube/config
</span></span></code></pre></div><p>Usually you wouldn&rsquo;t run pods on your control-plane node. However, since we are running a lab environment on a single VM, it&rsquo;s ok. To be able to schedule pods on the control-plane node, we need to remove the NoSchedule taint:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl taint node instance-20230205-0909 node-role.kubernetes.io/control-plane:NoSchedule-
</span></span></code></pre></div><h2 id="add-a-container-networking-interface">Add a Container Networking Interface</h2>
<p>Install Flannel to the cluster (reference <a href="https://github.com/flannel-io/flannel">https://github.com/flannel-io/flannel</a>)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
</span></span></code></pre></div><h2 id="set-up-bashrc">Set up bashrc</h2>
<p>Next, edit your bashrc with <code>vim ~/.bashrc</code> and add these lines:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">source</span> &lt;<span class="o">(</span>kubectl completion bash<span class="o">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">alias</span> <span class="nv">k</span><span class="o">=</span>kubectl
</span></span><span class="line"><span class="cl"><span class="nb">complete</span> -o default -F __start_kubectl k
</span></span></code></pre></div><p>Then run <code>source ~/.bashrc</code></p>
<p>This configures autocompletion for kubectl, and sets up &ldquo;k&rdquo; as an alias for kubectl.</p>
<h2 id="lets-run-a-pod">Let&rsquo;s run a pod!</h2>
<p>To see all pods running on your cluster:</p>
<p><code>k get pods -A</code></p>
<p>Now let&rsquo;s run a simple nginx pod and expose it:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">k run nginx --image<span class="o">=</span>nginx
</span></span><span class="line"><span class="cl">k expose pod nginx --port<span class="o">=</span><span class="m">80</span> --type<span class="o">=</span>NodePort
</span></span></code></pre></div><p>To find out which port it&rsquo;s running on, run <code>k get service</code>. In the PORT(S) column, there will be an nginx service exposing port 80 to a random port on the node in the range of 30000-32767.</p>
<p>In my case, it says &ldquo;80:31878/TCP&rdquo;</p>
<p>To see if we can reach the container, run:</p>
<p><code>curl localhost:31878</code></p>
<p>If everything went well, you will get back the HTML of the default index page served by NGINX:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ubuntu@instance-20230205-0909:~$ curl localhost:31878
</span></span><span class="line"><span class="cl">&lt;!DOCTYPE html&gt;
</span></span><span class="line"><span class="cl">&lt;html&gt;
</span></span><span class="line"><span class="cl">&lt;head&gt;
</span></span><span class="line"><span class="cl">&lt;title&gt;Welcome to nginx!&lt;/title&gt;
</span></span><span class="line"><span class="cl">&lt;style&gt;
</span></span><span class="line"><span class="cl">html <span class="o">{</span> color-scheme: light dark<span class="p">;</span> <span class="o">}</span>
</span></span><span class="line"><span class="cl">body <span class="o">{</span> width: 35em<span class="p">;</span> margin: <span class="m">0</span> auto<span class="p">;</span>
</span></span><span class="line"><span class="cl">font-family: Tahoma, Verdana, Arial, sans-serif<span class="p">;</span> <span class="o">}</span>
</span></span><span class="line"><span class="cl">&lt;/style&gt;
</span></span><span class="line"><span class="cl">&lt;/head&gt;
</span></span><span class="line"><span class="cl">&lt;body&gt;
</span></span><span class="line"><span class="cl">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
</span></span><span class="line"><span class="cl">&lt;p&gt;If you see this page, the nginx web server is successfully installed and
</span></span><span class="line"><span class="cl">working. Further configuration is required.&lt;/p&gt;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">&lt;p&gt;For online documentation and support please refer to
</span></span><span class="line"><span class="cl">&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&#34;http://nginx.org/&#34;</span>&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
</span></span><span class="line"><span class="cl">Commercial support is available at
</span></span><span class="line"><span class="cl">&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&#34;http://nginx.com/&#34;</span>&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">&lt;p&gt;&lt;em&gt;Thank you <span class="k">for</span> using nginx.&lt;/em&gt;&lt;/p&gt;
</span></span><span class="line"><span class="cl">&lt;/body&gt;
</span></span><span class="line"><span class="cl">&lt;/html&gt;
</span></span></code></pre></div><p>To reach the pod from the browser, open your port in the security group configured for the subnet of your VM.</p>
<p>Good luck with your new lab environment!</p>
<h2 id="links">Links</h2>
<p><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/</a>
<a href="https://kubernetes.io/docs/concepts/services-networking/service/">https://kubernetes.io/docs/concepts/services-networking/service/</a>
<a href="https://github.com/flannel-io/flannel">https://github.com/flannel-io/flannel</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Get a free 4 CPU 24GB Ram VM on from Oracle</title>
      <link>https://mischavandenburg.com/zet/free-oracle-vm/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/free-oracle-vm/</guid>
      <description>A few weeks ago someone gave me a tip. Oracle actually has a really good free tier offering.
You can host a 4CPU 24GB VM for free!
This is perfect for a lab environment.
I spent my evening creating the VM and setting up a kubernetes cluster from scratch.
Use this video to claim your free vm:
https://www.youtube.com/watch?v=NKc3k7xceT8</description>
      <content:encoded><![CDATA[<p>A few weeks ago someone gave me a tip. Oracle actually has a really good free tier offering.</p>
<p>You can host a 4CPU 24GB VM for free!</p>
<p>This is perfect for a lab environment.</p>
<p>I spent my evening creating the VM and setting up a kubernetes cluster from scratch.</p>
<p>Use this video to claim your free vm:</p>
<p><a href="https://www.youtube.com/watch?v=NKc3k7xceT8">https://www.youtube.com/watch?v=NKc3k7xceT8</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Setting up a new LUKS encrypted disk with dm-crypt in Arch Linux</title>
      <link>https://mischavandenburg.com/zet/articles/new-luks-encrypted-disk/</link>
      <pubDate>Sat, 21 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/new-luks-encrypted-disk/</guid>
      <description>Today I added a harddisk I had lying around because I needed some more space. On my Arch Linux system I have all my drives encrypted like a good boy. It can be a bit tricky when you are adding them because you need to configure a few different files and add different UUID&amp;rsquo;s in each of them.
Here are the steps I follow to add a new disk. Note that this how to assumes that you already have set up your system with dm-crypt.</description>
      <content:encoded><![CDATA[<p>Today I added a harddisk I had lying around because I needed some more space. On my Arch Linux system I have all my drives encrypted like a good boy. It can be a bit tricky when you are adding them because you need to configure a few different files and add different UUID&rsquo;s in each of them.</p>
<p>Here are the steps I follow to add a new disk. Note that this how to assumes that you already have set up your system with dm-crypt.</p>
<p>List out the disks with lsblk:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">(</span>ins<span class="o">)[</span>mischa@arch-beast ~<span class="o">]</span>$ lsblk
</span></span><span class="line"><span class="cl">NAME          MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINTS
</span></span><span class="line"><span class="cl">sda             8:0    <span class="m">0</span> 223.6G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">└─sda1          8:1    <span class="m">0</span> 223.6G  <span class="m">0</span> part
</span></span><span class="line"><span class="cl">  └─games     254:0    <span class="m">0</span> 223.6G  <span class="m">0</span> crypt /games
</span></span><span class="line"><span class="cl">sdb             8:16   <span class="m">0</span> 931.5G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">└─sdb1          8:17   <span class="m">0</span> 931.5G  <span class="m">0</span> part
</span></span><span class="line"><span class="cl">  └─data-hdd  254:2    <span class="m">0</span> 931.5G  <span class="m">0</span> crypt /data-hdd
</span></span><span class="line"><span class="cl">sdc             8:32   <span class="m">0</span> 931.5G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">└─sdc1          8:33   <span class="m">0</span> 931.5G  <span class="m">0</span> part
</span></span><span class="line"><span class="cl">  └─data-hdd2 254:3    <span class="m">0</span> 931.5G  <span class="m">0</span> crypt /data-hdd2
</span></span><span class="line"><span class="cl">sdd             8:48   <span class="m">0</span> 465.8G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">├─sdd1          8:49   <span class="m">0</span>   300M  <span class="m">0</span> part  /boot
</span></span><span class="line"><span class="cl">└─sdd2          8:50   <span class="m">0</span> 465.5G  <span class="m">0</span> part
</span></span><span class="line"><span class="cl">  └─root      254:1    <span class="m">0</span> 465.5G  <span class="m">0</span> crypt /
</span></span><span class="line"><span class="cl">sde             8:64   <span class="m">0</span> 931.5G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">└─sde1          8:65   <span class="m">0</span> 931.5G  <span class="m">0</span> part
</span></span></code></pre></div><p>I will be adding /dev/sde to my system. As you see, I already created a partition on it, named <code>sde1</code>. The mountpoint for the disk will be <code>/data-hdd3</code>.</p>
<p>If you still need to add your partition, use <code>sudo gdisk /dev/sde</code> to write a new table and partition.</p>
<h2 id="encryption">encryption</h2>
<p>First I create the mount point I&rsquo;ll use and set the appropriate permisssions:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo mkdir /data-hdd3
</span></span><span class="line"><span class="cl">sudo chown mischa:mischa /data-hdd3
</span></span></code></pre></div><p>Now we create a LUKS header and an encrypted filesystem on the disk.
Note that I&rsquo;m using the notation convention from the Arch Wiki where the &ldquo;#&rdquo; indicates that the command should be run as root.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># cryptsetup -y -v luksFormat /dev/sde1</span>
</span></span><span class="line"><span class="cl"><span class="c1"># cryptsetup open /dev/sde1 data-hdd3 </span>
</span></span><span class="line"><span class="cl"><span class="c1"># mkfs.ext4 /dev/mapper/data-hdd3</span>
</span></span><span class="line"><span class="cl"><span class="c1"># mount /dev/mapper/data-hdd3 /data-hdd3</span>
</span></span></code></pre></div><p>Verify that it worked and the new encrypted partition is mounted:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">arch-beast# lsblk
</span></span><span class="line"><span class="cl">NAME          MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINTS
</span></span><span class="line"><span class="cl">sda             8:0    <span class="m">0</span> 223.6G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">└─sda1          8:1    <span class="m">0</span> 223.6G  <span class="m">0</span> part
</span></span><span class="line"><span class="cl">  └─games     254:0    <span class="m">0</span> 223.6G  <span class="m">0</span> crypt /games
</span></span><span class="line"><span class="cl">sdb             8:16   <span class="m">0</span> 931.5G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">└─sdb1          8:17   <span class="m">0</span> 931.5G  <span class="m">0</span> part
</span></span><span class="line"><span class="cl">  └─data-hdd  254:2    <span class="m">0</span> 931.5G  <span class="m">0</span> crypt /data-hdd
</span></span><span class="line"><span class="cl">sdc             8:32   <span class="m">0</span> 931.5G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">└─sdc1          8:33   <span class="m">0</span> 931.5G  <span class="m">0</span> part
</span></span><span class="line"><span class="cl">  └─data-hdd2 254:3    <span class="m">0</span> 931.5G  <span class="m">0</span> crypt /data-hdd2
</span></span><span class="line"><span class="cl">sdd             8:48   <span class="m">0</span> 465.8G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">├─sdd1          8:49   <span class="m">0</span>   300M  <span class="m">0</span> part  /boot
</span></span><span class="line"><span class="cl">└─sdd2          8:50   <span class="m">0</span> 465.5G  <span class="m">0</span> part
</span></span><span class="line"><span class="cl">  └─root      254:1    <span class="m">0</span> 465.5G  <span class="m">0</span> crypt /
</span></span><span class="line"><span class="cl">sde             8:64   <span class="m">0</span> 931.5G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">└─sde1          8:65   <span class="m">0</span> 931.5G  <span class="m">0</span> part
</span></span><span class="line"><span class="cl">  └─data-hdd3 254:4    <span class="m">0</span> 931.5G  <span class="m">0</span> crypt /data-hdd3
</span></span></code></pre></div><h2 id="auto-mounting-at-boot">Auto mounting at boot</h2>
<p>We&rsquo;ll need to add this disk to the kerenel parameters, /etc/crypttab and /etc/fstab. I haven&rsquo;t gotten round to switching to systemd boot yet, but I will do so very soon.</p>
<p>Open tmux and split the pane. In the bottom pane, run <code>lsblk -f</code> to have all the UUIDs listed. Then open the grub configuration file with <code>sudoedit /etc/default/grub</code></p>
<p><img loading="lazy" src="/luks1.png" type="" alt=""  /></p>
<p>You can discern which uuid to add from the listed examples. For my new disk, I needed to add the following:</p>
<p><code>rd.luks.name=3169af6c-a129-448e-b451-d7767866f607 data-hdd3=/dev/mapper/data-hdd3</code></p>
<p>Then run <code>sudo grub-mkconfig -o /boot/grub/grub.cfg</code> to update grub with the new settings. Adjust the path if you use a different path for your boot partition!</p>
<p>Next, we add it to /etc/crypttab</p>
<p><img loading="lazy" src="/luks2.png" type="" alt=""  /></p>
<p>To mount the new encrypted partition at boot, we add it to /etc/fstab.</p>
<p><strong>Note that this time we need to use the UUID of the partition located at /dev/mapper/data-hdd3</strong></p>
<p><img loading="lazy" src="/luks3.png" type="" alt=""  /></p>
<p>Use <code>sudo findmnt --verify</code> to check if there is antyhing wrong with the file.</p>
<p>Now you should be able to reboot and your new encrypted disk should be mounted automatically.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">(</span>ins<span class="o">)[</span>mischa@arch-beast ~<span class="o">]</span>$ lsblk
</span></span><span class="line"><span class="cl">NAME          MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINTS
</span></span><span class="line"><span class="cl">sda             8:0    <span class="m">0</span> 223.6G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">└─sda1          8:1    <span class="m">0</span> 223.6G  <span class="m">0</span> part
</span></span><span class="line"><span class="cl">  └─games     254:1    <span class="m">0</span> 223.6G  <span class="m">0</span> crypt /games
</span></span><span class="line"><span class="cl">sdb             8:16   <span class="m">0</span> 931.5G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">└─sdb1          8:17   <span class="m">0</span> 931.5G  <span class="m">0</span> part
</span></span><span class="line"><span class="cl">  └─data-hdd  254:0    <span class="m">0</span> 931.5G  <span class="m">0</span> crypt /data-hdd
</span></span><span class="line"><span class="cl">sdc             8:32   <span class="m">0</span> 931.5G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">└─sdc1          8:33   <span class="m">0</span> 931.5G  <span class="m">0</span> part
</span></span><span class="line"><span class="cl">  └─data-hdd2 254:4    <span class="m">0</span> 931.5G  <span class="m">0</span> crypt /data-hdd2
</span></span><span class="line"><span class="cl">sdd             8:48   <span class="m">0</span> 465.8G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">├─sdd1          8:49   <span class="m">0</span>   300M  <span class="m">0</span> part  /boot
</span></span><span class="line"><span class="cl">└─sdd2          8:50   <span class="m">0</span> 465.5G  <span class="m">0</span> part
</span></span><span class="line"><span class="cl">  └─root      254:2    <span class="m">0</span> 465.5G  <span class="m">0</span> crypt /
</span></span><span class="line"><span class="cl">sde             8:64   <span class="m">0</span> 931.5G  <span class="m">0</span> disk
</span></span><span class="line"><span class="cl">└─sde1          8:65   <span class="m">0</span> 931.5G  <span class="m">0</span> part
</span></span><span class="line"><span class="cl">  └─data-hdd3 254:3    <span class="m">0</span> 931.5G  <span class="m">0</span> crypt /data-hdd3
</span></span></code></pre></div><h1 id="links">links</h1>
<p><a href="https://wiki.archlinux.org/title/Dm-crypt/Encrypting_an_entire_system#LUKS_on_a_partition">https://wiki.archlinux.org/title/Dm-crypt/Encrypting_an_entire_system#LUKS_on_a_partition</a></p>
<p><a href="https://wiki.archlinux.org/title/GRUB">https://wiki.archlinux.org/title/GRUB</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Setting up automated backups on my Arch Linux system with rsync and bash</title>
      <link>https://mischavandenburg.com/zet/arch-backup-setup-1/</link>
      <pubDate>Sat, 21 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/arch-backup-setup-1/</guid>
      <description>For the past few months I&amp;rsquo;ve been stuyding every hour of free time that I had. Now that I reached my certification goals for now, I finally had some time to do a chore I had been meaning to do for a long time.
My Arch Linux system is fully encrypted, and I make backups. But I was still doing it a bit haphazardly, usually every Friday.
I wanted to automate this for a long time now, but I never got round to it.</description>
      <content:encoded><![CDATA[<p>For the past few months I&rsquo;ve been stuyding every hour of free time that I had. Now that I reached my certification goals for now, I finally had some time to do a chore I had been meaning to do for a long time.</p>
<p>My Arch Linux system is fully encrypted, and I make backups. But I was still doing it a bit haphazardly, usually every Friday.</p>
<p>I wanted to automate this for a long time now, but I never got round to it. Today I made the first steps, but it is still in progress.</p>
<p>Naturally, I could use a tool like Timeshift or something similar to schedule my backups. However, I want to do it myself using rsync because I want to fully understand what I am backing up, when, and where. Rsync is also used in our environment at work, so I assume it is more common in enterprise and production environments.</p>
<h2 id="full-system-backup">full system backup</h2>
<p>Before I was making a full system backup every Friday using this command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo rsync -aAXH --info<span class="o">=</span>stats1,progress2 --exclude<span class="o">={</span><span class="s2">&#34;/dev/*&#34;</span>,<span class="s2">&#34;/proc/*&#34;</span>,<span class="s2">&#34;/sys/*&#34;</span>,<span class="s2">&#34;/tmp/*&#34;</span>,<span class="s2">&#34;/run/*&#34;</span>,<span class="s2">&#34;/mnt/*&#34;</span>,<span class="s2">&#34;/media/*&#34;</span>,<span class="s2">&#34;/lost+found&#34;</span>,<span class="s2">&#34;/home/*/.cache/*&#34;</span>,<span class="s2">&#34;/data-hdd/&#34;</span>,<span class="s2">&#34;/games/&#34;</span>,<span class="s2">&#34;/var/lib/docker/*&#34;</span>,<span class="s2">&#34;/home/mischa/music/*&#34;</span>,<span class="s2">&#34;/swapfile&#34;</span>, <span class="s2">&#34;/data-hdd2/&#34;</span>, <span class="s2">&#34;/data-hdd3/&#34;</span><span class="o">}</span> / /data-hdd/backups/arch-beast/01-01-23
</span></span></code></pre></div><p>This command creates a full backup of my entire root filesystem, and it should be possible to restore my entire system by just reversing the target and destination in the end.</p>
<p>However, as I was coming up with my new strategy, I thought this was overkill.</p>
<h2 id="slimming-down">slimming down</h2>
<p>All I really need to back up is my home directory and it would be nice to have my /etc directory backed up as well.</p>
<p>So I wrote a simple shell script to do this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="cp">#!/bin/bash
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="nv">BACKUPS_DESTINATION</span><span class="o">=</span><span class="s2">&#34;/data-hdd/backups/arch-beast&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># format:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># rsync -a --delete --quiet /path/to/backup /location/of/backup</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># stop the script if an error occurs</span>
</span></span><span class="line"><span class="cl"><span class="nb">set</span> -e
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">rsync -a --delete --quiet --exclude<span class="o">=</span><span class="s2">&#34;{&#34;</span>/home/*/.cache/*<span class="s2">&#34;}&#34;</span> /home/mischa <span class="nv">$BACKUPS_DESTINATION</span>/home
</span></span><span class="line"><span class="cl">rsync -a --delete --quiet /etc <span class="nv">$BACKUPS_DESTINATION</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;Made backups on: </span><span class="k">$(</span>date<span class="k">)</span><span class="s2">&#34;</span> &gt;&gt; /var/log/backup.log
</span></span></code></pre></div><p>-a flag from man page:</p>
<p><em>&ldquo;This  is  equivalent to -rlptgoD.  It is a quick way of saying you want recursion and want to preserve almost everything.&rdquo;</em></p>
<p>&ndash;delete: means files deleted on the source are to be deleted on the backup as well</p>
<h2 id="automation">automation</h2>
<p>I have a few scripts running in cronjobs on my system. I have a goal of putting them all in systemd timers, but I haven&rsquo;t gotten round to it yet. For now, I will just add my backup scripts to my existing cronjobs setup.</p>
<p>To make my backups every day, I added this to my crontab:</p>
<p><code>0 12 * * * /bin/bash /home/mischa/git/lab/bash/backup</code></p>
<p>Every day it will make a backup to the same directory and update the changed files, or delete the files I deleted from my system.</p>
<p>I also wanted to have a weekly backup happening on Monday.</p>
<p>I will make a more elaborate script to make a weekly directory, and rotate it with a new directory every week. But for now, I just chose a quick solution by creating a weekly version of my script and running it every Monday.</p>
<p>The only difference is the path:</p>
<p><code>BACKUPS_DESTINATION=&quot;/data-hdd/backups/arch-beast/weekly&quot;</code></p>
<p>In the crontab:</p>
<p><code>0 10 * * 1 /bin/bash /home/mischa/git/lab/bash/backup-weekly</code></p>
<h2 id="to-do">to do</h2>
<ul>
<li>set up weekly backup in the same script</li>
<li>create error handling and improve logging</li>
<li>set up in systemd timers instead of crontab</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>I passed the AZ-400 DevOps Expert today</title>
      <link>https://mischavandenburg.com/zet/passed-az-400/</link>
      <pubDate>Sat, 14 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/passed-az-400/</guid>
      <description>I&amp;rsquo;m typing this 30 minutes after I passed my AZ-400 exam. I&amp;rsquo;m sitting in a lovely cafe on Leidseplein in Amsterdam and feel relieved. Another significant certification bites the dust. This one took about 70 hours of study.
I started preparing immediately after passing my AZ-104 exam, which was a good move. The AZ-400 requires you to know many details about Azure services and how to access them. For example, Shared Access Signatures are only used for accessing storage accounts, but they came up quite often as alternative answers to the questions.</description>
      <content:encoded><![CDATA[<p>I&rsquo;m typing this 30 minutes after I passed my AZ-400 exam. I&rsquo;m sitting in a lovely cafe on Leidseplein in Amsterdam and feel relieved. Another significant certification bites the dust. This one took about 70 hours of study.</p>
<p>I started preparing immediately after passing my AZ-104 exam, which was a good move. The AZ-400 requires you to know many details about Azure services and how to access them. For example, Shared Access Signatures are only used for accessing storage accounts, but they came up quite often as alternative answers to the questions.</p>
<p>The exam itself was difficult, but the AZ-104 was harder. The AZ-104 exam was more challenging because the questions were complicated and required you to simultaneously balance many different factors in the mind. The AZ-400 was difficult because the answer alternatives that are provided are incredibly similar to each other, and they make you very insecure about what the right choice might be. As a result, I changed my answers many times.</p>
<p>I will do another study guide for this certification soon and publish my notes and Anki deck too. Now it&rsquo;s time to celebrate and relax a little.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Azure DevOps Personal Access Tokens are always for authenticating into ADO</title>
      <link>https://mischavandenburg.com/zet/azure-personal-access-tokens/</link>
      <pubDate>Fri, 13 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/azure-personal-access-tokens/</guid>
      <description>The PAT (Personal Access Token) often comes up during practice tests for the AZ-400.
One way to remember when to use a PAT is that these are only for authenticating into Azure DevOps, never to external services.
For example, you might get a question on connecting your Azure DevOps project with a GitHub account from Azure DevOps, and PAT will show up as one of the alternative answers. By remembering that PATs are only for authenticating into ADO, you can elminate this alternative, and make your choice easier.</description>
      <content:encoded><![CDATA[<p>The PAT (Personal Access Token) often comes up during practice tests for the AZ-400.</p>
<p>One way to remember when to use a PAT is that these are only for authenticating <strong>into</strong> Azure DevOps, never to external services.</p>
<p>For example, you might get a question on connecting your Azure DevOps project with a GitHub account from Azure DevOps, and PAT will show up as one of the alternative answers. By remembering that PATs are only for authenticating into ADO, you can elminate this alternative, and make your choice easier.</p>
<p>Personal Access Tokens are an alternative to passwords but should be treated in exactly the same way.</p>
<p><a href="https://learn.microsoft.com/en-us/azure/devops/organizations/accounts/use-personal-access-tokens-to-authenticate?view=azure-devops&amp;tabs=Windows">https://learn.microsoft.com/en-us/azure/devops/organizations/accounts/use-personal-access-tokens-to-authenticate?view=azure-devops&amp;tabs=Windows</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>The Pomodoro technique has won me over</title>
      <link>https://mischavandenburg.com/zet/pomodoro/</link>
      <pubDate>Fri, 13 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/pomodoro/</guid>
      <description>I did a lot of studying last year, and I achieved a few tough certifications. I&amp;rsquo;ve always been good at studying and never struggled with getting decent grades in university. As a result, I never felt the need to use particular techniques to pass my tests. However, now that I need to do my studies combined with a full-time job, I did some optimization and looked into study techniques.
One technique I&amp;rsquo;ve become very fond of is the Pomodoro Technique.</description>
      <content:encoded><![CDATA[<p>I did a lot of studying last year, and I achieved a few tough certifications. I&rsquo;ve always been good at studying and never struggled with getting decent grades in university. As a result, I never felt the need to use particular techniques to pass my tests. However, now that I need to do my studies combined with a full-time job, I did some optimization and looked into study techniques.</p>
<p>One technique I&rsquo;ve become very fond of is the Pomodoro Technique. I don&rsquo;t have any problems focusing for long periods, but I still decided to try it. I use the standard 25-minute study with a 5-minute break routine, and after four cycles, I take a 30-minute break.</p>
<p>The Pomodoro technique has been a way to force myself to take breaks, which I wasn&rsquo;t used to. I used to chip away at a specific task for hours. However, I discovered that when I take a break, walk around for five minutes, and apply myself to the task again, my mind is in a fresh state and much more receptive to the information. Perhaps the time I spend studying after a break is actually more productive because the mind had a little rest.</p>
<p>The technique also pushed my limits a bit more. I study more hours a day, considering that I also work full time. There is this moment where I want to quit studying, but I ask myself, &ldquo;do I have another Pomodoro in me?&rdquo;</p>
<p>Now that I&rsquo;ve gotten used to breaking things up into 25-minute chunks of time, I started using the Pomodoro technique for other areas in life as well, such as blog writing or coding projects.</p>
<p>You can use any tool you like to start using the Pomodoro technique and pick any break schedule that suits you. I&rsquo;ll link some resources below. All you need is some sort of timer. You can use a timer on your computer or a physical timer. I use the Forest app on my iPhone because it integrates with the iOs &ldquo;do not disturb&rdquo; and &ldquo;focus&rdquo; modes, so  I don&rsquo;t get any distracting notifications when I&rsquo;m on a Pomodoro.</p>
<p><a href="https://science.nichd.nih.gov/confluence/display/newsletter/2020/05/07/The+Pomodoro+Technique%3A+An+Effective+Time+Management+Tool">https://science.nichd.nih.gov/confluence/display/newsletter/2020/05/07/The+Pomodoro+Technique%3A+An+Effective+Time+Management+Tool</a></p>
<p><a href="https://www.youtube.com/watch?v=5WRO79zuJ4U">https://www.youtube.com/watch?v=5WRO79zuJ4U</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Using parameter expansion as search and replace</title>
      <link>https://mischavandenburg.com/zet/slash-syntax-replace/</link>
      <pubDate>Tue, 10 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/slash-syntax-replace/</guid>
      <description>Last modified: 2023-01-10
In this evening&amp;rsquo;s studies I came across this bash script in a tutorial by Rob Muhlenstein:
!#/bin/bash echo -e ${PATH//:/\\n} I could not make heads or tails of all these slashes and curly braces, since the output clearly indicated that search and replacement was being performed. I&amp;rsquo;m used to the sed / vim syntax: s/foo/bar
After some research I learned that &amp;lsquo;//&amp;rsquo; is a global search and replace syntax of several text processing programs.</description>
      <content:encoded><![CDATA[<p><em>Last modified: 2023-01-10</em></p>
<p>In this evening&rsquo;s studies I came across this bash script in a tutorial by Rob Muhlenstein:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">!#/bin/bash
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> -e <span class="si">${</span><span class="nv">PATH</span><span class="p">//:/</span><span class="se">\\</span><span class="nv">n</span><span class="si">}</span>
</span></span></code></pre></div><p>I could not make heads or tails of all these slashes and curly braces, since the output clearly indicated that search and replacement was being performed. I&rsquo;m used to the sed / vim syntax: <code>s/foo/bar</code></p>
<p>After some research I learned that &lsquo;//&rsquo; is a global search and replace syntax of several text processing programs. It is known as parameter expansion in bash.</p>
<p>Example:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nv">foo</span><span class="o">=</span><span class="s2">&#34;1234567890&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">foo</span><span class="p">//[0-9]/x</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span></code></pre></div><p>This replaces all the digits in the $foo variable with &lsquo;x&rsquo;, so the output would be xxxxxxxxxx</p>
<p>To do this with sed, you would do:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;</span><span class="nv">$foo</span><span class="s2">&#34;</span> <span class="p">|</span> sed <span class="s1">&#39;s/[0-9]/x/g&#39;</span>
</span></span></code></pre></div><p>For more info:</p>
<p><code>man bash</code></p>
<p><code>/parameter expansion</code></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Automatically adding my recent blog posts to my GitHub Readme</title>
      <link>https://mischavandenburg.com/zet/adding-posts-github-readme/</link>
      <pubDate>Mon, 09 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/adding-posts-github-readme/</guid>
      <description>My friend gave me a nice tip for customizing the readme on my personal GitHub page. I discovered there is a whole world of plugins and customizations out there.
I set up this one for my GitHub homepage. It uses a workflow to update the readme in my personal GitHub repo with the most recent posts from this blog, based on the RSS feed. Neat!
It was very easy to set up.</description>
      <content:encoded><![CDATA[<p>My friend gave me a nice tip for customizing the readme on my personal GitHub page. I discovered there is a whole world of plugins and customizations out there.</p>
<p>I set up <a href="https://github.com/abhisheknaiidu/awesome-github-profile-readme">this one</a> for my GitHub homepage. It uses a workflow to update the readme in my personal GitHub repo with the most recent posts from this blog, based on the RSS feed. Neat!</p>
<p>It was very easy to set up. If you don&rsquo;t have your own blog, you could configure it with a different RSS feed. Hacker News for example.</p>
<p><a href="https://github.com/gautamkrishnar/blog-post-workflow">https://github.com/gautamkrishnar/blog-post-workflow</a></p>
<p><a href="https://github.com/abhisheknaiidu/awesome-github-profile-readme">https://github.com/abhisheknaiidu/awesome-github-profile-readme</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Back to Bas(h)ics: leaving zsh for now</title>
      <link>https://mischavandenburg.com/zet/back-to-bashics/</link>
      <pubDate>Sun, 08 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/back-to-bashics/</guid>
      <description>I&amp;rsquo;ve used zsh for nearly two years now. I have a custom setup with autocompletion and a good looking prompt.
Recently I&amp;rsquo;ve been diving deeper into bash scripting, following tutorials by rwxrob. He emphasizes all the time that it is much better to stick to bash instead of zsh.
Advantages of using bash:
the default Linux shell available on any Linux system full documentation available anywhere at all times with man bash free software less dependent on external plugins and configurations more portable practice by working on the command line The fact that working on the commandline is already coding convinced me to leave my beloved customized prompt behind (for now) and go back to the basics.</description>
      <content:encoded><![CDATA[<p>I&rsquo;ve used zsh for nearly two years now. I have a custom setup with autocompletion and a good looking prompt.</p>
<p>Recently I&rsquo;ve been diving deeper into bash scripting, following tutorials by rwxrob. He emphasizes all the time that it is much better to stick to bash instead of zsh.</p>
<p>Advantages of using bash:</p>
<ul>
<li>the default Linux shell</li>
<li>available on any Linux system</li>
<li>full documentation available anywhere at all times with <code>man bash</code></li>
<li>free software</li>
<li>less dependent on external plugins and configurations</li>
<li>more portable</li>
<li>practice by working on the command line</li>
</ul>
<p>The fact that <a href="/content/zet/bash-cmdline-is-coding.md">working on the commandline is already coding</a> convinced me to leave my beloved customized prompt behind (for now) and go back to the basics.</p>
<p>I want to improve my bash scripting, and working in the bash shell will improve that just by virtue of doing my daily tasks on the command line.</p>
<p>Also I noticed I&rsquo;ve gotten used to zsh&rsquo;s excellent autocompletion and menu navigation. When I log in to servers at work, there is always this little moment of &ldquo;oh, I don&rsquo;t have that here&rdquo;. I want to get better at bash so I&rsquo;m not dependent on these external crutches anymore.</p>
<p>Also, I&rsquo;m going to port my zsh configuration to bash. My current zsh configuration loads a bunch of plugins, and it is more of a hassle to get set up on a new system.</p>
<p>I want to be able to pull my dotfiles repo and do very few steps to configure my environment.</p>
<p>But I&rsquo;m going to miss that good-looking prompt with all the lovely icons!</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>How to build and deploy a Docker container to an Azure VM using Azure Pipelines</title>
      <link>https://mischavandenburg.com/zet/docker-to-azure-vm/</link>
      <pubDate>Sun, 08 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/docker-to-azure-vm/</guid>
      <description>I wanted to build an application from a Dockerfile and deploy it to a VM. I used a default Svelte setup as an example app.
Naturally, Azure prefers that you deploy containers to services such as Azure Container Instances or App Services, so they don&amp;rsquo;t provide modules for the pipelines to deploy to docker servers as far as I could tell.
I searched for a long time but I could not find a solution.</description>
      <content:encoded><![CDATA[<p>I wanted to build an application from a Dockerfile and deploy it to a VM. I used a default Svelte setup as an example app.</p>
<p>Naturally, Azure prefers that you deploy containers to services such as Azure Container Instances or App Services, so they don&rsquo;t provide modules for the pipelines to deploy to docker servers as far as I could tell.</p>
<p>I searched for a long time but I could not find a solution. In the end I just ran shell commands from the pipeline to run the container on on the server.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">steps</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">script</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">      sudo docker stop svelte-test
</span></span></span><span class="line"><span class="cl"><span class="sd">      sudo docker rm svelte-test
</span></span></span><span class="line"><span class="cl"><span class="sd">      sudo docker run --name svelte-test -p 8080:80 -d mischavandenburg/svelte-test:$(Build.BuildId)</span><span class="w">      
</span></span></span></code></pre></div><p>You can find the full pipeline code, the app and Dockerfile in my lab repo:</p>
<p><a href="https://github.com/mischavandenburg/lab/tree/main/azure-pipelines/docker-to-azure-vm">https://github.com/mischavandenburg/lab/tree/main/azure-pipelines/docker-to-azure-vm</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>How to deploy to a Linux VM in Azure with Azure Pipelines</title>
      <link>https://mischavandenburg.com/zet/azure-pipelines-deploy-vm/</link>
      <pubDate>Sun, 08 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/azure-pipelines-deploy-vm/</guid>
      <description>To reach a VM from Azure Pipelines, you need to set up an environment.
Create your Linux VM in Azure.
In Azure DevOps, click envirnoments, new, and select the Virtual Machine option.
A command is generated for you. SSH into your VM and run the command.
Now the VM should show up under environments in Azure DevOps.
Set up a repo with an azure-pipelines.yml with these contents to test. under environment, set the same name as you did in Azure DevOps for your environment.</description>
      <content:encoded><![CDATA[<p>To reach a VM from Azure Pipelines, you need to set up an environment.</p>
<p>Create your Linux VM in Azure.</p>
<p>In Azure DevOps, click envirnoments, new, and select the Virtual Machine option.</p>
<p><img loading="lazy" src="/env1.png" type="" alt=""  /></p>
<p>A command is generated for you. SSH into your VM and run the command.</p>
<p><img loading="lazy" src="/env2.png" type="" alt=""  /></p>
<p>Now the VM should show up under environments in Azure DevOps.</p>
<p>Set up a repo with an azure-pipelines.yml with these contents to test. under <code>environment</code>, set the same name as you did in Azure DevOps for your environment.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">trigger</span><span class="p">:</span><span class="w"> 
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="l">main</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">pool</span><span class="p">:</span><span class="w"> 
</span></span></span><span class="line"><span class="cl"><span class="w">   </span><span class="nt">vmImage</span><span class="p">:</span><span class="w"> </span><span class="l">ubuntu-latest</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">jobs</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">deployment</span><span class="p">:</span><span class="w"> </span><span class="l">VMDeploy</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">displayName</span><span class="p">:</span><span class="w"> </span><span class="l">Deploy to VM</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">environment</span><span class="p">:</span><span class="w"> 
</span></span></span><span class="line"><span class="cl"><span class="w">   </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">dev</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">   </span><span class="nt">resourceType</span><span class="p">:</span><span class="w"> </span><span class="l">VirtualMachine</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">strategy</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">     </span><span class="nt">runOnce</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">deploy</span><span class="p">:</span><span class="w">   
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">steps</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span>- <span class="nt">script</span><span class="p">:</span><span class="w"> </span><span class="l">echo &#34;Hello world&#34;</span><span class="w">
</span></span></span></code></pre></div><p>You can see it when the deploy runs on the VM:</p>
<p><img loading="lazy" src="/env3.png" type="" alt=""  /></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Deploying a Linux VM to Azure with Terraform</title>
      <link>https://mischavandenburg.com/zet/terraform-linux-vm/</link>
      <pubDate>Sat, 07 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/terraform-linux-vm/</guid>
      <description>For a project I&amp;rsquo;m setting up my environment with Terraform.
I used this tutorial, but modified the code to make it simpler and easier to understand for beginners. The original uses a random module to generate random names, and generates a new SSH key. Also, this tutorial uses expensive VM tiers and Premium storage, which are not necessary when you are learning.
I also thought the SSH configuration was overcomplicated. My version just takes an SSH keypair stored at ~/.</description>
      <content:encoded><![CDATA[<p>For a project I&rsquo;m setting up my environment with Terraform.</p>
<p>I used <a href="https://learn.microsoft.com/en-us/azure/virtual-machines/linux/quick-create-terraform">this tutorial</a>, but modified the code to make it simpler and easier to understand for beginners. The original uses a random module to generate random names, and generates a new SSH key. Also, this tutorial uses expensive VM tiers and Premium storage, which are not necessary when you are learning.</p>
<p>I also thought the SSH configuration was overcomplicated. My version just takes an SSH keypair stored at <code>~/.ssh/id_rsa.pub</code></p>
<p>To run:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-hcl" data-lang="hcl"><span class="line"><span class="cl"><span class="k">terraform</span> <span class="k">init</span>
</span></span><span class="line"><span class="cl"><span class="k">terraform</span> <span class="k">plan</span>
</span></span><span class="line"><span class="cl"><span class="k">terraform</span> <span class="k">apply</span>
</span></span></code></pre></div><p>The scripts prints the public IP of the newly created VM. You should be able to SSH to it:</p>
<p><code>ssh azureuser@the_printed_ip_address</code></p>
<p>You can find the code in <a href="https://github.com/mischavandenburg/lab/tree/main/terraform/azure-simple-linux-vm">my &ldquo;lab&rdquo; repo on GitHub.</a></p>
<p><a href="https://github.com/mischavandenburg/lab/tree/main/terraform/azure-simple-linux-vm">https://github.com/mischavandenburg/lab/tree/main/terraform/azure-simple-linux-vm</a></p>
<p><a href="https://learn.microsoft.com/en-us/azure/virtual-machines/linux/quick-create-terraform">https://learn.microsoft.com/en-us/azure/virtual-machines/linux/quick-create-terraform</a></p>
<p><a href="https://learn.microsoft.com/en-us/azure/developer/terraform/authenticate-to-azure?source=recommendations&amp;tabs=bash#authenticate-to-azure-via-a-microsoft-account">https://learn.microsoft.com/en-us/azure/developer/terraform/authenticate-to-azure?source=recommendations&amp;tabs=bash#authenticate-to-azure-via-a-microsoft-account</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>How to follow symbolic links while searching with Telescope in neovim</title>
      <link>https://mischavandenburg.com/zet/neovim-telescope-follow-symlinks/</link>
      <pubDate>Fri, 06 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/neovim-telescope-follow-symlinks/</guid>
      <description>I use the Obsidian app, but I mostly write and search my notes with neovim. I added my zet directory from this blog repo into the Obsidian vault as a symbolic link, but I soon discovered that these files were not being searched.
Telescope.nvim uses ripgrep (rg) to do the live grepping in its search, and ripgrep does not follow symbolic links by default. You need to pass the -L flag to it.</description>
      <content:encoded><![CDATA[<p>I use the <a href="/zet/articles/obsidian-introduction/">Obsidian</a> app, but I mostly write and search my notes with neovim. I added my zet directory from this blog repo into the Obsidian vault as a symbolic link, but I soon discovered that these files were not being searched.</p>
<p>Telescope.nvim uses ripgrep (rg) to do the live grepping in its search, and ripgrep does not follow symbolic links by default. You need to pass the -L flag to it.</p>
<p>To pass the -L flag, and some other flags, I added the following to my telescope config file:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-lua" data-lang="lua"><span class="line"><span class="cl"><span class="c1">-- Custom ripgrep configuration:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kd">local</span> <span class="n">telescope</span> <span class="o">=</span> <span class="n">require</span><span class="p">(</span><span class="s2">&#34;telescope&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="kd">local</span> <span class="n">telescopeConfig</span> <span class="o">=</span> <span class="n">require</span><span class="p">(</span><span class="s2">&#34;telescope.config&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">-- Clone the default Telescope configuration</span>
</span></span><span class="line"><span class="cl"><span class="kd">local</span> <span class="n">vimgrep_arguments</span> <span class="o">=</span> <span class="p">{</span> <span class="n">unpack</span><span class="p">(</span><span class="n">telescopeConfig.values</span><span class="p">.</span><span class="n">vimgrep_arguments</span><span class="p">)</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">-- I want to search in hidden/dot files.</span>
</span></span><span class="line"><span class="cl"><span class="n">table.insert</span><span class="p">(</span><span class="n">vimgrep_arguments</span><span class="p">,</span> <span class="s2">&#34;--hidden&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1">-- I don&#39;t want to search in the `.git` directory.</span>
</span></span><span class="line"><span class="cl"><span class="n">table.insert</span><span class="p">(</span><span class="n">vimgrep_arguments</span><span class="p">,</span> <span class="s2">&#34;--glob&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">table.insert</span><span class="p">(</span><span class="n">vimgrep_arguments</span><span class="p">,</span> <span class="s2">&#34;!**/.git/*&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">-- I want to follow symbolic links</span>
</span></span><span class="line"><span class="cl"><span class="n">table.insert</span><span class="p">(</span><span class="n">vimgrep_arguments</span><span class="p">,</span> <span class="s2">&#34;-L&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">telescope.setup</span><span class="p">({</span>
</span></span><span class="line"><span class="cl">	<span class="n">defaults</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="c1">-- `hidden = true` is not supported in text grep commands.</span>
</span></span><span class="line"><span class="cl">		<span class="n">vimgrep_arguments</span> <span class="o">=</span> <span class="n">vimgrep_arguments</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="p">},</span>
</span></span><span class="line"><span class="cl">	<span class="n">pickers</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">find_files</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="c1">-- `hidden = true` will still show the inside of `.git/` as it&#39;s not `.gitignore`d.</span>
</span></span><span class="line"><span class="cl">			<span class="n">find_command</span> <span class="o">=</span> <span class="p">{</span> <span class="s2">&#34;rg&#34;</span><span class="p">,</span> <span class="s2">&#34;--files&#34;</span><span class="p">,</span> <span class="s2">&#34;--hidden&#34;</span><span class="p">,</span> <span class="s2">&#34;--glob&#34;</span><span class="p">,</span> <span class="s2">&#34;!**/.git/*&#34;</span><span class="p">,</span> <span class="s2">&#34;-L&#34;</span> <span class="p">},</span>
</span></span><span class="line"><span class="cl">		<span class="p">},</span>
</span></span><span class="line"><span class="cl">	<span class="p">},</span>
</span></span><span class="line"><span class="cl"><span class="p">})</span>
</span></span></code></pre></div><p>Based on the configuration examples found on the project&rsquo;s GitHub page.</p>
<p><a href="https://github.com/nvim-telescope/telescope.nvim">https://github.com/nvim-telescope/telescope.nvim</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Attaching the Ansible Language Server to yaml files in neovim (LSP)</title>
      <link>https://mischavandenburg.com/zet/ansible-lsp-fix/</link>
      <pubDate>Thu, 05 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/ansible-lsp-fix/</guid>
      <description>When you have an Ansible language server installed, you might find that your yaml LSP will attach to your current buffer, but the ansible language server won&amp;rsquo;t attach.
You can fix this by setting the correct file type for the current buffer:
:set ft=yaml.ansible
You could also adjust the Ansible LSP so it attaches to all yaml files. However, this does not work out for me, because I edit different yaml files for different purposes every day.</description>
      <content:encoded><![CDATA[<p>When you have an Ansible language server installed, you might find that your yaml LSP will attach to your current buffer, but the ansible language server won&rsquo;t attach.</p>
<p>You can fix this by setting the correct file type for the current buffer:</p>
<p><code>:set ft=yaml.ansible</code></p>
<p>You could also adjust the Ansible LSP so it attaches to all yaml files. However, this does not work out for me, because I edit different yaml files for different purposes every day. Not all yaml files are to be used with Ansible.</p>
<p>There is logic for the Ansible language server to figure out if you are working on Ansible yaml files based on the directory structure you&rsquo;re working in.</p>
<p>So setting the filetype when I needed works well for me.</p>
<p><a href="https://www.reddit.com/r/neovim/comments/tbd7g0/lsp_ansiblels_wont_attach_anymore/">https://www.reddit.com/r/neovim/comments/tbd7g0/lsp_ansiblels_wont_attach_anymore/</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>How to install the Openstack CLI on Linux</title>
      <link>https://mischavandenburg.com/zet/install-openstack-cli/</link>
      <pubDate>Thu, 05 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/install-openstack-cli/</guid>
      <description>Make sure to have pip installed.
Run pip install python-openstackclient
Pip will install a binary called &amp;ldquo;openstack&amp;rdquo; in ~/.local/bin
If the openstack command is not available in your session, you might need to add it to your PATH:
export PATH=&amp;quot;$HOME/.local/bin:$PATH&amp;quot;
Add this to your ~/.zshrc or ~/.bashrc to make sure this happens for each shell session.
Don&amp;rsquo;t forget to source your updated ~/.zshrc if you chose to add it:
source ~/.</description>
      <content:encoded><![CDATA[<p>Make sure to have pip installed.</p>
<p>Run <code>pip install python-openstackclient</code></p>
<p>Pip will install a binary called &ldquo;openstack&rdquo; in ~/.local/bin</p>
<p>If the openstack command is not available in your session, you might need to add it to your PATH:</p>
<p><code>export PATH=&quot;$HOME/.local/bin:$PATH&quot;</code></p>
<p>Add this to your ~/.zshrc or ~/.bashrc to make sure this happens for each shell session.</p>
<p>Don&rsquo;t forget to source your updated ~/.zshrc if you chose to add it:</p>
<p><code>source ~/.zshrc</code></p>
<p><a href="https://docs.openstack.org/newton/user-guide/common/cli-install-openstack-command-line-clients.html">https://docs.openstack.org/newton/user-guide/common/cli-install-openstack-command-line-clients.html</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>How to Reset a VM Root Password using the Openstack CLI</title>
      <link>https://mischavandenburg.com/zet/openstack-root-password/</link>
      <pubDate>Thu, 05 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/openstack-root-password/</guid>
      <description> Download the Openstack RC file from the Openstack portal. Click your username in the top right corner to find it. Source the RC file to make the environment variables avaialable to your current session: source ~/my_openstack.sh Find the instance ID of your VM from the portal. Run openstack server set --root-password be3xxxx5-8348-418b-xxxb-c4xxxx575cd You will be prompted for the new password which will be set on the virtual machine. </description>
      <content:encoded><![CDATA[<ol>
<li>Download the Openstack RC file from the Openstack portal. Click your username in the top right corner to find it.</li>
<li>Source the RC file to make the environment variables avaialable to your current session: <code>source ~/my_openstack.sh</code></li>
<li>Find the instance ID of your VM from the portal.</li>
<li>Run <code>openstack server set --root-password be3xxxx5-8348-418b-xxxb-c4xxxx575cd</code></li>
<li>You will be prompted for the new password which will be set on the virtual machine.</li>
</ol>
]]></content:encoded>
    </item>
    
    <item>
      <title>How to run installed pip packages as binaries</title>
      <link>https://mischavandenburg.com/zet/run-installed-pip-packages/</link>
      <pubDate>Thu, 05 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/run-installed-pip-packages/</guid>
      <description>When you install a pip package which is meant to be run from the command line as a command, you might find that it is not available to you after installation.
If this happens, it might be that the path is missing from your PATH variable. Therefore, the shell does not source these binaries when initiated, and does not know that these executables exist.
You can find the location of your binaries by running pip show package_name</description>
      <content:encoded><![CDATA[<p>When you install a pip package which is meant to be run from the command line as a command, you might find that it is not available to you after installation.</p>
<p>If this happens, it might be that the path is missing from your PATH variable. Therefore, the shell does not source these binaries when initiated, and does not know that these executables exist.</p>
<p>You can find the location of your binaries by running <code>pip show package_name</code></p>
<p>Usually the binaries will be located in ~/.local/bin on a UNIX based system.</p>
<p>To add this to your path, run:</p>
<p><code>export PATH=&quot;$HOME/.local/bin:$PATH&quot;</code></p>
<p>Add this to your ~/.zshrc or ~/.bashrc to make sure this happens for each shell session.</p>
<p>Don&rsquo;t forget to source your updated ~/.zshrc if you chose to add it:</p>
<p><code>source ~/.zshrc</code></p>
<p><a href="https://stackoverflow.com/questions/29980798/where-does-pip-install-its-packages">https://stackoverflow.com/questions/29980798/where-does-pip-install-its-packages</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Working on the command line is already coding</title>
      <link>https://mischavandenburg.com/zet/bash-cmdline-is-coding/</link>
      <pubDate>Thu, 05 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/bash-cmdline-is-coding/</guid>
      <description>Rob Muhlenstein makes an interesting point that using bash on the command line is already coding. When you are running commands in the terminal, you are coding one line at a time.
When you put these commands in a file you have a bash script. Therefore, he argues that bash should be your first language.
I think this is such an interesting point. I&amp;rsquo;ve been using Linux and working on the command line for years but it never dawned on me that I, in fact, was coding while working on the command line.</description>
      <content:encoded><![CDATA[<p>Rob Muhlenstein makes an interesting point that using bash on the command line is already coding. When you are running commands in the terminal, you are coding one line at a time.</p>
<p>When you put these commands in a file you have a bash script. Therefore, he argues that bash should be your first language.</p>
<p>I think this is such an interesting point. I&rsquo;ve been using Linux and working on the command line for years but it never dawned on me that I, in fact, was coding while working on the command line. However, when I was writing bash scripts, I did consider myself to be coding. There is literally no difference. A bash script is just a string of commands that you would enter manually anyway.</p>
<p><a href="https://rwx.gg/">https://rwx.gg/</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Fall in love with sed.</title>
      <link>https://mischavandenburg.com/zet/fall-in-love-with-sed/</link>
      <pubDate>Wed, 04 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/fall-in-love-with-sed/</guid>
      <description>Sed, it&amp;rsquo;s so powerful. I remember I struggled with finding practical uses for it when I did my LPIC-1 certification. But now I find myself using it several times a week. It is so powerful to edit multiple files at a time. I use it for work, but also for making changes to my entire second brain in Obsidian with one command.
Today I needed to update my /articles/ links to /zet/articles/ links because I&amp;rsquo;m restructuring my website.</description>
      <content:encoded><![CDATA[<p>Sed, it&rsquo;s so powerful. I remember I struggled with finding practical uses for it when I did my LPIC-1 certification. But now I find myself using it several times a week. It is so powerful to edit multiple files at a time. I use it for work, but also for making changes to my entire second brain in Obsidian with one command.</p>
<p>Today I needed to update my /articles/ links to /zet/articles/ links because I&rsquo;m restructuring my website. Here is the sed expression that is executed for every markdown file that is found by fd:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sed -i <span class="s1">&#39;s/\/articles\//\/zet\/articles\//g&#39;</span> <span class="k">$(</span>fd .md<span class="k">)</span>
</span></span></code></pre></div><p>The result:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">diff --git a/content/zet/move-to-zet.md b/content/zet/move-to-zet.md
</span></span><span class="line"><span class="cl">index 1e37283..3b817e3 <span class="m">100644</span>
</span></span><span class="line"><span class="cl">--- a/content/zet/move-to-zet.md
</span></span><span class="line"><span class="cl">+++ b/content/zet/move-to-zet.md
</span></span><span class="line"><span class="cl">@@ -6,7 +6,7 @@ tags:
</span></span><span class="line"><span class="cl"> - Zettelkasten
</span></span><span class="line"><span class="cl"> ---
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">-I<span class="s1">&#39;ve transitioned my note taking system towards a Zettelkasten system. I still use directories for folders and make copious links, but more
</span></span></span><span class="line"><span class="cl"><span class="s1">often than not I put them in the larger generic 00-zettelkasten directory in my [Obsidian](/articles/obsidian-introduction/) vault.
</span></span></span><span class="line"><span class="cl"><span class="s1">+I&#39;</span>ve transitioned my note taking system towards a Zettelkasten system. I still use directories <span class="k">for</span> folders and make copious links, but more
</span></span><span class="line"><span class="cl">often than not I put them in the larger generic 00-zettelkasten directory in my <span class="o">[</span>Obsidian<span class="o">](</span>/zet/articles/obsidian-introduction/<span class="o">)</span> vault.
</span></span></code></pre></div><p>These sites are super useful to help you formulate your expressions:</p>
<p><a href="https://sed.js.org/">https://sed.js.org/</a></p>
<p><a href="https://regex101.com/">https://regex101.com/</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Application Insights: Telemetry Sampling</title>
      <link>https://mischavandenburg.com/zet/application-insights-sampling/</link>
      <pubDate>Tue, 03 Jan 2023 08:41:09 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/application-insights-sampling/</guid>
      <description>Telemetry is the collection of measurements or other data at remote points, and transmitting that data to a receiver for monitoring.
Sampling is used to reduce telemetry traffic and costs for storage and data in Application Insights.
For small and medium sized applications sampling is generally not necessary.
Advantages of sampling:
Throttling data when the application suddenly sends a high volume of telemetry in a short time This saves costs! Keeping a pricing tier quota Reduce network traffic from telemetry collection Three different kinds of sampling:</description>
      <content:encoded><![CDATA[<p>Telemetry is the collection of measurements or other data at remote points, and transmitting that data to a receiver for monitoring.</p>
<p>Sampling is used to reduce telemetry traffic and costs for storage and data in Application Insights.</p>
<p>For small and medium sized applications sampling is generally not necessary.</p>
<p>Advantages of sampling:</p>
<ul>
<li>Throttling data when the application suddenly sends a high volume of telemetry in a short time
<ul>
<li>This saves costs!</li>
</ul>
</li>
<li>Keeping a pricing tier quota</li>
<li>Reduce network traffic from telemetry collection</li>
</ul>
<p>Three different kinds of sampling:</p>
<ul>
<li>adaptive sampling
<ul>
<li>automatically adjusts volume of telemetry</li>
<li>from ASP.NET or Azure Functions</li>
<li>only for these two</li>
</ul>
</li>
<li>fixed-rate sampling
<ul>
<li>rate is set by the administrator</li>
<li>use when you have a clear idea of the appropriate sampling percentage</li>
<li>reduces volume from
<ul>
<li>ASP.NET or ASP.NET Core server</li>
<li>Java server</li>
<li>Python applications</li>
<li>User browsers</li>
</ul>
</li>
</ul>
</li>
<li>ingestion sampling
<ul>
<li>used when monthly quota is often met</li>
<li>reduces amount of processed and retained traffic by Application Insights
<ul>
<li>less processing = less cost</li>
<li>doesn&rsquo;t reduce telemetry traffic sent from the app</li>
<li>happens at Applications Insight service endpoint</li>
</ul>
</li>
<li>disabled if SDK samples telemetry</li>
<li>can set sampling rate without redeploying the app</li>
<li>only applies when no other sampling is in effect
<ul>
<li>supports all Application Insights SDK&rsquo;s</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Pipelines: Continuous Monitoring</title>
      <link>https://mischavandenburg.com/zet/pipelines-continuous-monitoring/</link>
      <pubDate>Tue, 03 Jan 2023 08:30:03 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/pipelines-continuous-monitoring/</guid>
      <description>This term can be confusing. Initially I thought it meant monitoring of the pipelines themselves. However, in the context of Azure Release Pipelines, continuous monitoring refers to something else.
Continuous monitoring leverages metrics from other services such as Application Insights. You can set up release gates based on these metrics. For example, you can set up a release gate to roll back the deployment if an alert is being fired for high CPU usage in the application.</description>
      <content:encoded><![CDATA[<p>This term can be confusing. Initially I thought it meant monitoring of the pipelines themselves. However, in the context of Azure Release Pipelines, continuous monitoring refers to something else.</p>
<p>Continuous monitoring leverages metrics from other services such as Application Insights. You can set up release gates based on these metrics. For example, you can set up a release gate to roll back the deployment if an alert is being fired for high CPU usage in the application.</p>
<p>You can set up several of these checks. If all these checks pass, the pipeline can proceed.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Distributed Tracing</title>
      <link>https://mischavandenburg.com/zet/distributed-tracing/</link>
      <pubDate>Tue, 03 Jan 2023 07:58:44 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/distributed-tracing/</guid>
      <description>Debugging is done using call stacks in monolithic applications. Nowadays it is more common to deploy an application using a microservices architecture. Microservices make it easier to update certain parts of the application, and allow for more frequent deployments.
Using microservices does have a disadvantage: you cannot use the local call stack for debugging, because calls are sent to different microservices.
Distributed tracing is an implementation of the call stack in the cloud.</description>
      <content:encoded><![CDATA[<p>Debugging is done using <a href="/zet/call-stacks/">call stacks</a> in monolithic applications. Nowadays it is more common to deploy an application using a microservices architecture. Microservices make it easier to update certain parts of the application, and allow for more frequent deployments.</p>
<p>Using microservices does have a disadvantage: you cannot use the local call stack for debugging, because calls are sent to different microservices.</p>
<p>Distributed tracing is an implementation of the call stack in the cloud. It is usually implemented by adding an agent, <a href="/zet/sdk/">SDK</a>, or library to the service. In Azure you can enable distributed tracing via Application Insights through auto-instrumentation or SDKs.</p>
<p><a href="https://learn.microsoft.com/en-us/azure/azure-monitor/app/transaction-diagnostics">Unified cross-component transaction diagnostics</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>What is a SDK?</title>
      <link>https://mischavandenburg.com/zet/sdk/</link>
      <pubDate>Tue, 03 Jan 2023 07:50:55 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/sdk/</guid>
      <description>A software development kit (SDK) is a set of tools provided by the manufacturer of (usually) a hardware platform, operating system (OS), or programming language.
SDKs contain all the tools you need to get started. They typically contain a compiler, a debugger and an API. But they can also contain documentation and testing tools.</description>
      <content:encoded><![CDATA[<p>A software development kit (SDK) is a set of tools provided by the manufacturer of (usually) a hardware platform, operating system (OS), or programming language.</p>
<p>SDKs contain all the tools you need to get started. They typically contain a compiler, a debugger and an API. But they can also contain documentation and testing tools.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Copyright and License</title>
      <link>https://mischavandenburg.com/zet/articles/copyright-license/</link>
      <pubDate>Tue, 03 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/copyright-license/</guid>
      <description>All of the content on this blog is licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0).
You are free to: Share — copy and redistribute the material in any medium or format
Under the following terms: Attribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</description>
      <content:encoded><![CDATA[<p>All of the content on this blog is licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0).</p>
<h1 id="you-are-free-to">You are free to:</h1>
<p>Share — copy and redistribute the material in any medium or format</p>
<h1 id="under-the-following-terms">Under the following terms:</h1>
<p><strong>Attribution</strong> — You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</p>
<p><strong>NonCommercial</strong> — You may not use the material for commercial purposes.</p>
<p><strong>NoDerivatives</strong> — If you remix, transform, or build upon the material, you may not distribute the modified material.</p>
<p><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">https://creativecommons.org/licenses/by-nc-nd/4.0/</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>How This Blog is Created, Written and Hosted</title>
      <link>https://mischavandenburg.com/zet/articles/how-this-blog-is-created/</link>
      <pubDate>Tue, 03 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/how-this-blog-is-created/</guid>
      <description>As I alluded to in my article about Obsidian, I am very fond of editing my text in neovim. Naturally, if you want to edit in neovim, you need to have your text as local files. I keep all of my personal notes in markdown.
Previously I was using WordPress, but the editing and writing experience became torture which I could not endure any longer. I looked for a different solution that would allow me to edit my files locally instead of in the browser.</description>
      <content:encoded><![CDATA[<p>As I alluded to in my <a href="/zet/articles/obsidian-introduction">article about Obsidian,</a> I am very fond of editing my text in neovim. Naturally, if you want to edit in neovim, you need to have your text as local files. I keep all of my personal notes in markdown.</p>
<p>Previously I was using WordPress, but the editing and writing experience became torture which I could not endure any longer. I looked for a different solution that would allow me to edit my files locally instead of in the browser.</p>
<p>I discovered <a href="https://gohugo.io/">Hugo</a> and I fell in love with it immediately.</p>
<p>Hugo is a static site generator based on markdown files. My entire blog is written in markdown files which are stored <a href="https://github.com/mischavandenburg/blog">in a GitHub repo.</a>. I write my blog posts in vim and when I&rsquo;m done I use Hugo to generate the updated website.</p>
<p>The result is what you see in the &ldquo;public&rdquo; directory in the GitHub repo. This public directory is pushed to a different repo which is hooked up to my hosting provider. My hosting provider uses Plesk, and with Plesk I have the option to connect the GitHub repo to the web server with a webhook. When I push to my hosting repo, the contents are gathered by the webserver and served as public web content.</p>
<p>My complete writing and publishing workflow looks like this:</p>
<ol>
<li>Create a new markdown file</li>
<li>Write the note or article</li>
<li>Save the file and run the &ldquo;hugo&rdquo; command to regenerate the website</li>
<li>Run the &ldquo;publish&rdquo; script. This is a custom script I wrote that takes the contents of the &ldquo;public&rdquo; directory to my hosting repo</li>
<li>Push the newly generated website to the hosting repo</li>
<li>And we&rsquo;re live! 🚀 🎉</li>
</ol>
<p>It is such a smooth and convenient process. I can literally have a new note published to the interet within a few minutes, and it is all done from the command line using my favorite tools.</p>
<p><a href="https://github.com/mischavandenburg/blog">Blog GitHub repo</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Call Stacks</title>
      <link>https://mischavandenburg.com/zet/call-stacks/</link>
      <pubDate>Mon, 02 Jan 2023 21:11:26 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/call-stacks/</guid>
      <description>When you call a function, the system sets aside space in memory for the function to do its work. Those chunks are called &amp;ldquo;stack frames&amp;rdquo; or &amp;ldquo;function frames.&amp;rdquo;
These frames are arranged in a stack. The frame for the most recently called function is always at the top of the stack. When a new function is called, it becomes the active frame, and it is on top of the stack.</description>
      <content:encoded><![CDATA[<p>When you call a function, the system sets aside space in memory  for the function to do its work. Those chunks are called &ldquo;stack frames&rdquo; or &ldquo;function frames.&rdquo;</p>
<p>These frames are arranged in a stack. The frame for the most recently called function is always at the top of the stack. When a new function is called, it becomes the active frame, and it is on top of the stack.</p>
<p>The function that is actually doing something at the moment is on top of the stack and is known as the &ldquo;active frame.&rdquo;</p>
<p>When the function finishes its work, the frame is popped off of the stack. The frame in second place becomes the active frame. It had been paused in the meantime, and now it is active again, because it is on top.</p>
<p>Functions that are not on top, are not running.</p>
<p><a href="https://www.youtube.com/watch?v=aCPkszeKRa4">This video explains it well.</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Starting a Project</title>
      <link>https://mischavandenburg.com/zet/starting-a-project/</link>
      <pubDate>Mon, 02 Jan 2023 21:00:11 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/starting-a-project/</guid>
      <description>I&amp;rsquo;m starting a project with a friend. Developing an application. We make a good team, he&amp;rsquo;s great at coding and knows the backend too.
He&amp;rsquo;ll do the development, I&amp;rsquo;m in charge of hosting. We&amp;rsquo;re setting everything up in Azure DevOps, so it is a great way to practice my Azure skills and apply the things I&amp;rsquo;ve learned in my recently obtained AZ-104 Azure Administrator certification.
Even though it is a small scale hobby project, I still plan to approach it as if it was an enterprise production application.</description>
      <content:encoded><![CDATA[<p>I&rsquo;m starting a project with a friend. Developing an application. We make a good team, he&rsquo;s great at coding and knows the backend too.</p>
<p>He&rsquo;ll do the development, I&rsquo;m in charge of hosting. We&rsquo;re setting everything up in Azure DevOps, so it is a great way to practice my Azure skills and apply the things I&rsquo;ve learned in my recently obtained <a href="/zet/articles/az-104-study-guide/">AZ-104</a> Azure Administrator certification.</p>
<p>Even though it is a small scale hobby project, I still plan to approach it as if it was an enterprise production application. I&rsquo;ll set up a full CI/CD pipeline with testing in a secure manner. Credentials stored in an Azure key vault and images pushed to a private registry.</p>
<p>This is going to be fun!</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Going to Publish Smaller, and More Often</title>
      <link>https://mischavandenburg.com/zet/move-to-zet/</link>
      <pubDate>Mon, 02 Jan 2023 20:52:50 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/move-to-zet/</guid>
      <description>I&amp;rsquo;ve transitioned my note taking system towards a Zettelkasten system. I still use directories for folders and make copious links, but more often than not I put them in the larger generic 00-zettelkasten directory in my Obsidian vault.
The concept of &amp;ldquo;atomic notes&amp;rdquo; is also very important in Zettelkasten methods. Notes should be small and concise.
Up until this point I&amp;rsquo;ve been publishing full articles on my blog. I came across Rob Muhlestein yesterday and I was very inspired by his setup and public zettelkasten.</description>
      <content:encoded><![CDATA[<p>I&rsquo;ve transitioned my note taking system towards a Zettelkasten system. I still use directories for folders and make copious links, but more often than not I put them in the larger generic 00-zettelkasten directory in my <a href="/zet/articles/obsidian-introduction/">Obsidian</a> vault.</p>
<p>The concept of &ldquo;atomic notes&rdquo; is also very important in Zettelkasten methods. Notes should be small and concise.</p>
<p>Up until this point I&rsquo;ve been publishing full articles on my blog. I came across <a href="https://github.com/rwxrob">Rob Muhlestein</a> yesterday and I was very inspired by his setup and public zettelkasten. I think I&rsquo;ll move to a similar approach. Still planning to write and publish full articles as well, but also including atomic notes and personal status updates.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Obsidian: A Note Taking App For DevOps Engineers</title>
      <link>https://mischavandenburg.com/zet/articles/obsidian-introduction/</link>
      <pubDate>Sun, 01 Jan 2023 19:05:13 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/obsidian-introduction/</guid>
      <description>As a DevOps or Cloud engineer, you work with many different technologies daily. Therefore, a good engineer needs a solid foundation in Linux, a lot of knowledge about your cloud solution, networking, CI/CD, at least one programming language, and many other topics.
Not only are there many of these categories, but within these categories, there are several alternatives. For example, in the category of infrastructure management, there is Ansible, Puppet, or Chef, each with its particular approach and configuration methods.</description>
      <content:encoded><![CDATA[<p>As a DevOps or Cloud engineer, you work with many different technologies daily. Therefore, a good engineer needs a solid foundation in Linux, a lot of knowledge about your cloud solution, networking, CI/CD, at least one programming language, and many other topics.</p>
<p>Not only are there many of these categories, but within these categories, there are several alternatives. For example, in the category of infrastructure management, there is Ansible, Puppet, or Chef, each with its particular approach and configuration methods.</p>
<p>It can be challenging to keep everything memorized at all times. However, when I started my journey to become a DevOps engineer, I kept meticulous notes of everything I learned, and this practice has been highly beneficial so far. This blog is a result of the notes I keep every day.</p>
<p>Note-taking is an essential part of the DevOps engineer&rsquo;s toolkit. It allows you to stay organized, track changes, document processes, and keep track of important information. Using a powerful note-taking app like Obsidian can streamline your note-taking process and work more efficiently.</p>
<h1 id="obsidian-for-devops-engineers">Obsidian for DevOps engineers</h1>
<p>Having a reliable and efficient system in place for managing your notes and documentation is crucial. That&rsquo;s where <a href="https://obsidian.md/">Obsidian</a> comes in. Obsidian is a powerful note-taking app that can help you organize and manage your notes more efficiently.</p>
<p>One of the main features of Obsidian is its use of &ldquo;vaults.&rdquo; A vault is a folder containing your notes as markdown text files. Your notes are stored on your machine rather than in the cloud. This gives you complete control over your data. You always have access to your local text files and can interact with them or back them up as you see fit.</p>
<p>Because your notes are stored as markdown files, you can use different tools to write or edit your notes. I mostly use neovim for editing, but I use the Obsidian application for making new links and visualization. Moreover, storing your notes as files allows you to run python scripts on your notes and customize your workflow as needed. This can be especially useful for automating tasks, streamlining your work, or making bulk updates.</p>
<h1 id="links-and-graph-view">Links and graph view</h1>
<p>In addition to its local storage capabilities, Obsidian also offers several other valuable features for DevOps engineers. For example, you can use the &ldquo;graph view&rdquo; to visualize your notes and see how they&rsquo;re related to one another. This can be particularly useful for understanding complex systems and tracking changes over time.</p>
<p><img loading="lazy" src="/graph-view.png" type="" alt=""  /></p>
<p>These relations between notes are created by &ldquo;[[markdown links]]&rdquo;. When a note receives many links, its dot size will increase on the graph view, and in this manner, it is easy to see which notes or topics are significant in your vault and play an important role in your life.</p>
<p><img loading="lazy" src="/local-graph.png" type="" alt=""  /></p>
<p>This is the local graph view, showing all the linked notes to my Linux note.</p>
<h1 id="conclusion">Conclusion</h1>
<p>Many note-taking apps, such as Evernote, Notion, or Roam research, are available. I tried a few of these, but Obsidian was the best solution for me. Mainly because your notes are stored as markdown files on your machine and because they offer a syncing service with end-to-end encryption. Their graph view provides an interesting way to navigate your notes and discover unexpected connections.</p>
<p>Give it a try. It&rsquo;s free.</p>
<p><a href="https://obsidian.md/">Obsidian website</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>What Are Containers?</title>
      <link>https://mischavandenburg.com/zet/articles/what-are-containers/</link>
      <pubDate>Sun, 01 Jan 2023 16:17:58 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/what-are-containers/</guid>
      <description>When you learn about DevOps, you will come across the concept of a container early on. This is a &amp;ldquo;Mischa Explains&amp;rdquo; article where I attempt to explain a concept in my own words as simply as possible. I use the Feynman technique and pretend to explain it to a 12-year-old.
Virtualization To understand containers, we need to understand virtualization. Virtualization is the process of creating &amp;ldquo;fake computers&amp;rdquo; or &amp;ldquo;virtual computers&amp;rdquo; on a physical computer.</description>
      <content:encoded><![CDATA[<p>When you learn about DevOps, you will come across the concept of a container early on. This is a &ldquo;Mischa Explains&rdquo; article where I attempt to explain a concept in my own words as simply as possible. I use the Feynman technique and pretend to explain it to a 12-year-old.</p>
<h1 id="virtualization">Virtualization</h1>
<p>To understand containers, we need to understand virtualization. Virtualization is the process of creating &ldquo;fake computers&rdquo; or &ldquo;virtual computers&rdquo; on a physical computer.</p>
<p>On your desk, you have a laptop or a desktop PC. This machine has hardware such as a motherboard, a hard disk, and a video card. To run programs on your computer, it needs an Operating System. Usually, Windows, macOS, or Linux are used.</p>
<p>Let&rsquo;s say you have a computer running Windows, but you want to run a program that can only run on Linux. One solution is to buy another laptop and put it beside your Windows laptop on your desk. So now you have two computers with two different operating systems.</p>
<p>Fortunately, there are other solutions. We can use virtualization to make a Virtual Machine. A virtual machine is created by software to imitate a fully functional running computer inside your current operating system. You can create a virtual machine that runs Linux on your Windows computer. Your Windows computer running the Linux virtual machine is known as the **host.</p>
<p>Now you don&rsquo;t need to buy another computer to run your Linux program. Instead, you can boot up your Linux virtual machine and run your program when needed. If you have a powerful computer, you could run ten or more virtual machines, each of which has its own operating system and custom environment.</p>
<h1 id="containers">Containers</h1>
<p>Every time you create a virtual machine, the virtual machine needs a complete operating system to work. So, first, the software creates a virtual processor, virtual video card, and a virtual network interface. Then, it runs a fully functional operating system on that virtual hardware. This takes up a lot of resources.</p>
<p>Containers are lightweight packages of software. They are designed to do a very specific task, and therefore they only contain the resources they need to do that task. Nothing more.</p>
<p>Containers use the operating system of the physical computer to run. They have a very minimal, lightweight operating system inside them, but it only contains the elements they need to do their specific task. Therefore, containers are very easy to distribute, and you can run them very quickly.</p>
<h1 id="containers-are-like-newspapers">Containers are like newspapers</h1>
<p>Containers are like newspapers. Newspapers have a particular task: providing you with the day&rsquo;s news. You cannot use newspapers to study for your mathematics exam. You use your math book to study for your math exam. If you want to be informed of the day&rsquo;s news, you use a newspaper. This is what I mean by containers having a specific task.</p>
<p>Next, newspapers are printed on a specific kind of paper. When you buy an expensive book, it will have a sturdy and durable cover, and the pages are made of nice thick paper that will last a long time. The pages don&rsquo;t tear very quickly, and when the book gets wet, it can withstand it. This thick cover and high-quality papers are like the operating system of a virtual machine.</p>
<p>Newspapers, on the other hand, are printed on very thin paper. Because they are designed to distribute the news to you effectively, newspapers do not need to be stored forever or do any other tasks. If you used thick, expensive paper for newspapers, they would become costly, and no one would buy them anymore. The paper is optimized to bring the news to you.</p>
<p>In the same way, the container only comes with the components it needs to do its specific task. Therefore, the container is optimized for its purpose. As a result, they can be distributed more quickly and do not take up a lot of resources when running.</p>
<p>There are other benefits to containers, such as improving the ability to autoscale your application, but I will expand on those in a future blog post.</p>
<h1 id="further-study">Further study</h1>
<p>To learn more about containers, you can use the following resources:</p>
<p><a href="https://youtu.be/r6YIlPEC4y4">Containers &amp; Friends from John Savill&rsquo;s DevOps Masterclass</a></p>
<p><a href="https://docs.docker.com/get-started/overview/">Docker Documentation</a></p>
<p><a href="https://youtu.be/3c-iBn73dDE">Docker Tutorial for Beginners</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Getting Started with Neovim: kickstart.nvim</title>
      <link>https://mischavandenburg.com/zet/articles/kickstart-nvim/</link>
      <pubDate>Sun, 01 Jan 2023 09:11:57 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/kickstart-nvim/</guid>
      <description>I&amp;rsquo;ve been using neovim for over a year now, and I&amp;rsquo;m very happy that I went through the initial difficulty of learning vim. One of the best perks of using neovim is that you can customize your entire editing experience and workflow. However, it can be a daunting experience to start with an empty configuration and set up everything from scratch.
I started with an empty vanilla vim config and slowly added the plugins as I went along.</description>
      <content:encoded><![CDATA[<p>I&rsquo;ve been using neovim for over a year now, and I&rsquo;m very happy that I went through the initial difficulty of learning vim. One of the best perks of using neovim is that you can customize your entire editing experience and workflow. However, it can be a daunting experience to start with an empty configuration and set up everything from scratch.</p>
<p>I started with an empty vanilla vim config and slowly added the plugins as I went along. Videos by content creators such as ThePrimagen were also helpful in getting inspiration on which plugins I might like for my setup. But this might not be suitable for everyone. I was only editing yaml files and writing simple Python scripts at the time, whereas you might be looking for an IDE experience out of the box.</p>
<p>Recently I discovered <a href="https://github.com/nvim-lua/kickstart.nvim">kickstart.nvim</a> by neovim core maintainer TJdeVries.</p>
<p>I decided to give it a try, and I was pleasantly surprised. It is a great starting setup for a beginner. It is simple and does not overwhelm you with thousands of features.</p>
<p>I&rsquo;ve completely rewritten my config based on kickstart.nvim, and I am delighted with the result. Especially the LSP setup is very well thought out, and it works much better than the setup I came up with on my own.</p>
<p>TJ DeVries also made a <a href="https://youtu.be/stqUbv-5u2s">video</a> introducing kickstart.nvim and going through the initial setup.</p>
<p>I highly recommend kickstart.nvim if you are interested in using neovim and are looking for a sane place to start.</p>
<h1 id="links">links</h1>
<p><a href="https://github.com/nvim-lua/kickstart.nvim">kickstart.nvim</a></p>
<p><a href="https://youtu.be/stqUbv-5u2s">Kickstart video by TJdeVries</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Study Guide: AZ-104 Azure Administrator Associate</title>
      <link>https://mischavandenburg.com/zet/articles/az-104-study-guide/</link>
      <pubDate>Fri, 30 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/az-104-study-guide/</guid>
      <description>TLDR It took me 80 hours of studying to gain this certification. Here are my notes and Anki deck: GitHub repo
Introduction When I did my English degree at University, exams were usually handwritten essays that needed to be done within a 3-hour timeframe. Sometimes we had multiple-choice tests, and I would always consider them a guaranteed pass because multiple-choice was much easier in my experience.
That opinion has changed since I&amp;rsquo;ve obtained a few IT certifications.</description>
      <content:encoded><![CDATA[<h1 id="tldr">TLDR</h1>
<p>It took me 80 hours of studying to gain this certification. Here are my notes and Anki deck:
<a href="https://github.com/mischavandenburg/az-104-azure-administrator">GitHub repo</a></p>
<h1 id="introduction">Introduction</h1>
<p>When I did my English degree at University, exams were usually handwritten essays that needed to be done within a 3-hour timeframe. Sometimes we had multiple-choice tests, and I would always consider them a guaranteed pass because multiple-choice was much easier in my experience.</p>
<p>That opinion has changed since I&rsquo;ve obtained a few IT certifications. These tests are hard! I&rsquo;m typing this while sitting on the bus on my way home from my AZ-104 exam. I passed with an 860 score of 1000, where 700 or higher is a pass. But it was an astonishingly tough exam. Usually, I finish quickly and spend at least half an hour reviewing my answers. I had only 5 minutes to review my questions this time because I had used up all of the available time. The questions required intense concentration and were time-consuming because I needed to compare many options which were very similar to each other. There were no easy questions.</p>
<h1 id="preparation">Preparation</h1>
<p>I studied 80 hours for this exam in a month. I work full-time as a DevOps Engineer, so I study in the evenings and on weekends. I have my Azure Fundamentals and CKA, but I only work with Azure occasionally in my current role.</p>
<p>Here is what I did to prepare for my exam:</p>
<ul>
<li>Go through all of the <a href="https://learn.microsoft.com/en-us/certifications/exams/az-104">Microsoft Learn modules for the AZ-104</a></li>
<li>Watch the entire <a href="https://www.youtube.com/playlist?list=PLlVtbbG169nGlGPWs9xaLKT1KfwqREHbs">AZ-104 study list by John Savill</a></li>
<li>Practice exams on <a href="https://tutorialsdojo.com/">TutorialsDojo</a> until I could pass them with 90%+ scores</li>
<li>Microsoft ESI practice exams</li>
<li><a href="https://learn.microsoft.com/en-us/shows/exam-readiness-zone/preparing-for-az-104-manage-azure-identities-and-governance-1-of-5">Microsoft AZ-104 Exam prep videos</a></li>
</ul>
<h2 id="microsoft-learn">Microsoft Learn</h2>
<p>You really need to master all of the subject matter. Only completing the Microsoft Learn modules is not enough preparation. They are more like summaries. At the end of each module, they provide links to the documentation for the subject for further study. Unfortunately, Microsoft does not go easy on you. It expects you to know obscure details of nearly every service this exam covers. Therefore, I advise going beyond the Microsoft Learn modules and studying the linked articles after each module.</p>
<h2 id="youtube-az-104-study-playlist-by-john-savill">YouTube AZ-104 Study Playlist by John Savill</h2>
<p>I&rsquo;m not sure if it&rsquo;s better to watch this playlist first and then do the Microsoft modules or the other way around. I did the Microsoft modules first, but for my next exam (AZ-400 DevOps Expert), I&rsquo;ll start with the videos and then do the Microsoft Learn modules.</p>
<h2 id="tutorialsdojo">Tutorialsdojo</h2>
<p>These practice exams are excellent. I used them in preparation for my fundamentals exam.</p>
<p>The best thing about them is that they provide extensive documentation and explanation of the questions. So after you finish the exam, you can study a lot with these examples.</p>
<h2 id="esi-practice-exams">ESI Practice Exams</h2>
<p>You&rsquo;re lucky if your organization participates in Microsoft&rsquo;s <a href="esi.microsoft.com/">Enterprise Skills Initiative</a>. The practice exams provided in the ESI environment give you a good indication of what you can expect at the exam. I first did the Tutorialsdojo exams and then moved on to the ESI exams, and I was humiliated. The ESI questions are very complex and hard to solve, and I learned a lot from these exams.</p>
<p>There are 210 questions total, and I worked through all of them, and whenever I failed a question, I did a deeper dive into the question&rsquo;s theme.</p>
<h1 id="studying">studying</h1>
<p>I take notes in Obsidian, and I use Anki for spaced repetition. I highly recommend keeping a deck of Anki cards and continuously testing yourself. You will need to memorize a lot of details. For example, you are expected to remember that storage accounts of the FileStorage type do not support Geo Redundant Storage. You can find my Anki deck in the <a href="https://github.com/mischavandenburg/az-104-azure-administrator">GitHub repo.</a></p>
<h1 id="links">links</h1>
<p><a href="https://learn.microsoft.com/en-us/certifications/exams/az-104">AZ-104 Exam page with learning modules</a></p>
<p><a href="https://www.youtube.com/playlist?list=PLlVtbbG169nGlGPWs9xaLKT1KfwqREHbs">John Savill&rsquo;s AZ-104 Study playlist</a></p>
<p><a href="https://github.com/mischavandenburg/az-104-azure-administrator">GitHub repo containing my notes and Anki deck</a></p>
<p><a href="https://learn.microsoft.com/en-us/shows/exam-readiness-zone/preparing-for-az-104-manage-azure-identities-and-governance-1-of-5">Microsoft AZ-104 Exam prep videos</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Lab Project: GitOps with ArgoCD, Azure Pipelines and Minikube</title>
      <link>https://mischavandenburg.com/zet/articles/lab-argocd-azure-pipelines/</link>
      <pubDate>Sat, 24 Dec 2022 13:49:55 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/lab-argocd-azure-pipelines/</guid>
      <description>This weekend I had a lot of fun with a project. I wanted to learn more about GitOps and try out ArgoCD.
My goal was to be able to deploy an application from a GitHub repo to my local Kubernetes cluster running in minikube. There are many options I could have used, such as running Jenkins in my cluster. But I wanted to use Azure pipelines for practice, which complicates the deployment to my local cluster, because the cluster is not running on Azure.</description>
      <content:encoded><![CDATA[<p>This weekend I had a lot of fun with a project. I wanted to learn more about GitOps and try out ArgoCD.</p>
<p>My goal was to be able to deploy an application from a GitHub repo to my local Kubernetes cluster running in minikube. There are many options I could have used, such as running Jenkins in my cluster. But I wanted to use Azure pipelines for practice, which complicates the deployment to my local cluster, because the cluster is not running on Azure. I also wanted to try out ArgoCD and learn more about GitOps.</p>
<p>The application is a simple web app that I wrote which displays a quote in the browser:</p>
<p><img loading="lazy" src="/app.png" type="" alt=""  /></p>
<h1 id="gitops-and-structure">GitOps and Structure</h1>
<p>GitOps is used to automate the process of provisioning infrastructure. Infrastructure as code is used to generate the same environment every time the environment is deployed.</p>
<p>For my project I have two separate GitHub repos. <a href="https://github.com/mischavandenburg/static-quote-app">The first repo</a> contains the code for a simple web app I created and the Dockerfile to generate the image. I call this my application repo. The other repo is <a href="https://github.com/mischavandenburg/static-quote-app-gitops">my GitOps repo</a> which contains the manifest files to deploy the application in Kubernetes. I decided to leverage Helm to create my manifest files. This way I can create templates and define my desired values in a values.yaml file in the repo.</p>
<p>Ultimately my goal was to use an Azure pipeline to build an image from my application repo and push it to Docker hub. This new image is given a new tag which needs to be stored. The first pipeline should trigger a new pipeline that makes a pull request to the GitOps repo to update the tag in my Helm chart.</p>
<p>ArgoCD will then scan the GitOps repo and realize that the tag has been updated, and deploy the new tag to my cluster.</p>
<h1 id="minikube">Minikube</h1>
<p>I used <a href="https://minikube.sigs.k8s.io/docs/">minikube</a> to deploy my local Kubernetes cluster. Another option is <a href="https://kind.sigs.k8s.io/">kind</a> (Kubernetes In Docker) but I wanted to use a VM approach this time.</p>
<h1 id="argocd">ArgoCD</h1>
<p><a href="https://argo-cd.readthedocs.io/">ArgoCD</a> is a declarative GitOps continuous delivery tool for Kubernetes. This is the solution I used to continuously scan my GitOps repo. When ArgoCD detects a change in the desired state, it will compare it with the state in my running cluster and make changes accordingly. I found a really good <a href="https://redhat-scholars.github.io/argocd-tutorial/argocd-tutorial/02-getting_started.html">tutorial to run ArgoCD in minikube</a>.</p>
<p><img loading="lazy" src="/argocd-dashboard.png" type="" alt="ArgoCD dashboard"  /></p>
<h1 id="azure-pipelines">Azure Pipelines</h1>
<p>With my cluster running on my local machine and my repos set up, I needed to use Azure Pipelines to bring it all together. Building the image and pushing it to Docker Hub wasn&rsquo;t a big deal. But I had two big challenges in my desired setup: I needed to pass the new tag number to a new pipeline, and I needed to use Azure Pipelines to create a new PR to my GitOps repo.</p>
<h3 id="passing-a-value-from-one-pipeline-to-another">Passing a value from one pipeline to another</h3>
<p>Interestingly, this wasn&rsquo;t as easy as it sounds, and from my internet searching it seemed that many people struggled with this. I decided to use the Variable Groups in Azure DevOps. However, after I finished writing my pipeline, I discovered I had no problems with reading the value from the Variable Groups, but it was impossible to update it using existing pipeline tasks. So I had to a bit of hacking to make it work. In the end I had to use the Azure CLI from within the pipeline to update my variable:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">stage</span><span class="p">:</span><span class="w"> </span><span class="l">update_tag</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">jobs</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">job</span><span class="p">:</span><span class="w"> </span><span class="l">update_tag_variable </span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">displayName</span><span class="p">:</span><span class="w"> </span><span class="l">Update Tag Variable</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">steps</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">bash</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">          az pipelines variable-group variable \
</span></span></span><span class="line"><span class="cl"><span class="sd">          update --group-id 202 \
</span></span></span><span class="line"><span class="cl"><span class="sd">          --org $(System.CollectionUri) \
</span></span></span><span class="line"><span class="cl"><span class="sd">          --project $(System.TeamProject) \
</span></span></span><span class="line"><span class="cl"><span class="sd">          --name tag --value $(Build.BuildId)</span><span class="w">          
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">env</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">AZURE_DEVOPS_EXT_PAT</span><span class="p">:</span><span class="w"> </span><span class="l">$(System.AccessToken)</span><span class="w">
</span></span></span></code></pre></div><p>This didn&rsquo;t feel like a very elegant solution, but it was the only solution I could come up with.</p>
<p>I also struggled a lot with permissions. I needed to find the correct service principal to assign the administrator rights to. <a href="https://stackoverflow.com/questions/52986076/having-no-permission-for-updating-variable-group-via-azure-devops-rest-api-from">This post</a> really helped to solve my problem.</p>
<h3 id="submitting-a-pr-to-a-github-repo">Submitting a PR to a GitHub repo</h3>
<p>When I started writing my pipeline I thought it would be very straightforward to just submit a PR to a repo, but I quickly discovered that this is not natively supported in Azure pipelines yet. In fact, I could not find a way to submit a PR at all. I had to settle for a solution that checks out the GitOps repo and creates a new branch. This new branch updates the tag in the values.yaml with the new tag that was passed from the previous pipeline.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">variables</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="l">mischa-quote</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">passed_tag</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l">$[variables.tag]</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">branch_name</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;pipeline-$(passed_tag)&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">pool</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">vmImage</span><span class="p">:</span><span class="w"> </span><span class="l">ubuntu-latest</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">steps</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">checkout</span><span class="p">:</span><span class="w"> </span><span class="l">self</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">persistCredentials</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">clean</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">script</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">      git config --global user.email &#34;mischa@pipeline.com&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">      git config --global user.name &#34;Mischa Pipeline&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">      git switch -c &#34;$(branch_name)&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">      sed -i &#34;s/tag:.*/tag: $(passed_tag)/&#34; values.yaml 
</span></span></span><span class="line"><span class="cl"><span class="sd">      git add .
</span></span></span><span class="line"><span class="cl"><span class="sd">      git commit -m &#34;Update tag to $(passed_tag)&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">      git push --set-upstream origin &#34;$(branch_name)&#34;</span><span class="w">      
</span></span></span></code></pre></div><p>This also felt a bit hacky to do with explicit shell commands, but it was the only way I could find to achieve my goal. I used sed to update the tag.</p>
<h1 id="result">Result</h1>
<p>The resulting deployment pipeline is as follows.</p>
<ol>
<li>I make a commit to my application repo, which triggers a build pipeline in Azure DevOps:</li>
</ol>
<p><img loading="lazy" src="/trigger-pipeline1.png" type="" alt=""  /></p>
<ol start="2">
<li>This resulted in an image pushed to my Docker Hub:</li>
</ol>
<p><img loading="lazy" src="/docker-hub.png" type="" alt=""  /></p>
<ol start="3">
<li>The pipeline created a new branch in my GitOps repo. Unfortunately, I have to make the PR myself, but as you can see, the pipeline successfully updates the values.yaml with the new tag which we also saw in Docker Hub:</li>
</ol>
<p><img loading="lazy" src="/new-branch.png" type="" alt=""  /></p>
<p><img loading="lazy" src="/update-tag.png" type="" alt=""  /></p>
<ol start="4">
<li>When I merged the pull request, ArgoCD detected the change and deployed a new pod with the new tag.</li>
</ol>
<p><img loading="lazy" src="/argocd-sync.png" type="" alt=""  /></p>
<ol start="5">
<li>Running a <code>kubectl describe</code> on the pod also verifies that we have the correct image:</li>
</ol>
<p><img loading="lazy" src="/kubectl-tag.png" type="" alt=""  /></p>
<h1 id="conclusion">Conclusion</h1>
<p>This was a fun challenge, but I learned a lot from solving the problems I encountered and my entire Saturday flew by in an uninterrupted flow state. I had some good practice in setting up Azure pipelines, learned about Helm, and did my first implementation of GitOps. Not bad for a day&rsquo;s work!</p>
<h1 id="links">Links</h1>
<p><a href="https://github.com/mischavandenburg/static-quote-app">Application GitHub repo</a></p>
<p><a href="https://github.com/mischavandenburg/static-quote-app-gitops">GitOps repo</a></p>
<p><a href="https://minikube.sigs.k8s.io/docs/">minikube</a></p>
<p><a href="https://kind.sigs.k8s.io/">kind</a></p>
<p><a href="https://argo-cd.readthedocs.io/">ArgoCD</a></p>
<p><a href="https://redhat-scholars.github.io/argocd-tutorial/argocd-tutorial/02-getting_started.html">tutorial to run ArgoCD in minikube</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Tokens and Identity on the Internet</title>
      <link>https://mischavandenburg.com/zet/articles/identity/</link>
      <pubDate>Sun, 18 Dec 2022 00:55:47 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/identity/</guid>
      <description>Introduction Have you ever thought about your identity on the internet? How does LinkedIn know it is you when you log in to LinkedIn? And when you allow LinkedIn to post to your Twitter, how does LinkedIn access your account and not your kindergarten teacher&amp;rsquo;s account?
This is a &amp;ldquo;Mischa Explains&amp;rdquo; article where I attempt to explain a concept in my own words as simply as possible. I use the Feynman technique and pretend to explain it to a 12-year-old.</description>
      <content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>
<p>Have you ever thought about your identity on the internet? How does LinkedIn know it is you when you log in to LinkedIn? And when you allow LinkedIn to post to your Twitter, how does LinkedIn access your account and not your kindergarten teacher&rsquo;s account?</p>
<p>This is a &ldquo;Mischa Explains&rdquo; article where I attempt to explain a concept in my own words as simply as possible. I use the Feynman technique and pretend to explain it to a 12-year-old.</p>
<h1 id="identity">Identity</h1>
<p>The first step in this process is identity. You need a starting point; for many of us, this can be our Google account. You signed up for this account and probably verified this with your phone number.</p>
<p>This relates to <strong>authentication</strong>. Authentication is the process of verifying identity. You&rsquo;ll need to provide the correct password when you log in to your Google account. You must give a valid password to log in to your account and access the resources. Google uses your password to <strong>authenticate</strong> that it is you.</p>
<h1 id="authorization">Authorization</h1>
<p>Then we have <strong>authorization</strong>. Authorization means granting access to particular resources. For example, let&rsquo;s say you are working in the science classroom at school. In the classroom is a bookcase that everybody can use: it is not dangerous, and every student can take the books they need without asking for permission. In the back of the science classroom is a cabinet that contains chemicals. It would be very dangerous if everybody could go into the cabinet and take out the sulphuric acid. Not everybody might know how dangerous sulphuric acid is. That&rsquo;s why the cabinet is locked.</p>
<p>If you need something from the chemicals cabinet, you need to ask permission from the teacher. You need to be <strong>authorized</strong> by the teacher to take out the sulphuric acid. When you make your request, the teacher may ask you questions to ensure you know what you are doing. He might even ask you for your school ID card because he has not seen you before. The teacher <strong>authenticates</strong> you by asking for your school ID, and then he <strong>authorizes</strong> you to take out the sulphuric acid.</p>
<h1 id="tokens">Tokens</h1>
<p>How do we accomplish this on the internet?</p>
<p>To verify identities on the internet, we have identity providers. Google is an identity provider. Azure AD is also an identity provider. An open-source identity provider is Keycloak.</p>
<p>Identity providers use <em>tokens</em> to verify identity and authorize access to resources. There are two types of tokens: ID tokens and access tokens. And for each token, there is an associated protocol.</p>
<h2 id="id-tokens">ID tokens</h2>
<p>OpenID Connect, also known as OIDC, is an open standard for authentication. Identity providers have agreed with each other that they will use this standard. When you go through an OpenID workflow, the result is an ID token, proving that the user has been authenticated.</p>
<p>Your school ID card is the ID token in our science class example. When you started at your school, you went through a registration process. Your parents probably handled this. Your name was written down, and the school verified that it was you by looking at your passport and talking to your parents. The result of this process was your school ID card, which you use to borrow books from the library. The school ID card proves that you are a student of that school and that you can use the facilities at the school.</p>
<h2 id="access-tokens">Access tokens</h2>
<p>These are specifically designed to allow access to a resource. For example, this resource could be a file on a server or a database.</p>
<p>Access tokens are strictly for authorization and use the OAuth 2.0 standard.</p>
<p>In our science class example, the token would be the key to the chemicals cabinet. The teacher authorizes you to access the cabinet and gives you the key to the cabinet.</p>
<h2 id="putting-it-together">Putting it Together</h2>
<p>Now let&rsquo;s put it together with an example.</p>
<p>You just created a new Facebook account and want to add all your friends. However, you have a Google account, and Facebook can use the contacts in your Google account to automatically add all of your friends.</p>
<p>Your Google account can only be accessed by you, and your contacts are locked away behind a password. But it is possible to grant Facebook access to this.</p>
<p>On Facebook, you select the &ldquo;import contacts from Google&rdquo; function. Facebook sends you to Google, and Google will ask you to log in. Google is the teacher in our science class example. Google needs you to <strong>authenticate</strong> to prove that it is you. When this is done, Google generates an ID token using OIDC for Facebook: Google gives Facebook a school ID that it can use.</p>
<p>Next, Facebook needs access to the contacts in your Google account. In our example, Facebook asks to take the sulphuric acid from the chemicals cabinet. You will see a menu that specifies what Facebook wants to do, and you need to give your permission. When you give your permission, Google generates an OAuth 2.0 token for Facebook. In other words, Google gives the key to the chemicals cabinet to Facebook, and Facebook is now authorized to take the sulphuric acid.</p>
<p>When both of these tokens are generated, Facebook contacts Google and asks if it can take the sulphuric acid from the chemicals cabinet.</p>
<p>Google, the teacher, asks Facebook for the school ID, and Facebook shows the ID card it received earlier. When Google is satisfied with the ID and successfully authenticates Facebook, it gives Facebook the key to the chemicals cabinet. Facebook is now authorized to take out the sulphuric acid. Facebook is now authorized to access the contacts in your Google account.</p>
<h1 id="links">Links</h1>
<p>You can use these resources to learn more about this topic:</p>
<p><a href="https://www.youtube.com/watch?v=t18YB3xDfXI">An Illustrated Guide to OAuth and OpenID connect</a></p>
<p><a href="https://www.youtube.com/watch?v=M4JIvUIE17c">ID Tokens vs Access Tokens - Do you know the difference?</a></p>
<p><a href="https://learn.microsoft.com/en-us/azure/active-directory/develop/id-tokens">Microsoft Learn: ID Tokens</a></p>
<p><a href="https://learn.microsoft.com/en-us/azure/active-directory/develop/security-tokens">Microsoft Learn: Security Tokens</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Podcast Tip: The system that runs Norway&#39;s welfare payments</title>
      <link>https://mischavandenburg.com/zet/articles/nav-podcast/</link>
      <pubDate>Fri, 02 Dec 2022 20:45:46 +0100</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/nav-podcast/</guid>
      <description>An interesting podcast episode describing the system that runs Norway&amp;rsquo;s welfare payments. It was interesting to hear that they focus on open source and that everything runs one one big kubernetes cluster.
It was also very inspiring to hear that they went from 4 nightly deployments a year to 1700 deployments a week.
Link to the podcast episode:
The system that runs Norway&amp;rsquo;s welfare payments</description>
      <content:encoded><![CDATA[<p>An interesting podcast episode describing the system that runs Norway&rsquo;s welfare payments. It was interesting to hear that they focus on open source and that everything runs one one big kubernetes cluster.</p>
<p>It was also very inspiring to hear that they went from 4 nightly deployments a year to 1700 deployments a week.</p>
<p>Link to the podcast episode:</p>
<p><a href="https://changelog.com/shipit/78">The system that runs Norway&rsquo;s welfare payments</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Book Notes: Ikigai: The Japanese Secret to a Long and Happy Life</title>
      <link>https://mischavandenburg.com/zet/articles/ikigai-book/</link>
      <pubDate>Tue, 18 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/ikigai-book/</guid>
      <description>I came across this little book while doing some research for my Jiro Dreams of Sushi article. While scanning the book&amp;rsquo;s description, I was intrigued by this sentence: &amp;ldquo;the happiness of always being busy.&amp;rdquo; Although I work hard every day, I also enjoy ticking off the last item on my to-do list and enjoying some rest. Is there more happiness in being in constant activity?
Ikigai &amp;ldquo;He who has a why to live for can bear with almost any how.</description>
      <content:encoded><![CDATA[<p>I came across this little book while doing some research for my <a href="/zet/articles/jiro-sushi">Jiro Dreams of Sushi article.</a> While scanning the book&rsquo;s description, I was intrigued by this sentence: &ldquo;the happiness of always being busy.&rdquo; Although I work hard every day, I also enjoy ticking off the last item on my to-do list and enjoying some rest. Is there more happiness in being in constant activity?</p>
<h2 id="ikigai">Ikigai</h2>
<blockquote>
<hr>
<p>&ldquo;He who has a why to live for can bear with almost any how.&rdquo;</p>
<ul>
<li>Friedrich Nietzsche</li>
</ul>
<hr>
</blockquote>
<p>According to Japanese culture, everybody has a purpose in life: the ikigai. &ldquo;Ikigai is the reason we get up in the morning.&rdquo; The book explores this concept through interesting stories and brings related notions from various areas to explain ikigai to the Western mind.</p>
<p>There are many different ikigai, and people can have several ikigai simultaneously. For example, some people have their vegetable garden as their ikigai, while others have drawing or calligraphy. <a href="/zet/articles/jiro-sushi">Jiro</a> has making sushi as his ikigai. Even though many people have their work as their ikigai, it does not necessarily have to be the case. Ikigai can also be of a social nature, such as a family or local community.</p>
<p>
  <img loading="lazy" src="https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2F3nw94z2pgadc432nw33p8qg5-wpengine.netdna-ssl.com%2Fwp-content%2Fuploads%2F2018%2F01%2Fwhat-is-ikigai-darling.png&amp;f=1&amp;nofb=1&amp;ipt=f5fa9a17a2865d283af2bfdf8d978f1dcb0e9ff4e7fb646fb2c26c4963f95e77&amp;ipo=images" alt=""  /></p>
<h2 id="retirement">Retirement</h2>
<p>When I watched Jiro Dreams of Sushi for the first time, I was surprised that Jiro still worked at his restaurant every day at age 85. This book explores that further.</p>
<p>Apparently, many people never really retire in Japan. The concept of retirement, as we know it in the West, does not exist in the Japanese language: there is &ldquo;no word in Japanese that means retire in the sense of &ldquo;leaving the workforce for good&rdquo; as in English.&rdquo; People continue doing what they like doing as long as their health allows. They keep following their ikigai.</p>
<p>This concept was very refreshing to me and had a profound influence on how I imagine my own future. After becoming more intentional about my career, I became interested in <a href="https://www.investopedia.com/terms/f/financial-independence-retire-early-fire.asp">FIRE</a> and the possibility of retiring early. However, as I contemplated this strategy, I discovered that I didn&rsquo;t want to stop working because I really enjoy my work.</p>
<p>Why would I want to retire if I&rsquo;m doing what I love? That was the whole point of switching my career to IT, and this is where Western culture can learn a lot from Japanese culture. In Japanese culture, there is more emphasis on aligning your career with your interests and continuing that passion well into old age.</p>
<blockquote>
<hr>
<p>When to quit? The job you have worked so hard for? I never once hated my job. I fell in love with my work and gave my life to it. Even though I&rsquo;m 85 years old, I don&rsquo;t feel like retiring.</p>
<ul>
<li>Jiro</li>
</ul>
<hr>
</blockquote>
<h2 id="longevity">Longevity</h2>
<p>A large part of the book is about the residents of Okinawa: the island with the highest number of people 100 years old or older. The authors tell the story of their visit to the island to reveal the secret of the Okinawan&rsquo;s long lives. It is not uncommon to see people working in the fields who are well into their eighties. The Okinawans freely share their secrets with us, and the authors do a great job translating the Japanese principles into actionable advice.</p>
<p>Much of the advice is centered around dietary habits. Not only what to eat, but also how to eat it: &ldquo;Okinawans stop eating when they feel their stomachs reach 80 percent of their capacity, rather than overeating&rdquo;, a practice which is named &ldquo;hara haci bu.&rdquo; They consume a daily average of 1800 to 1900 calories, a significant difference from the 2200 to 3300 calories consumed by an average person in the US every day.</p>
<p>Another theme that keeps returning is the crucial function of social groups for finding meaning in life, especially in old age: &ldquo;It is customary in Okinawa to form close bonds within local communities. A moai is an informal group of people with common interests who look out for one another. For many, serving the community becomes part of their ikigai.&rdquo;</p>
<p>This book may be a good resource if you are interested in longevity. Even though I only expected to read about the ikigai concept, it contains a surprisingly large amount of valuable advice and practical tips to live a longer and happier life.</p>
<h2 id="chock-full">Chock-full</h2>
<p>&ldquo;Ikigai: The Japanese Secret to a Long and Happy Life&rdquo; is a small book that covers an astonishing amount of subjects. It covers topics such as meditation, flow states, and exercise methods such as Tai Chi and Qi Gong. The disadvantage is that it does not deeply cover any of these topics. But I think this book can be a very useful springboard for further research and a great introduction to many different subjects that can improve your life.</p>
<p>It is a short read, but it contains a lot of wisdom, and I learned much from this book. It has changed the way I think of retirement and the way I approach my work. I am also very interested in health and longevity. Through the conversations with the centenarians of Okinawa, I received a lot of helpful tips, which I&rsquo;m already applying to my daily life.</p>
<p>Have I discovered the art of being busy? I believe I took this definition too literally when I started reading this book. I found that Okinawans lead active lives full of meaning but also take plenty of rest. Always being busy does not mean that you have to work yourself to death. However, it does mean that the secret to reaching a healthy old age is to keep doing what you love. There is no need to become sedentary and only sit in front of the TV when you turn 67. I think Western culture has a lot to learn in this area.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Book Notes: The Unicorn Project</title>
      <link>https://mischavandenburg.com/zet/articles/unicorn-project/</link>
      <pubDate>Tue, 11 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/unicorn-project/</guid>
      <description>This book is the sequel to the Phoenix project. Both books are set at Parts Unlimited, a fictitious company that supplies car parts to DIY mechanics and repair shops. Phoenix is a new system that Parts Unlimited has worked on for multiple years. It is supposed to handle order processing and communication between manufacturing, stores, and clients. Phoenix will also play a role in sales and marketing. The company has been gathering customer data for years, but it cannot use any of this data yet.</description>
      <content:encoded><![CDATA[<p>
  <img loading="lazy" src="https://m.media-amazon.com/images/W/IMAGERENDERING_521856-T1/images/I/91UM5i4nirL.jpg" alt="Unicorn Project"  /></p>
<p>This book is the sequel to <a href="/zet/articles/phoenix-project">the Phoenix project</a>. Both books are set at Parts Unlimited, a fictitious company that supplies car parts to DIY mechanics and repair shops. Phoenix is a new system that Parts Unlimited has worked on for multiple years. It is supposed to handle order processing and communication between manufacturing, stores, and clients. Phoenix will also play a role in sales and marketing. The company has been gathering customer data for years, but it cannot use any of this data yet. Phoenix will enable it to generate targeted marketing campaigns from the data when it&rsquo;s finished.</p>
<p>But as we saw in the previous book, it is far from finished, and things go wrong all the time. The company is not doing well, the stock prices are falling, and they need an edge over the competition. Phoenix will be their edge, but they&rsquo;ve been working on it for years. Eventually, management decided that Phoenix needed to be deployed in two weeks. But it is far from ready.</p>
<p>The main character in the Unicorn project is Maxine, a senior developer who temporarily transferred to a different department. She had to work on the Phoenix project against her will because of an unfortunate situation that needed a scapegoat. However, Maxine decides to make the best out of it, and she&rsquo;d like to begin with one thing: to get a Phoenix build going on her laptop.</p>
<p>Very quickly, she finds it impossible to run a full build of the Phoenix project due to missing files and other elements. She is appalled and makes it her mission to get the build going, but she meets another hurdle every step of the way. Missing credentials. Missing binaries and libraries. And for each of these hurdles, she must submit a ticket with a different department. Very soon, she has over 20 tickets running with long waiting times. Just to get a build going on her machine so she can work! Dozens of developers were hired to work on the Phoenix project. But when she asks them if they&rsquo;ve managed to get a build going yet, Maxine is horrified to discover that they&rsquo;ve tried for several months but haven&rsquo;t made any progress. Maxine has made more progress in a week.</p>
<blockquote>
<hr>
<p>&ldquo;Everyone around here thinks features are important because they can see them in their app, on the web page, or in the API. But no one seems to realize how important the build process is. Developers cannot be productive without a great build, integration, and test process.&rdquo;</p>
<ul>
<li>Unicorn Project</li>
</ul>
<hr>
</blockquote>
<p>After a few weeks, Maxine receives an invitation to have a drink with a group of people who are very interested in her. When she arrives at the bar, she meets the Rebellion: a group of developers, managers, and people from Operations, who are tired of the old organizational structure and want to make real changes. They think out of the box and experiment with new technologies, even though they are not authorized to do so.</p>
<p>With the Rebellion, Maxine significantly improves the build and deployment process. They recognized that Phoenix actually never was being built in its entirety. Developers were always working on parts of the application. However, after a lot of struggle, they create a build process that enables each developer to become operational on his first day.</p>
<p>This is the first step of a long series of exciting events that lead to Phoenix becoming a success. By the end of the book, they have a completely new development and testing process, and they can deploy changes to production without needing to take the entire application down. This allows them to create targeted marketing campaigns and respond to changes in the market. The first campaign was a huge success and generated the highest sales ever.</p>
<p>Maxine&rsquo;s struggle with the build process was an eye-opening experience for me. It gave me a very practical example of the need for DevOps principles to enable delivering value to customers. It is also something I recognize in my current organization. For example, projects can get stuck on a firewall change that needs to be approved by an external party. By implementing DevOps principles and arranging teams according to the &ldquo;you build it, you run it&rdquo; principle, teams can be responsible for the entire process from idea to production and therefore have a very short release cycle for their application.</p>
<p>I thoroughly enjoyed the first part of the book. However, the second part was less engaging to me. It became long-winded and felt like butter spread over too much bread. The author demonstrates a high level of technical experience and knowledge through his descriptions of processes, deployments, and fictional applications. Although I understand the intention of making Parts Unlimited a believable company, I think it could have been accomplished with much less detail and words.</p>
<p>The second part has more corporate drama, such as temporarily suspended managers without any clear reason. The focus shifts from a development and operations perspective to a managerial perspective. Maybe I will reread the book in a few years and this part will make a lot more sense to me then. The same happened when I reread the Phoenix project. I could not understand some aspects of the book, which became much clearer to me when I revisited it after gaining experience in the field.</p>
<p>I highly recommend this book to anyone working as a developer, DevOps Engineer, or in operations, especially if you are starting your career. The book gave me a lot of insights into &ldquo;the old way of working&rdquo; and a better understanding of the need for DevOps principles in the modern IT landscape. However, make sure to read the Phoenix project first.</p>
<h2 id="the-unicorn-project-a-novel-about-developers-digital-disruption-and-thriving-in-the-age-of-data-by-gene-kim">The Unicorn Project: A Novel about Developers, Digital Disruption, and Thriving in the Age of Data by Gene Kim</h2>
]]></content:encoded>
    </item>
    
    <item>
      <title>I&#39;m In Love with my Work: Lessons from a Japanese Sushi Master</title>
      <link>https://mischavandenburg.com/zet/articles/jiro-sushi/</link>
      <pubDate>Sat, 08 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/jiro-sushi/</guid>
      <description>Last week I watched &amp;ldquo;Jiro Dreams of Sushi&amp;rdquo; again. It must be the eighth time I revisited this piece of art. I have very little interest in sushi, but there is much more to this documentary. It was first recommended to me by my good friend Anders more than five years ago, but I couldn&amp;rsquo;t grasp its underlying life lessons the first time around.
Even though I couldn&amp;rsquo;t fully understand the message, I was intrigued by it and kept returning to the documentary.</description>
      <content:encoded><![CDATA[<p>Last week I watched &ldquo;Jiro Dreams of Sushi&rdquo; again. It must be the eighth time I revisited this piece of art. I have very little interest in sushi, but there is much more to this documentary. It was first recommended to me by my good friend Anders more than five years ago, but I couldn&rsquo;t grasp its underlying life lessons the first time around.</p>
<p>Even though I couldn&rsquo;t fully understand the message, I was intrigued by it and kept returning to the documentary. Over the years, I managed to grasp more of its deeper meaning. The documentary is about Jiro: one of the best sushi chefs in the world. His tiny restaurant, located in a subway station, even received a 3 Michelin star rating. So how does a man operating from a subway restaurant become the best in the world? And what can a DevOps Engineer learn from a sushi chef?</p>
<p>
  <img loading="lazy" src="https://culinaireambiance.com/wp-content/uploads/2019/12/1_J8Bw5e9O9AfmLNdDKRRv6A-1.jpeg" alt="Jiro, the Master"  /></p>
<p>Jiro is referred to as a shokunin, a Japanese cultural phenomenon that doesn&rsquo;t have a direct translation into English. A shokunin is an artisan or a craftsman who has devoted his entire life to becoming a master of his craft. For example, there are shokunin carpenters, weavers, and blacksmiths. The shokunin achieve mastery by doing the same action over and over again and trying to improve with every repetition.</p>
<blockquote>
<hr>
<p>&ldquo;Once you decide on your occupation, you must immerse yourself in your work. You have to fall in love with your work. Never complain about your job. You must dedicate your life to mastering your skill. That&rsquo;s the secret of success and is the key to being regarded honorably.&rdquo;</p>
<ul>
<li>Jiro</li>
</ul>
<hr>
</blockquote>
<p>We get to know Jiro as a man who is devoted to his occupation. He follows the same routine every day; he even gets on the train from the same position. He says he dislikes holidays and prefers to be at work. He used to get up at 5:00 and get home after 22:00. Even at age 85, he&rsquo;s still working at the restaurant every day, simply because he doesn&rsquo;t want to do anything else.</p>
<p>But it&rsquo;s not just about putting in long hours and working past the retirement age. It&rsquo;s also about the mindset and the attitude that Jiro has adopted towards his craft:</p>
<blockquote>
<hr>
<p>&ldquo;I&rsquo;ve seen many chefs who are self-critical, but I&rsquo;ve never seen another chef who is so hard on himself. He sets the standard for self-discipline. He&rsquo;s always looking ahead. He&rsquo;s never satisfied with his work. He&rsquo;s always trying to find ways to make the sushi better or to improve his skills. Even now, that&rsquo;s what he thinks about every day.&rdquo;</p>
<ul>
<li>Masuhiro Yamamoto, Food Critic.</li>
</ul>
<hr>
</blockquote>
<h2 id="finding-my-craft">Finding my Craft</h2>
<p>It was hard to decide on my occupation. It took me 32 years and a lot of trial and error to find the answer. I have always envied people who knew they wanted to become a nurse since childhood.</p>
<p>What occupation should I choose? I answered this question by answering another question: what do I like to do in my free time? The answer was clear: I tinkered with computers. I had already coded several websites and loved experimenting with game automation. I ran &ldquo;bot farms&rdquo; on Linux servers which I configured myself from the command line.</p>
<p>I decided to make IT and tech my occupation. Fortunately for me, the job market was in my favor, and employers were much more willing to consider candidates without a formal background in IT. If you&rsquo;d like to read more, I wrote about my journey into DevOps in <a href="/my-journey-into-devops-so-far">this article</a>.</p>
<h2 id="the-ways-of-the-shokunin">The Ways of the Shokunin</h2>
<p>After making the career change, I adopted the shokunin mindset and dedicated my life to mastering my craft. In practical terms, I needed to commit to devoting my time to my profession and avoid straying from it. Making this commitment wasn&rsquo;t very difficult because I chose my career based on my free-time activities. It&rsquo;s more about adopting a mindset of striving to make everything relate to each other. For example, I try to select hobby projects that directly relate to what I&rsquo;m working with during the day. This way, my leisure activities will strengthen my professional skills, and my professional expertise will improve my hobby projects, creating a feedback loop that will eventually lead to results.</p>
<p>However, sticking to one thing does not necessarily come naturally to me. I have a tendency to pick up many different hobbies and get very excited about them. First, I will become extremely interested in a particular subject. Then, after approximately three months, I put it aside and become excited about something else. This isn&rsquo;t necessarily a case of <a href="https://en.wikipedia.org/wiki/Shiny_object_syndrome">Shiny Object Syndrome</a> because I tend to return to these hobbies in a cyclical pattern. As I apply myself to learning the skill with a feverish intensity, I&rsquo;m able to make a lot of progress during these bouts of obsession. But to the outside world, it might seem like I&rsquo;m constantly changing my mind about what I want.</p>
<p>I always considered this a negative character trait, but over the past few years, I&rsquo;ve learned to embrace it and guide this tendency in the right direction. Because I chose IT as my occupation and DevOps as an area of specialization, I gave myself a broad scope of interest with many sub-skills to learn. If you look at the <a href="https://roadmap.sh/devops">DevOps Roadmap</a>, you&rsquo;ll see that a good DevOps Engineer must master many different skills.</p>
<p>It&rsquo;s like I&rsquo;ve given myself a large playground with a fence around it, full of exciting things to learn. I can go down a Python rabbit hole for a few months and improve my coding skills. Later, I find myself sucked into <a href="https://mischavandenburg.com/linux-creativity/">builing my own OS</a> and learning more about Linux in the process. The fence around the playground isn&rsquo;t locked, but I do my best to stay inside the fence. When, for example, I start getting the urge to get back into music production again, I consider that it will take up a few hours a day to get back on track. Investing these hours into something that brings me joy, but is also related to my occupation, would be a better option. Reflecting in this way keeps me focused on my goals.</p>
<p>My website is another example of this fusion between professional and free-time activities. I love to write. It&rsquo;s a relaxing activity, even though it is very challenging sometimes. After I learn about a new subject, I try to write about it. This is the best way I know to verify whether I&rsquo;ve really understood the topic. When I sit down to write about something, I force myself to think clearly about it and to make sure everything aligns properly in my mind. When I cannot explain the concepts in a few clear sentences, I know I don&rsquo;t fully understand it yet.</p>
<blockquote>
<hr>
<p>&ldquo;Clear thinking becomes clear writing; one can&rsquo;t exist without the other.&rdquo;</p>
<ul>
<li>William Zinsser, On Writing Well</li>
</ul>
<hr>
</blockquote>
<p>Another way my blog supports my professional development is because I prefer to write my blog posts in neovim in the markdown format. Writing and editing text often involves moving a lot of words and paragraphs around. Because I use vim to do my writing, I&rsquo;m also constantly practicing the keybinds I use for writing and editing code during my day job. I often learn a new motion for my blog writing, which I use the next day at work.</p>
<h2 id="never-finished">Never Finished</h2>
<blockquote>
<hr>
<p>&ldquo;All I want to do is make better sushi. I do the same thing over and over, improving bit by bit. There is always a yearning to achieve more. I&rsquo;ll continue to climb, trying to reach the top, but no one knows where the top is. Even at my age, after decades of work, I don&rsquo;t think I have achieved perfection. But I feel ecstatic all day. I love making sushi. That&rsquo;s the spirit of the shokunin.&rdquo;</p>
<ul>
<li>Jiro</li>
</ul>
<hr>
</blockquote>
<p>Jiro taught me never to be satisfied with my skills. If I desire to become a master of my craft, there will never be a point where I can lean back and think that I&rsquo;ve learned enough. There&rsquo;s always something to improve, which is why I chose an IT career. My thirst for learning cannot be quenched, and there&rsquo;s always something to learn in this field.</p>
<p>
  <img loading="lazy" src="https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.ytimg.com%2Fvi%2FR2L5IrkQTV0%2Fmaxresdefault.jpg&amp;f=1&amp;nofb=1&amp;ipt=447c7c4a93a10b64191d653525bfe6ab6ab5aba0f3e9a1c64bc3617420bbc109&amp;ipo=images" alt="Never Finished"  /></p>
<h2 id="devotion">Devotion</h2>
<p>Although it might seem like a simple food documentary, do not be deceived. Jiro Dreams of Sushi is full of valuable life lessons. It&rsquo;s one of the documentaries which I revisit regularly. It&rsquo;s relaxing to watch, and I get hugely inspired by Jiro&rsquo;s devotion to his craft and powerful teachings every time.</p>
<p>Jiro&rsquo;s guidance has changed my life. He made me realize that if I wished to become a master of my craft, I needed to devote my life to it. I changed my lifestyle, so the things I do in my free time strengthen my professional skills. I also became more mindful of the content I consume and the things I read, trying to keep it related to my occupation. He showed me the true meaning of the phrase &ldquo;my work is my hobby.&rdquo;</p>
<blockquote>
<hr>
<p>&ldquo;Always look beyond and above yourself. Always try to improve on yourself. Always strive to elevate your craft. That&rsquo;s what he taught me.&rdquo;</p>
<ul>
<li>Yoshikazu, Jiro&rsquo;s oldest son.</li>
</ul>
<hr>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>How and Why I Started Using Vim</title>
      <link>https://mischavandenburg.com/zet/articles/how-started-vim/</link>
      <pubDate>Sun, 18 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/how-started-vim/</guid>
      <description>If you are just starting your Linux journey, you might have noticed that a few camps exist in the Linux world. Just like in any other area of life, it seems that groups of human beings enjoy dividing themselves instead of living in harmony. There are camps centered around Linux distributions (I use Arch, btw) but also around text editors.
The Beginning The reason why I started to use vim is rather practical.</description>
      <content:encoded><![CDATA[<p>If you are just starting your Linux journey, you might have noticed that a few camps exist in the Linux world. Just like in any other area of life, it seems that groups of human beings enjoy dividing themselves instead of living in harmony. There are camps centered around Linux distributions (I use Arch, btw) but also around text editors.</p>
<h2 id="the-beginning">The Beginning</h2>
<p>The reason why I started to use vim is rather practical. During the first part of my traineeship, I had access to subscriptions on AWS and Azure to experiment with virtual machines. This was a perfect place to learn to work with Ansible. Many of the labs involved setting up a few virtual machines, and I destroyed many VMs when I made some big mistakes in the configuration.</p>
<p>I was constantly working on new environments, and quickly it became very tiring to set everything up to connect remotely with Visual Studio Code every time.</p>
<p>So I just started to edit the text files on the virtual machines with the included editor, which happened to be vim.</p>
<h2 id="obsession-in-its-infancy">Obsession in its Infancy</h2>
<p>When you first use vim, it is a rather disorienting experience. But in every tutorial, I was told it would be difficult in the beginning but much faster and more effective in the end. I found this very appealing because I like to do things the hard way and challenge myself.</p>
<p>I discovered that there were people out there who did all of their text editing and coding in vim. I met programmers who refuse to use anything else and people who write entire books in vim. So there had to be something to it.</p>
<p>It also fitted very well with my intention of working on the command line as much as possible and moving away from GUI applications whenever possible. I like to move in this direction because I love the idea of controlling your entire workflow with your keyboard instead of using your mouse, and vim fits perfectly into this picture.</p>
<h2 id="what-i-like-after-nine-months">What I like after Nine Months</h2>
<p>At this point, I&rsquo;ve been using vim as my primary text editor for about nine months. In my current job, I work a lot with yaml files stored in private git repositories.</p>
<p>I only work with these files from the command line, and I don&rsquo;t have any other code editor installed. I use ripgrep and fzf (fuzzy file finder) to search through the files, and I use neovim to edit them. When I need to search for files from within vim, I use the awesome Telescope plugin.</p>
<p>In these months, I&rsquo;ve picked up a few tricks, and I am starting to see the power of vim. The best thing I like about it is that I don&rsquo;t have to leave my terminal window to do the tasks I need to do. Instead, I can search through the files I need to work with, open them, make adjustments, and commit them to the repository. Then I enter the command to run the ansible playbook, and it all happens in the same window, and I don&rsquo;t have to lift my fingers from the keyboard.</p>
<h4 id="keyboard-shortcuts">Keyboard Shortcuts</h4>
<p>Now that I am gaining more experience with vim, I&rsquo;m picking up more advanced usages that significantly improve my workflow. For example, &ldquo;da(&rdquo; meaning &ldquo;delete around parentheses&rdquo; to quickly delete the text between two parentheses. Or &ldquo;da&lt;&rdquo; to very quickly delete HTML tags. Another great feature is the visual block mode, where I can add comment tags to many lines simultaneously, for example.</p>
<h4 id="searching-and-navigation">Searching and Navigation</h4>
<p>Navigating large text files has become incredibly quick since I started using vim. Of course, it takes some getting used to, but it is a lovely experience to open a file, press / to search and enter the keyword and immediately arrive at the point I need to be—no scrolling with the mouse and no need to lift my hands from the keyboard.</p>
<p>I also love the ability to jump from sentence to sentence using ) or paragraphs using }.</p>
<h4 id="multiple-files">Multiple Files</h4>
<p>It takes a little while to get used to, but when you get into it, it is effortless to open up two files at a time if you need information from both. Often I need data from 4 or more files, and opening them quickly with keyboard commands has significantly improved my workflow speed.</p>
<h4 id="customization">Customization</h4>
<p>One of the things I enjoy most about vim is the ability to customize it exactly to my needs. I&rsquo;m completely in charge of the plugins which are loaded into vim and which colors it uses, and this appeals a lot to me. However, it can be rather overwhelming in the beginning. To be honest, it is still overwhelming after ten months. It can be tough to get an idea of where to start, which plugins you need, and which settings you need to change.</p>
<p>I just started with the base install of vim and started from there. Every time I required a particular functionality, I searched around to see if a plugin was available. Very often, someone out there had the same problem as you and created a plugin for it. For example, I recently installed a plugin for using emojis in vim 😄</p>
<h2 id="how-to-get-started">How to Get Started</h2>
<p>The short answer is to simply start using vim for all of your text editing, whether it be coding or writing for pleasure. It is a cliche to say, but it will be hard in the beginning, but I promise you it will pay off in the end.</p>
<p>The second thing I&rsquo;d recommend is to run vimtutor on a Linux machine. Do this once a day for a couple of weeks, and you&rsquo;ll know how to edit text files on any Linux system for the rest of your life, which is a precious skill.</p>
<p>Finally, don&rsquo;t spend too much time reading about all the available plugins. Your needs will become apparent to you as you start to use vim for all of your tasks, and you can search for plugins to address those needs. This way, you start with a minimal editor, which you&rsquo;ll build according to your needs.</p>
<h1 id="good-luck">Good Luck!</h1>
]]></content:encoded>
    </item>
    
    <item>
      <title>Book Notes: The Phoenix Project</title>
      <link>https://mischavandenburg.com/zet/articles/phoenix-project/</link>
      <pubDate>Fri, 16 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/phoenix-project/</guid>
      <description>When I started my DevOps traineeship, I borrowed this book from my boss and read it from cover to cover. I loved the story and the characters; it helped me understand &amp;ldquo;the old way&amp;rdquo; of doing things and the merits of implementing DevOps principles.
I reread the book ten months later. In the meantime, I&amp;rsquo;ve learned many new skills and technologies and started working full-time as a DevOps engineer.</description>
      <content:encoded><![CDATA[<p>
  <img loading="lazy" src="https://m.media-amazon.com/images/W/IMAGERENDERING_521856-T1/images/I/914-sUgELZL.jpg" alt="Phoenix Project"  /></p>
<p>When I started my DevOps traineeship, I borrowed this book from my boss and read it from cover to cover. I loved the story and the characters; it helped me understand &ldquo;the old way&rdquo; of doing things and the merits of implementing DevOps principles.</p>
<p>I reread the book ten months later. In the meantime, I&rsquo;ve learned many new skills and technologies and started working full-time as a DevOps engineer.</p>
<p>Not surprisingly, it made a lot more sense to me this time, and I&rsquo;m sure it will be even better when I reread it a few years later. In this article, I&rsquo;ll share my thoughts and notes on the book.</p>
<h2 id="the-story">The Story</h2>
<p>The main character is Bill Palmer, a mid-level IT manager in a manufacturing company called Parts Unlimited. Within a few pages, he is called into the CEO&rsquo;s office, and he is promoted to the VP of Operations, putting him in charge of IT, much against his own will or desire.</p>
<p>The situation Bill enters is a humorously chaotic one. We are thrown straight into a Sev 1 incident where managers point fingers and shout at each other. We quickly get the impression that this is a dysfunctional department that only performs tasks for the manager who shouts the loudest while fighting off crippling outages.</p>
<blockquote>
<p>It’s like the Wild West out here. We’re mostly shooting from the hip.”<br>
<em>The Phoenix Project</em></p>
</blockquote>
<p>The bulk of the story revolves around how Bill, together with his team of managers, Wes, Patty, and John, manage to turn this chaos into a department that does work according to a streamlined plan in a much more predictable manner.</p>
<p>To achieve this goal, Bill is introduced to Erik, a prospective board member of the company. Erik becomes Bill&rsquo;s mentor and guides Bill through the process of creating order in the chaos. Their interaction reminds me of Zen masters training their disciples by asking deep questions which don&rsquo;t have an immediately apparent answer.</p>
<h2 id="master--disciple">Master &amp; Disciple</h2>
<p>Erik takes Bill to the manufacturing plant of Parts Unlimited and tries to impress upon Bill that manufacturing planning principles from Lean can be applied to IT work. Erik argues that an IT department could be structured like a factory production line, but Bill is not ready to accept this.</p>
<p>A fundamental notion from manufacturing principles is that work should always be moving forwards along the production line, never backward. But unfortunately, this is very often the case in the &ldquo;old&rdquo; way of working: the development team works on an application for several months, and when they are finished with it, they throw it over the fence to the Operations people, whose job it is to deploy the application.</p>
<blockquote>
<p>One of the developers had actually walked in a couple of minutes ago and said, “Look, it’s running on my laptop. How hard can it be?”<br>
<em>The Phoenix Project</em></p>
</blockquote>
<p>However, as we see happening time after time in the book, usually the application is incompatible with the infrastructure it is deployed to. As a result, the application needs to go back to development. According to manufacturing theory, this is a situation where work goes backward through the production line, which we must avoid at all costs.</p>
<h2 id="implementation">Implementation</h2>
<p>Erik challenges Bill to start doing ten deployments a day instead of one deployment every nine months. Understandably, this is a ridiculous notion to Bill. The last few deployments were disastrous events that required his entire department to pull all-nighters through the weekend, and still, the stores were not managing to process all orders and payments.</p>
<p>However, Bill takes his mentor&rsquo;s advice and figures out a way to do it together with his team. One of the main problems they uncovered was the inconsistent deployment and production environments.</p>
<p>The solution to this problem was to involve the operations people in the development stage right from the beginning, so the development environment matched the production environment exactly. The environments were standardized and put in code with version control, and things started progressing quickly.</p>
<blockquote>
<p>As Wes talks, I think about Erik challenging me to think like a plant manager as opposed to a work center supervisor. I suddenly realize that he probably meant that I needed to span the departmental boundaries of Development and IT Operations.
<em>The Phoenix Project</em></p>
</blockquote>
<p>This is just one of the problems addressed by melting away the fence between Development and Operations. By the end of the book, the two camps started to work together much better. They come closer to the target of 10 deployments a day, and the DevOps way of working was born.</p>
<h2 id="closing-thoughts">Closing Thoughts</h2>
<p>I think this book is a must-read for anyone considering entering the DevOps field or anyone already working with DevOps.</p>
<p>As a nerd who loves structure and organization, the theme of the story is incredibly entertaining and satisfying to me. The authors excellently capture the transition from an utterly disorganized situation to a predictable environment with happy co-workers. Actually, I&rsquo;m a little embarrassed by how much joy this transition brings me.</p>
<p>Especially the second time around, it helped me better understand the underlying principles that enable the DevOps way of working in an organization. Moreover, it paints a great picture of how an organization can change for the better by embracing DevOps principles and how these changes express themselves in the improved quality and speed of software development and deployment. All of these advantages lead to delivering better value to the customer, which is the core focus of any productive and creative endeavor involving customers and end users.</p>
<h2 id="the-phoenix-project-written-by-gene-kim-kevin-behr-george-spafford">The Phoenix Project, written by Gene Kim, Kevin Behr, George Spafford</h2>
]]></content:encoded>
    </item>
    
    <item>
      <title>Certified Kubernetes Administrator (CKA) Exam Tips</title>
      <link>https://mischavandenburg.com/zet/articles/cka-tips/</link>
      <pubDate>Tue, 13 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/cka-tips/</guid>
      <description>I recently obtained my CKA certification. I started this certification journey with zero knowledge of Kubernetes. However, I was already working as a DevOps Engineer, and I know a fair bit of Linux. I daily drive Arch Linux and have LPIC-1 certification. It was handy to know where files are located on Linux systems and how to interact with systemd services. I also knew yaml quite well because I work with Ansible daily.</description>
      <content:encoded><![CDATA[<p>I recently obtained my CKA certification. I started this certification journey with zero knowledge of Kubernetes. However, I was already working as a DevOps Engineer, and I know a fair bit of Linux. I daily drive Arch Linux and have LPIC-1 certification. It was handy to know where files are located on Linux systems and how to interact with systemd services. I also knew yaml quite well because I work with Ansible daily. I passed on my first try, and I did one session of killer.sh.</p>
<h3 id="my-preparation">My preparation</h3>
<ul>
<li><a href="https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests/">KodeKloud CKA Course</a></li>
<li><a href="https://killer.sh/">Killer.sh Mock Exam</a></li>
<li><a href="https://killercoda.com/killer-shell-cka">Killercoda</a></li>
</ul>
<p>I kept track of the time I spent on this certification. In total, I spent 80 hours on study and practice.</p>
<h3 id="in-hindsight">In Hindsight</h3>
<p>I spent too much time repeating things during the KodeKloud course. This is the one thing I would do differently if I could start over. I went over some modules multiple times and kept meticulous notes. However, I have hardly used any of those notes. But they will be nice to have for the future.</p>
<p>I learned most from the killer.sh exams. So I would advise you to go through the KodeKloud course and do all the exercises, but don&rsquo;t spend too much time repeating stuff. If you don&rsquo;t understand the topic at all, it is, of course, necessary to repeat it. But you don&rsquo;t need to know all the details.</p>
<h3 id="killersh">Killer.sh</h3>
<p>After I finished the KodeKloud course, I purchased the exam voucher and started the killer.sh on Saturday morning. I wanted to simulate the exam experience as much as possible, so I set the timer and did not allow myself to stand up for two hours. My first round was humiliating. I only managed to get 24 out of 125 points. A little shocked by the experience, I spent the whole Saturday going through all the solutions of the exercises that killer.sh provides. The explanations they give are extensive, and I found them helpful. Saturday evening, I went out for dinner with friends, and on Sunday morning, I passed killer.sh. I spent the whole Sunday studying the solutions more and more, and on my last try on Sunday evening, I scored 115 out of 125.</p>
<h2 id="tips">Tips:</h2>
<ul>
<li>
<p>I know tmux quite well and used it extensively during the killer.sh, but it was not necessary during the exam. No need to learn it if you don&rsquo;t know it already.</p>
</li>
<li>
<p>Knowing vim well will save you a lot of time at the exam. For example, dG to delete all lines until the end of the file from your current location. Run &ldquo;vimtutor&rdquo; on a Linux system to learn the basics.</p>
</li>
<li>
<p>You cannot use bookmarks. Learn how to search the docs efficiently. One handy one I figured out was to control + F and enter &ldquo;kind: Pod&rdquo; or &ldquo;kind: PersistentVolume&rdquo; to immediately go to the example YAML.</p>
</li>
<li>
<p>my exam environment did not need much extra configuration. All I added to my .bashrc was alias v=vim and export do=&quot;&ndash;dry-run=client -o yaml&quot; so you can use &ldquo;k run Nginx $do &gt; Nginx.YAML&rdquo;</p>
</li>
<li>
<p>The exam environment is not as bad as people make it out to be on the internet. There is a little delay while scrolling through the docs in the browser, but working in the terminal didn&rsquo;t give me any problems. Get used to the environment on killer.sh, and there should not be any surprises in the real exam environment.</p>
</li>
<li>
<p>Skip questions you cannot solve immediately. But don&rsquo;t spend time reviewing all the questions, sorting by the highest % and doing those first. You will lose a lot of time evaluating all of these questions. It is much better to solve the questions during your first pass through and skip the ones you cannot immediately solve.</p>
</li>
<li>
<p>When the 120-minute timer ran out, I was presented with a screen that said &ldquo;quit&rdquo; or &ldquo;request more time.&rdquo; I was pretty sure I could not get more time for this exam, so I just pressed &ldquo;quit.&rdquo; After I pressed quit, the application closed immediately, and there was no confirmation whatsoever that they received my exam results or anything. This was extremely disorienting, and I was left doubting if I had done it correctly. Eventually, I could see in the Linux Foundation portal that my exam was in Grading status.</p>
</li>
<li>
<p>Speed is of the essence. An hour before my exam, I used killercoda to get into the mood and get things up to speed. Learn to solve things quickly and don&rsquo;t spend time having to arrange terminal windows on your screen or stumbling around in vim. You cannot afford to lose time on these things.</p>
</li>
<li>
<p>Finally, this video is an excellent summary of all the necessary tips and information: <a href="https://www.youtube.com/watch?v=8VK9NJ3pObU"></a></p>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Building my Own OS: Linux as a Creative Activity</title>
      <link>https://mischavandenburg.com/zet/articles/linux-creativity/</link>
      <pubDate>Sat, 10 Sep 2022 12:22:28 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/linux-creativity/</guid>
      <description>NOTE: In this article, I use a rather broad definition of “Operating System.” I do not intend to appear as if I wrote and compiled my own Linux kernel, nor do I understand the inner workings of the kernel written in C. Instead, with “building my OS from scratch,” I intend to convey that I used a minimal Linux distribution as a starting point and started building from there.
introduction I started using GNU/Linux as a daily driver about six months ago, and I have not regretted the decision ever since.</description>
      <content:encoded><![CDATA[<p><em>NOTE: In this article, I use a rather broad definition of “Operating System.” I do not intend to appear as if I wrote and compiled my own Linux kernel, nor do I understand the inner workings of the kernel written in C. Instead, with “building my OS from scratch,” I intend to convey that I used a minimal Linux distribution as a starting point and started building from there.</em></p>
<h2 id="introduction">introduction</h2>
<p>I started using GNU/Linux as a daily driver about six months ago, and I have not regretted the decision ever since. There has not been a single use case where I needed to use Windows for anything at all.</p>
<p>As I was getting more used to daily driving Linux, I noticed how much I enjoyed the ability to customize my operating system and workflow. Finally, after spending a weekend going down the customization rabbit hole, I had a good-looking terminal and customized neovim to perform as I needed it.</p>
<p>Not much later, I came across Arch Linux and the idea of building your own operating system from the ground up. I was instantly intrigued and knew I wanted to do the same. A few months have passed since I first came across Arch Linux, and now I am writing this blog post in neovim on my custom OS that I created from scratch. The font, the spacing, the colors, everything is exactly how I like it, and I love using it.</p>
<p>When you first install Arch Linux, all you get is a black screen with a blinking cursor. However, the experience of creating a fully functional graphical environment from “nothing” has been extremely satisfying, and I learned so much about GNU/Linux in the process. I also realized that this could be seen as a creative activity, like a painter creating his masterpiece from a blank canvas or a sculptor carefully chipping away at a block of marble.</p>
<h2 id="not-just-graphics">not just graphics</h2>
<p>When I say customization, I am not just referring to the visual aspects of the operating system. The things going on “under the hood” must also be carefully configured when you use a minimal distro such as Arch Linux.</p>
<p>Arch Linux comes with very few packages preinstalled, and every time you wish to add something to your system, you need to install it and enable the service in systemd. For example, after I did the installation and created my user account, I needed to run a command with root privileges. To my great surprise, even the “sudo” command was unavailable and needed to be installed.</p>
<p>This is the aspect I learned most from. Whenever I desired a certain functionality from my operating system, I needed to install and enable it. This has given me a much better understanding of the processes and daemons running on my system. It has also given me a greater appreciation of all the elements needed to provide a working environment.</p>
<h2 id="graphic-violence">graphic violence</h2>
<p>When you create an Arch Linux installation USB and boot it up, you are greeted with a command line and nothing else.</p>
<p><img loading="lazy" src="/creative1.png" type="" alt=""  /></p>
<p>When you install something more beginner-friendly, such as Ubuntu or Manjaro KDE, your installation will include a graphical desktop environment. But on Arch Linux, you must install and configure this yourself. Furthermore, to be able to render a graphical environment, you will also need to install and configure a display server such as Xorg.</p>
<p>When I started on my journey, I intended to create something that used minimal resources with a minimal look. Having used GNOME on Manjaro for a few months, I was very satisfied, but I wanted to try a tiling window manager to shave down even more resource usage. After some research, I ended up with the Awesome Window Manager. Here are some screenshots of the final result:</p>
<p><img loading="lazy" src="/creative2.png" type="" alt=""  /></p>
<p>This is what my desktop looks like when I boot up.</p>
<p><img loading="lazy" src="/creative3.png" type="" alt=""  />
Here I’m editing my window manager configuration file, while I have a browser open and keep an eye on my system resources</p>
<p><img loading="lazy" src="/creative4.png" type="" alt=""  /></p>
<p>my music listening setup, using mpd + ncmpcpp, cava and sptlrx. the lyrics are shown in real time as the music is played.</p>
<h2 id="creativity">creativity</h2>
<p><a href="https://dictionary.cambridge.org/dictionary/english">The Cambridge Dictionary</a> defines creativity as “the ability to produce original and unusual ideas, or <strong>to make something new or imaginative</strong>.”</p>
<p>When you embark on a journey, such as creating your operating system, you will probably start with a particular intention or a goal that you will work towards. With this goal in mind, you can start searching for the tools and color schemes you need to create the system that you have in mind. The result is a unique combination of tools, colors, fonts, and programs specifically tailored to your needs and wants and chosen by you.</p>
<p>Is this any different from a painter starting with a blank canvas or a musician starting with a fragment of a melody, ending up with a complete symphony? Entering commands into a computer terminal might not strike everybody as a creative activity. Still, I have found that it is a very effective and satisfying way of expressing myself and creating something I love to use daily. As an IT professional, I spend most of my time behind my computer. Doesn’t it make sense to put effort into building something customized to your needs?</p>
<h4 id="resources">resources</h4>
<p>If you want to start building your own OS, I recommend these resources:</p>
<p><a href="https://archlinux.org/">Arch Linux</a></p>
<p><a href="https://wiki.archlinux.org/">Arch Wiki</a></p>
<p><a href="https://www.reddit.com/r/unixporn/">r/unixporn – a subreddit about customization</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>How to Upgrade Java and Jenkins on Ubuntu 18.04</title>
      <link>https://mischavandenburg.com/how-to-upgrade-java-and-jenkins-on-ubuntu-18-04/</link>
      <pubDate>Tue, 19 Jul 2022 18:01:26 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/how-to-upgrade-java-and-jenkins-on-ubuntu-18-04/</guid>
      <description>Last week I upgraded Jenkins to the latest version on the server infrastructure at work. Starting with the Jenkins 2.357 release, Java 11 or Java 17 will be required to run Jenkins. Also, the upcoming LTS release will require Java 11.
This means that I also needed to update Java on our Jenkins servers. Here are the steps that I did to perform the Jenkins and Java upgrade.
SSH into the server and stop the service.</description>
      <content:encoded><![CDATA[<p>Last week I upgraded Jenkins to the latest version on the server infrastructure at work. Starting with the Jenkins 2.357 release, Java 11 or Java 17 <a href="https://www.jenkins.io/blog/2022/06/28/require-java-11/">will be required to run Jenkins</a>. Also, the upcoming LTS release will require Java 11.</p>
<p>This means that I also needed to update Java on our Jenkins servers. Here are the steps that I did to perform the Jenkins and Java upgrade.</p>
<p>SSH into the server and stop the service. Then get the latest upgrades for your server, which is good practice:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">service jenkins stop
</span></span><span class="line"><span class="cl">apt-get update
</span></span><span class="line"><span class="cl">apt-get upgrade
</span></span></code></pre></div><p>Depending on your setup, the apt-get upgrade command might upgrade Jenkins to the latest version that does not require Java 11+. In my case, that was 3.346.</p>
<p><strong>When you get a question about updating your current config file, take the default option. This option keeps your current configuration.</strong></p>
<p>However, if your Jenkins is installed from a binary or another source, you might need to upgrade Jenkins to 3.346 using the Jenkins.war file:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> /usr/share/jenkins
</span></span><span class="line"><span class="cl">mv jenkins.war jenkins.war.old
</span></span><span class="line"><span class="cl">wget https://updates.jenkins-ci.org/latest/jenkins.war
</span></span><span class="line"><span class="cl">service jenkins start
</span></span></code></pre></div><p>When you start Jenkins, it will be updated to the latest version that does not require Java 11 or higher. You will notice that there will be a new folder called migrate in /usr/share/jenkins , and the jenkins.war is now located in /usr/share/java</p>
<p>This is where I got confused because it did not patch to the latest version, only up to 3.346 and the jenkins.war file was no longer being updated from the /usr/share/jenkins folder.</p>
<p>The reason is that this update moves the .war file to the /usr/share/java directory.</p>
<h1 id="java">java</h1>
<p>To get Jenkins to the latest version, we need to install or update Java and check if it has worked:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">apt-get install default-jre
</span></span><span class="line"><span class="cl">java -version
</span></span></code></pre></div><p>Now that you have updated the java version, you are ready to update Jenkins to the latest version.</p>
<p>Notice that we use the /usr/share/java folder now, instead of /usr/share/jenkins</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">service jenkins stop
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> /usr/share/java
</span></span><span class="line"><span class="cl">mv jenkins.war jenkins.war.old
</span></span><span class="line"><span class="cl">wget https://updates.jenkins-ci.org/latest/jenkins.war
</span></span><span class="line"><span class="cl">service jenkins start
</span></span></code></pre></div><h1 id="nodes">nodes</h1>
<p>When I accessed the Jenkins GUI, everything seemed fine, and my version was up to 3.358.</p>
<p>However, I noticed that the build nodes were all offline. When inspecting the logs, I saw the following error:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">java.io.EOFException
</span></span><span class="line"><span class="cl">	at java.base/java.io.ObjectInputStream<span class="nv">$PeekInputStream</span>.readFully<span class="o">(</span>ObjectInputStream.java:2905<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at java.base/java.io.ObjectInputStream<span class="nv">$BlockDataInputStream</span>.readShort<span class="o">(</span>ObjectInputStream.java:3400<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at java.base/java.io.ObjectInputStream.readStreamHeader<span class="o">(</span>ObjectInputStream.java:936<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at java.base/java.io.ObjectInputStream.&lt;init&gt;<span class="o">(</span>ObjectInputStream.java:379<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at hudson.remoting.ObjectInputStreamEx.&lt;init&gt;<span class="o">(</span>ObjectInputStreamEx.java:49<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at hudson.remoting.Command.readFrom<span class="o">(</span>Command.java:142<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at hudson.remoting.Command.readFrom<span class="o">(</span>Command.java:128<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at hudson.remoting.AbstractSynchronousByteArrayCommandTransport.read<span class="o">(</span>AbstractSynchronousByteArrayCommandTransport.java:35<span class="o">)</span>
</span></span><span class="line"><span class="cl">	at hudson.remoting.SynchronousCommandTransport<span class="nv">$ReaderThread</span>.run<span class="o">(</span>SynchronousCommandTransport.java:61<span class="o">)</span>
</span></span><span class="line"><span class="cl">Caused: java.io.IOException: Unexpected termination of the channel
</span></span><span class="line"><span class="cl">	at hudson.remoting.SynchronousCommandTransport<span class="nv">$ReaderThread</span>.run<span class="o">(</span>SynchronousCommandTransport.java:75<span class="o">)</span>
</span></span></code></pre></div><p>Observing that the error had something to do with Java, I ssh’d into the build nodes and updated Java there as well with the same command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">apt-get install default-jre
</span></span></code></pre></div><p>After updating Java on the build node, head back to the GUI on the master node and restart the build node.</p>
<p>It should now be online again.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>5 Reasons Why I Changed my Career to IT in my Thirties</title>
      <link>https://mischavandenburg.com/5-reasons-why-i-changed-my-career-to-it-in-my-thirties/</link>
      <pubDate>Mon, 04 Jul 2022 20:04:33 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/5-reasons-why-i-changed-my-career-to-it-in-my-thirties/</guid>
      <description>In this post, I’d like to share the five main reasons why I changed my career to IT in my thirties. Making a career change can be daunting, especially when you are past your twenties, and employers can get more skeptical of hiring and training you. However, when I passed my twenties and became more serious and intentional about my life and career, I decided to take the plunge and hope for the best.</description>
      <content:encoded><![CDATA[<p>In this post, I’d like to share the five main reasons why I changed my career to IT in my thirties. Making a career change can be daunting, especially when you are past your twenties, and employers can get more skeptical of hiring and training you. However, when I passed my twenties and became more serious and intentional about my life and career, I decided to take the plunge and hope for the best. These are listed in no particular order.</p>
<h2 id="1-job-opportunities">#1 job opportunities</h2>
<p>IT has always been a field with many job opportunities, and with the current movements of digitalization and automation, there is no sign that it will slow down.</p>
<p>According to <a href="https://www.bls.gov/ooh/computer-and-information-technology/home.htm">the U.S. Bureau of Labor Statistics</a>, “Employment in computer and information technology occupations is projected to grow 13 percent from 2020 to 2030, faster than the average for all occupations. These occupations are projected to add about 667,600 new jobs. Demand for these workers will stem from greater emphasis on cloud computing, the collection and storage of big data, and information security.”</p>
<p>The situation is no different here in the Netherlands. Currently, there is a shortage of people in IT, and employers are much more willing to provide training to motivated individuals to make a change.</p>
<h2 id="2-remote-work">#2 remote work</h2>
<p>I think remote work is one of the best parts of living in post-pandemic 2022. I am an introvert, and having a quiet, stable space without distractions, which is the same from day to day, is a huge boost to my productivity.</p>
<p>Secondly, I think it is crucial to be mindful of your posture and body while working at a desk. For example, I am dependent on having a standing desk which I adjust more than ten times a day. I also need a chair suitable for my body type to avoid getting stiff and getting a sore back. Although some offices take care of providing these facilities to their employees, I think it is beneficial to invest in your own setup, which you can tailor to your own needs.</p>
<p>Thirdly, working from anywhere in the world is a massive advantage. I am not very interested in living a digital nomad lifestyle, working from a MacBook in coffee shops, but I think it’s great that you can spend some time abroad while working from that location.</p>
<h2 id="3-personal-interest">#3 personal interest</h2>
<p>This is a big one. You should not change your career to IT just because it earns well or because you think you can work from the beach in Thailand. I have been tinkering with computers and programming languages since I was a kid and have always enjoyed it. I always found myself “the computer guy” in groups of friends or colleagues.</p>
<p>However, for some reason, I never managed to make my career out of it until now, and I get a lot of satisfaction from my work every day after I made the change.</p>
<h2 id="4-high-income">#4 high income</h2>
<p>It is no secret that tech jobs are some of the best paying jobs in the U.S., having<a href="https://www.bls.gov/oes/current/oes_nat.htm#15-0000"> a mean wage of $99,860</a>. And if you work your way up into management, there are even higher salaries. Here <a href="https://www.nationaleberoepengids.nl/salaris/ict">in the Netherlands</a>, it is also a financially sound choice, with a mean wage of €47.200</p>
<h2 id="5-fast-changing-field">#5 fast-changing field</h2>
<p>IT is a broad field with many little niches you can get into, and every niche is constantly developing. Being in IT means you will need to stay on board by continuing to learn the new technologies and languages to keep on track.</p>
<p>This is also what makes it exciting to me, being a life-long learner. There is always more to learn and some cutting-edge technology to become familiar with, which can improve your workflow and your deployments.</p>
<h2 id="links">links:</h2>
<p><a href="https://www.bls.gov/ooh/computer-and-information-technology/home.htm">Computer and Information Technology Occupations – US Bureau of Labor Statistics</a></p>
<p><a href="https://www.bls.gov/oes/current/oes_nat.htm#15-0000">May 2021 National Occupational Employment and Wage Estimates</a></p>
<p><a href="https://www.nationaleberoepengids.nl/salaris/ict">Salaries in IT - Dutch</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>How To Move Steam Game Files To a Separate Hard Drive on Linux</title>
      <link>https://mischavandenburg.com/how-to-move-your-steam-game-files-to-a-separate-hard-drive-on-linux/</link>
      <pubDate>Thu, 26 May 2022 09:03:44 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/how-to-move-your-steam-game-files-to-a-separate-hard-drive-on-linux/</guid>
      <description>I have installed my OS on a 240GB SSD, and I prefer to keep my data on a different disk to leave enough space to work with. I wanted to move my steam game files to a separate hard drive on Linux. I’ll show you what I did to make this happen in this article. I use Manjaro GNOME.
First, you need a correctly partitioned hard drive.
To wipe your drive clean and have a single partition on it, we’ll use GParted.</description>
      <content:encoded><![CDATA[<p>I have installed my OS on a 240GB SSD, and I prefer to keep my data on a different disk to leave enough space to work with. I wanted to move my steam game files to a separate hard drive on Linux. I’ll show you what I did to make this happen in this article. I use Manjaro GNOME.</p>
<p>First, you need a correctly partitioned hard drive.</p>
<p>To wipe your drive clean and have a single partition on it, we’ll use GParted.</p>
<p><img loading="lazy" src="/gparted1.png" type="" alt=""  />
Select your disk in the upper right corner.</p>
<p>Then go to Device and select Create Partition Table:</p>
<p><img loading="lazy" src="/parted2.png" type="" alt=""  /></p>
<p>Follow the wizard and use an ext4 filesystem. NTFS can cause problems because Steam cannot read it properly.</p>
<h1 id="mounting">mounting</h1>
<p>To use a disk or a partition in Linux, it needs to be mounted.</p>
<p>List your devices and identify the one you wish to mount by using the “lsblk” command.</p>
<p>In my case, I wish to mount the drive sdc1</p>
<p><img loading="lazy" src="/lsblk.png" type="" alt=""  /></p>
<p>On Linux, all filesystems need to be mounted before they can be used. I wanted my whole disk to be available in the directory /mnt/data</p>
<p>Before mounting, I created the directory.</p>
<p><code>cd /mnt</code></p>
<p><code>sudo mkdir data</code></p>
<p>When you make the directory by using sudo, the directory owner will be the root user. This means that you cannot access the directory and write to it from your own user.</p>
<p>Use this command to change the ownership of the directory. Replace “mischa” with your username.</p>
<p><code>sudo chmod mischa:mischa data</code></p>
<p>Verify that the directory now has the correct ownership:</p>
<p><img loading="lazy" src="/steamgames1.png" type="" alt=""  /></p>
<p>Now you can mount your directory, so it is available for use.</p>
<p><code>mount /dev/sdc1 /mnt/data</code></p>
<h1 id="mounting-on-boot">mounting on boot</h1>
<p>For the mount to happen automatically on startup, you’ll need to add it to the /etc/fstab file. We start by finding the UUID of our disk.</p>
<p>Use the following command:</p>
<p><code>ls -al /dev/disk/by-uuid/</code></p>
<p><img loading="lazy" src="/steamgames2.png" type="" alt=""  /></p>
<p>In my case the UUID will be 50d608bc-a7ad-4ff6-bf44-bb6f26efa4f6</p>
<h3 id="etcfstab">/etc/fstab</h3>
<p>open the file in your favorite editor. I like to use vim.</p>
<p><code>sudo vim /etc/fstab</code></p>
<p>Add a new entry to your /etc/fstab file and use the following parameters:</p>
<p><code>UUID=50d608bc-a7ad-4ff6-bf44-bb6f26efa4f6 /mnt/data ext4 defaults 0 0</code></p>
<p><img loading="lazy" src="/steamgames3.png" type="" alt=""  /></p>
<p>Before we go further, verify that we did this correctly by using the following command:</p>
<p><code>findmnt --</code>verify</p>
<p>This will verify the /etc/fstab file. Not meaning to scare you, but an incorrectly configured fstab may lead to an unbootable system.</p>
<p>Now reboot your system and check if your disk is mounted automatically.</p>
<p>It is also a good idea to cd to your mounted directory and touch a file to see if you have write permissions.</p>
<p><img loading="lazy" src="/steamgames4.png" type="" alt=""  /></p>
<h1 id="steam">Steam</h1>
<p>Now it’s time to set things up in Steam. Open Steam and open your settings.</p>
<p><img loading="lazy" src="/steamgames5.png" type="" alt=""  /></p>
<p>go to Downloads –&gt; Steam Library Folders</p>
<p>Click the + button and navigate to your mounted drive.</p>
<p><img loading="lazy" src="/steamgames6.png" type="" alt=""  /></p>
<p>To test, install a game and reboot your system.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Goodbye Windows, Hello Linux! Switching to Linux as my Daily Driver</title>
      <link>https://mischavandenburg.com/goodbye-windows-hello-linux-switching-to-linux-as-my-daily-driver/</link>
      <pubDate>Tue, 26 Apr 2022 06:38:57 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/goodbye-windows-hello-linux-switching-to-linux-as-my-daily-driver/</guid>
      <description>After getting my LPIC-1 certification, my interest in Linux continued to grow. When I started my new job, I performed more sysadmin tasks, such as increasing the size of filesystems or removing backups, and it felt good to put the theory into practice.
I was still using Windows in my personal setup, and I started running into limitations. Finally, I realized how much I appreciated the freedom and control over my system that Linux gave me.</description>
      <content:encoded><![CDATA[<p>After getting my LPIC-1 certification, my interest in Linux continued to grow. When I started my new job, I performed more sysadmin tasks, such as increasing the size of filesystems or removing backups, and it felt good to put the theory into practice.</p>
<p>I was still using Windows in my personal setup, and I started running into limitations. Finally, I realized how much I appreciated the freedom and control over my system that Linux gave me. So I decided I wanted to switch to Linux for my daily operating system. But a few things were holding me back. Will I be able to play my favorite games? Will I run into a lot of problems with my sound and microphone? Are all the apps I need for working from home available on Linux? Can I even edit videos on Linux?</p>
<h2 id="preparing-to-switch">preparing to switch</h2>
<p>I made a dual boot install of Ubuntu to try things out to answer these questions. I started things off by setting small goals for myself. For example, I need to be able to work from home. Which programs do I need? And I started from there.</p>
<p>I had no problems installing Slack, Zoom, Teams, and all the other programs I needed for my work. I was very surprised by how well all of the external hardware worked. My Bluetooth keyboard and mouse worked immediately without needing to install any drivers. Even my USB webcam worked instantly without any problems.</p>
<p>To put this into perspective, I spent an entire afternoon getting my keyboard to work correctly on my MacBook. It needed a lot of extra drivers. Still, there is a 4-second delay when I press my volume up/down keys when working on my MacBook. All of this works perfectly on my Linux OS without any delays and without any drivers or extra configuration.</p>
<p>I kept my dual boot setup for a while as I was trying out different distros, and I eventually ended up with Manjaro as my distro of choice. I like it because it is based on Arch Linux, and it gives you access to the Arch User Repository (AUR). I want to use Arch eventually, and I figured this might be a good stepping stone.</p>
<h2 id="fate-decided-for-me">fate decided for me</h2>
<p>This dual boot setup continued for a while as I was warming up to the idea of completely abandoning Windows. I set Linux as my default boot option, and after a few weeks, I realized I hadn’t needed to boot into Windows for anything at all. However, I still didn’t feel quite ready to switch completely to Linux.</p>
<p>One evening I wanted to make another fresh install to check out the GNOME version of Manjaro. I was doing a lot of chores at the same time, and it was getting quite late, but I wanted to have the installer running as I was doing other tasks. Probably not my most brilliant move.</p>
<p>You probably know what is coming: in between my chores, I started the installation. In a moment of carelessness, I managed to point the installation to my Windows partition, and it was completely wiped and replaced with a sparkling fresh Linux install.</p>
<p>“Well, I guess I am moving to Linux today!” I thought while I suppressed a hint of panic as I racked my brain to see if I had lost any important files. I knew that most of my important stuff was safely backed up in the cloud. But if I had formatted my Windows drive by choice, rather than by accident, I would have backed up a lot more files.</p>
<h2 id="first-week-without-windows">first week without windows</h2>
<p>A week ago, I lost my complete Windows install, but there hasn’t been a single moment where I regretted making the switch. Fortunately, it also seems that I did not lose anything important.</p>
<p>I am learning so much by forcing myself to use Linux as a daily driver. Most things are correctly configured out of the box. But sometimes, you have to do some work to get the configuration you like.</p>
<p>For instance, after installing Steam, I wanted to have the game files located on a different hard disk because my OS SSD is only 256GB. This required me to format my data SSD to an ext4 filesystem and mount it in a folder. I also needed to add it to my /etc/fstab file to make sure that it mounts automatically when I boot my PC.</p>
<p>These tasks have been great practice for the things I need to do on my servers at work, and they will make me approach these tasks with a little more confidence because I have done them before on my personal setup. This is the great advantage of having Linux as a daily driver if you are becoming a DevOps Engineer or Linux System Administrator.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Yadm: Keep Track of Your Precious Dotfiles</title>
      <link>https://mischavandenburg.com/yadm-keep-track-of-your-precious-dotfiles/</link>
      <pubDate>Sat, 02 Apr 2022 09:55:03 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/yadm-keep-track-of-your-precious-dotfiles/</guid>
      <description>This week I learned about yadm: yet another dotfile manager. It is the perfect way to keep track of all your custom configuration files, known as dotfiles.
Even if you have only a little bit of experience with Linux, you know that everything is managed in files. All configuration parameters are set or changed in text files stored on the hard disk. These files are usually located in your home directory and are hidden by default.</description>
      <content:encoded><![CDATA[<p>This week I learned about <a href="https://yadm.io">yadm: yet another dotfile manager</a>. It is the perfect way to keep track of all your custom configuration files, known as dotfiles.</p>
<p>Even if you have only a little bit of experience with Linux, you know that everything is managed in files. All configuration parameters are set or changed in text files stored on the hard disk. These files are usually located in your home directory and are hidden by default. This is indicated by prefixing the file with a period. So the configuration file for the vim editor is .vimrc, and for zshell you use the .zshrc. This is why configuration files are referred to as dotfiles.</p>
<h3 id="customisation">customisation</h3>
<p>The more I work with Linux, the more I appreciate the ability to customize things. When I first started, I was pretty satisfied with the vanilla experience. You punch your commands into the terminal, and you do your tasks. What more could you need?</p>
<p>This started to change when I began working with senior engineers who built their custom setups over the years. I saw them opening 3 terminal windows in a perfect ratio with beautiful colorschemes or previewing files in a file browser directly in vim so they could split them vertically and edit them side by side.</p>
<p>I wanted to create a similar setup by adding settings and plugins to the .vimrc and .zshrc files. However, before going down this rabbit hole, I asked myself the following question. How can I bring this configuration with me to other machines? What happens if my laptop gets stolen and I lose my precious configuration files?</p>
<h3 id="yet-another-dotfile-manager">yet another dotfile manager</h3>
<p>Enter yadm. I had thought of putting my dotfiles in a GitHub repo, but this brings up a whole set of new challenges where you would need to create symbolic links across your system to have the files in their correct places. Yadm solves this problem.</p>
<p>Yadm turns your home directory ( ~/ ) into a Git repo which can be pushed to a host of your choice. You can add your files one by one, and yadm will track them. The best thing is that you can add the files from all over your system, and yadm will not bother with any of the other files in your home directory.</p>
<h3 id="you-want-git-for-your-dotfiles">you want git for your dotfiles</h3>
<p>Setting up your configuration files in a git repository has a lot of advantages:</p>
<ul>
<li>configuration is saved in multiple places</li>
<li>easily share your configuration across machines</li>
<li>version control</li>
</ul>
<p>Version control is especially useful. You will always be able to trace back that one plugin you used a few years ago, but you cannot remember the name of. And it is fun to watch your configuration grow over time.</p>
<h3 id="setting-up-yadm">setting up yadm</h3>
<p><a href="https://yadm.io/docs/install">Installing yadm</a> is a breeze. For my mac I just used</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">brew install yadm
</span></span></code></pre></div><p>or you can use the apt-get or dnf install equivalents if you are on Linux.</p>
<p><strong>You interact with yadm the same way you interact with git. You simply replace the word git with yadm in the commands.</strong></p>
<p>Then you navigate to your home directory and set up the repository. If you don’t have a repository yet:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">yadm init
</span></span><span class="line"><span class="cl">yadm add &lt;important file&gt;
</span></span><span class="line"><span class="cl">yadm commit
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">yadm remote add origin &lt;url&gt;
</span></span><span class="line"><span class="cl">yadm push -u origin &lt;<span class="nb">local</span> branch&gt;:&lt;remote branch&gt;
</span></span></code></pre></div><p>Or if you already have a dotfiles repository:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">yadm clone &lt;url&gt;
</span></span><span class="line"><span class="cl">yadm status
</span></span></code></pre></div><p>And that’s it. Now add your configuration files and push them to your hosted repo:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">yadm add ~/.vimrc 
</span></span><span class="line"><span class="cl">yadm add ~/.zshrc
</span></span><span class="line"><span class="cl">yadm commit -m <span class="s2">&#34;first commit&#34;</span>
</span></span><span class="line"><span class="cl">yadm push
</span></span></code></pre></div><p>You will notice that yadm expects you to add all the files every time you want to make a new commit. Use this command to stage all the files you added previously:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">yadm add -u
</span></span></code></pre></div><h3 id="enjoy-your-synched-customisation">enjoy your synched customisation</h3>
<p>Having your dotfiles in a GitHub repo makes it easy to set up your preferred settings on a new machine or environment. So install yadm and pull your repo, and off you go!</p>
<p>I hope you will enjoy it as much as I do. Crafting a customized setup takes a lot of time and effort, and now that I finally have an excellent solution to keep track of my files, I am ready to dive into the customization rabbit hole.</p>
<p>Download yadm <a href="https://yadm.io">here.</a> Here you will also find all the necessary information to install and configure your yadm.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>LPIC-1 Study Guide</title>
      <link>https://mischavandenburg.com/lpic-1-study-guide/</link>
      <pubDate>Wed, 16 Mar 2022 21:20:19 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/lpic-1-study-guide/</guid>
      <description>I recently obtained my LPIC-1 certification, and in this blog post, I’ll share the strategy and techniques I used to pass this exam and share my thoughts on the certification. Because I am a Linux novice, the exam was a pretty tough grind for me. This article offers a beginner’s perspective on the LPIC-1 certification. Is the LPIC-1 hard to pass? Keep reading to find out.
Before this certification, I had only a little bit of experience.</description>
      <content:encoded><![CDATA[<p>I recently obtained my LPIC-1 certification, and in this blog post, I’ll share the strategy and techniques I used to pass this exam and share my thoughts on the certification. Because I am a Linux novice, the exam was a pretty tough grind for me. This article offers a beginner’s perspective on the LPIC-1 certification. Is the LPIC-1 hard to pass? Keep reading to find out.</p>
<p>Before this certification, I had only a little bit of experience. I deployed LAMP stacks using Ansible and configured VMs to be able to communicate with each other using only the command line. I also did a “Linux Fundamentals” video training. I could navigate the filesystem, edit text files and work in the terminal, but that was about it.</p>
<p><img loading="lazy" src="/hard.jpg" type="" alt=""  /></p>
<h3 id="is-it-hard">Is it hard?</h3>
<p>For a beginner: yes, it was hard! But if you are a Linux administrator with a few years of experience, these exams probably are not very difficult to pass. However, even if you are experienced, be prepared to do a lot of memorization. Even though the requirements on the website seem very basic and straightforward, when you dig into the study resources, you will soon discover that you need to learn a large host of commands and many of their accompanying parameters. For example, you will need to know what grep -H does precisely, the difference between passwd -l, chage -l, and chage -L, the location of the directory that contains all the timezones, and the directory that contains the printer configurations for CUPS.</p>
<h3 id="the-certification">The certification</h3>
<p>The LPIC-1 certificate requires candidates to pass the LPI 101-500 and 102-500 exams. These exams test the candidates on various subjects, such as file management, boot loaders, networking fundamentals, user and group management, file systems and partitioning, and much more.</p>
<p>Each exam has a 600-page syllabus, so to get your LPIC-1 certification, you need to work through 1200 pages and memorize a few hundred commands and parameters. However, if you work as a Linux sysadmin, you’ll probably know many of these commands and concepts.</p>
<h3 id="study-materials">Study Materials</h3>
<p>I attended a 4-day course that covered both exams. However, because of the large amount of information that needs to be covered, the teacher could only address the subjects on a superficial level. Therefore, I would advise you to be suspicious of any courses that promise to prep you for the exams in 4 days if you are a beginner. I estimate that you need at least double that amount to get some proper explanation of the material.</p>
<h4 id="lpi-syllabus">LPI Syllabus</h4>
<p>After finishing the course, it became clear that I needed a lot of studying to pass the exams. Fortunately, LPI has created a syllabus for each exam. These are available for free on the <a href="https://www.lpi.org/our-certifications/lpic-1-overview">LPI.org website.</a></p>
<h4 id="lpi-practice-exams">LPI Practice Exams</h4>
<p>It is crucial to test your knowledge. This is the resource I used:</p>
<p><a href="https://amzn.to/3KHAkCZ">LPIC-1 Linux Professional Institute Certification Practice Tests: Exam 101-500 and Exam 102-500 </a></p>
<p>If you are a member of O’Reilly’s, you can read the book there. It contains around 90 practice questions for every chapter in the LPI syllabus. The questions test your knowledge in detail and are a great way to determine whether you have fully grasped the material.</p>
<p>However, the book was written in 2019 and contains questions about certain subjects that have since been removed from the exams. So if you suddenly encounter questions that do not seem familiar at all, make sure to double-check that it is actually an exam objective.</p>
<p>Lastly, I used <a href="https://www.udemy.com/share/1029gO3@XonS1Jh2QkFmIV_mN-r8Rbx04vCYiyykPhTpewu5iLZQVNIYMVe4z53YFSxp2tly/">these practice exams on Udemy</a>.</p>
<h3 id="memorization">Memorization</h3>
<p>As I have stated before, the exams require you to do a lot of memorization. Fortunately, we have some tools and techniques available to help us with this task.</p>
<p>The primary tool is Anki. If you are not familiar with it, Anki is a very simple and free program that allows you to create flashcards that you can use to study and test your knowledge. The best thing about Anki is that it implements spaced repetition. You can download and learn more about Anki here: <a href="https://apps.ankiweb.net/">https://apps.ankiweb.net</a></p>
<p>Secondly, I am fond of memory techniques. You can remember things much more quickly by visualizing them in your mind or utilizing techniques such as Memory Palaces or the Method of Loci. If you are interested in learning more about memory techniques, I highly recommend Dr. Anthony Metivier’s <a href="https://www.youtube.com/c/AnthonyMetivierMMM/featured">YouTube channel.</a></p>
<p><img loading="lazy" src="/palace.jpg" type="" alt=""  /></p>
<h3 id="strategy">Strategy</h3>
<p>Here is the strategy that I used to pass the exams:</p>
<ul>
<li>Read through a chapter and take notes.</li>
<li>Make Anki flashcards for all the commands and flags that you do not know yet</li>
<li>Do the exercises at the end of the chapter</li>
<li>Do the practice exam for your chapter from the exam book, which should give you a good indication of how well you have grasped the material.</li>
<li>Make flashcards of all the questions that you answered wrong (trust me, there will be quite a few)</li>
<li>Use Anki to test yourself and memorize all of the commands and exam questions</li>
</ul>
<h3 id="tips">Tips:</h3>
<ul>
<li>Do your Anki reviews every day. On some days I was adding more than 100 new cards, which will lead to a lot of reviews in the coming days</li>
<li>Although the syllabus for exam 101 explained things very well, the 102 syllabus sometimes is very meager in its explanations and you might need to supplement with reading man pages, YouTube videos, and other tutorials. For example, I needed to find quite a bit of supplementary material for chapter 109 Networking Fundamentals.</li>
<li>Ask for help if you don’t understand a certain topic</li>
<li>Don’t think you can get away with skipping a topic. You will be tested on absolutely everything that’s in the syllabus, trust me.</li>
<li>Try doing it together with someone else. I was doing it together with my friend and colleague, and it was extremely useful to be able to share things I struggled with and to discuss things with him to understand them better. Thank you for the good times, Gino!</li>
</ul>
<h3 id="my-thoughts-on-the-certification">My thoughts on the certification</h3>
<p>The subject matter is extensive, and I know my way around Linux much better now. Therefore, if I encounter a problem, I am better positioned to assess where the cause might be and then solve the problem from there. I also feel I have a much better grasp of basic networking concepts, which will prove to be very useful in many situations in my work as a DevOps Engineer.</p>
<p>However, there are also a few drawbacks to this certification. I think there is too much emphasis on memorizing commands and their flags. I think it is not necessary to memorize all of the possible parameters of the chage command because, in the real world, I would take a quick look at the man page to find the parameter that I need. The exams force you to memorize many parameters in a short time, and to be honest, you will probably forget about them very quickly anyway.</p>
<p>But overall, I am pleased and grateful to my employer that I was able to obtain this certification, and it has made me hungry for more, and I am very eager to continue my learning in this domain.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>My First Contribution to Open Source</title>
      <link>https://mischavandenburg.com/my-first-contribution-to-open-source/</link>
      <pubDate>Fri, 18 Feb 2022 16:21:12 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/my-first-contribution-to-open-source/</guid>
      <description>Two months ago I knew nothing about GitHub. This week my first pull request got merged into master!
Programming tutorials and books very often suggest that you should try to contribute to open source in order to practice your skills. Even though I am still on the beginner level in Python, I managed to find something I could contribute with. But there were a few things I needed to learn in order to be able to do so.</description>
      <content:encoded><![CDATA[<p>Two months ago I knew nothing about GitHub. This week my first pull request got merged into master!</p>
<p>Programming tutorials and books very often suggest that you should try to contribute to open source in order to practice your skills. Even though I am still on the beginner level in Python, I managed to find something I could contribute with. But there were a few things I needed to learn in order to be able to do so.</p>
<p>GitHub is a place where many open source projects are hosted. Projects are hosted in “repositories” available to the public. Everyone can go in and take a look at the code. And the great thing about it is that everyone can contribute to the code as well.</p>
<p>Two months ago I knew almost nothing about GitHub. Surely, I had often downloaded software from GitHub, and I knew it had to do with version control. But I had no idea that it was such a powerful system of enabling collaboration for software projects.</p>
<p>During an assignment in my DevOps Traineeship I spent some time learning about Github and the Git language. I learned about repositories, branches, commits and pull requests. Now I wanted to take it to the next level and make a contribution of my own somewhere.</p>
<h3 id="the-project">the project</h3>
<p>As I have mentioned in other posts, I love game automation, and recently I discovered the Botty project, which is a bot written for the game Diablo 2: Resurrected. The bot is written in Python, which means that it is a great way of applying my Python learning to something I am passionate about.</p>
<p>The bot uses computer vision in order to recognise what is on the screen and run scripts accordingly. The monsters in the game drop items, and if you want the bot to pick up items, it will need to be taught which items it needs to pick up.</p>
<p>This is done by adding some images to its image database and adding the filenames to a list of items. When the bot scans the screen for items, it will look for a match in its image database, and when it matches, it will click the corresponding pixels on the screen to pick up the item.</p>
<p>Here’s what an image in the database looks like:</p>
<p><img loading="lazy" src="/axe.png" type="" alt=""  /></p>
<p>I am doing a Holy Grail project in this game, which means that I am collecting every item in the game. It is quite an undertaking as there are 506 items in the game, and some items have a drop chance of 1 : 1.000.000. Luckily I have a bot to help me with this project.</p>
<p>Not surprisingly, many items were still missing from the bot because it is a fairly new project that is still in development. And as I needed my bot to pick up the items I needed, I decided to add these 46 missing items to the database.</p>
<h3 id="forks-commits-and-pull-requests">forks, commits and pull requests</h3>
<p>After doing the work I still needed to figure out how I should offer these items to the project. Luckily someone shared a few very helpful tutorials in the project’s discord. This is the tutorial I used for my first contribution:</p>
<p><a href="https://www.dataschool.io/how-to-contribute-on-github/">Step-by-step Guide to Contributing on Github</a></p>
<p>You begin with “forking” the project repository, which basically means making your own copy of all the code in the project. Then you add your contribution to the fork by cloning it to your local machine and making your changes to a new branch.</p>
<p>When you have committed your changes and pushed your new branch to your fork, you are ready to make your first pull request. A pull request is a way of telling the project that you have something to contribute. You are sharing your version of the project repository including your proposed changes, and someone from the project will take a look at your suggestions and see if they are useful and compatible.</p>
<h3 id="merged-into-master">merged into master</h3>
<p>After a few days someone had a look at my contribution and requested me to make a few small adjustments. When I managed to incorporate those my contribution was accepted, and my changes were “merged into master”, which means that my contribution was brought into the main version of the project’s code.</p>
<p><img loading="lazy" src="/merged.png" type="" alt=""  /></p>
<h3 id="i-learned-a-lot">I learned a lot!</h3>
<p>As I am typing out this article, I am very satisfied with how much I have learned in the past few months already. I remember being very confused about all the GitHub terminology when I attended my first meetings during my DevOps traineeship.</p>
<p>Going through the process of making a contribution to open source on GitHub has been an enriching experience. It seemed quite intimidating at the start, but by following a good tutorial I managed to successfully submit my first pull request. I feel I have a much better understanding of Git, GitHub and the workflow.</p>
<p>Another valuable lesson I learned is that you don’t need to be a Senior Engineer in order to be able to contribute to open source. Although this project is written in Python, my contribution had very little to do with code, but I provided assets which were required by the code. So if you are a beginner at programming, you can look for other ways to contribute, such as fixing spelling mistakes in the documentation, providing images or writing wiki pages.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>My Mirst Useful Python Script</title>
      <link>https://mischavandenburg.com/my-first-useful-python-script/</link>
      <pubDate>Sat, 05 Feb 2022 05:49:06 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/my-first-useful-python-script/</guid>
      <description>The best part of learning Python is trying to identify things in my life which I can automate by writing a script. Learning a programming language involves doing a lot of exercises that sometimes lack a connection with the real world. But after I decided to go for it, I am always on the lookout for projects. Not only for my job as a DevOps Engineer, but also for my private life.</description>
      <content:encoded><![CDATA[<p>The best part of learning Python is trying to identify things in my life which I can automate by writing a script. Learning a programming language involves doing a lot of exercises that sometimes lack a connection with the real world. But after I decided to go for it, I am always on the lookout for projects. Not only for my job as a DevOps Engineer, but also for my private life. In this case, I needed to write a program that parses log files from a bot so I could get a total number of runs. You can have a look at the final result in my Diablo 2 <a href="https://github.com/mischavandenburg/diablo2">GitHub repo. </a></p>
<p>Like I wrote in my <a href="https://mischavandenburg.com/my-journey-into-devops-so-far/">journey into DevOps article</a>, I love automating games. Diablo 2 is a game that was originally released in 2002 and which recently was remastered. Diablo 2 always had a very strong presence of bots in the online game, and it didn’t take long before I also joined the ride.</p>
<p>A few months after the remaster the first bots have started emerging as well. There is a a particularly good one written in Python which is an <a href="https://github.com/aeon0/botty">open source project</a>, which is a perfect opportunity for me to learn more about Python by trying to understand its code and solving problems. I was very excited to discover it because I was playing quite a few hours a week. Diablo 2 is a very grindy game and it takes a lot of time to find the needed gear. Now I could finally outsource my grinding to the computer again.</p>
<h2 id="the-problem">the problem</h2>
<p>Although the bot is very functional and does several tasks very well, there are still features missing because it is relatively early in its development. One of these features is keeping track of the total amount of runs that the bot has done. In Diablo, every time you play you start a “game” or an instance. In that game there are certain bosses you can kill, and when you are finished you exit your game. This is called a run. Then you create a new game and everything is reset, and you get another shot at killing the bosses to get the precious gear.</p>
<p>Being the nerd that I am, I like to keep track of the total amount of runs that the bot has done. On these numbers I like to apply some calculations to see how many items I get per xxxx runs and suchlike. The bot keeps track of the amount of runs it does per session and stores them in a log file. But there is no functionality of seeing the total amount of runs you have done, and when I discovered this, I realised I had my first little Python project.</p>
<h2 id="log-files">log files</h2>
<p>Every time you close the bot after a session, a log file is created that looks like this:</p>
<p><img loading="lazy" src="/logfile.png" type="" alt=""  /></p>
<p>It is formatted as a .txt file and shows information about the bosses that were run and the items that were found. Most importantly, it contains the amount of games that were done in the session. Even after only using the bot for a short time I had over 100 log files to go through, and that’s where I needed a script that would go through these files for me and add the numbers in order to get the total amount of runs.</p>
<h2 id="the-script">the script</h2>
<p>After completing chapter 9 and 10 in the Automate the Boring Stuff book, I learned about file paths and opening files and reading from them. Now it was time to apply that knowledge. The process went pretty well and soon I had a script that would open the files for me.</p>
<p>Things got a little bit more complicated when I needed to read information from the files. And of course, this operation would almost certainly involve the dreaded topic of regex. In the end it was not as bad, and I ended up with the following regex:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">regex_games</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s2">&#34;Games:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">regex_dict</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="s1">&#39;nihla&#39;</span><span class="p">:</span> <span class="s1">&#39;Nihl|Nihlatak&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="s1">&#39;pindle&#39;</span><span class="p">:</span> <span class="s1">&#39;Pin|Pindle&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="s1">&#39;eldritch&#39;</span><span class="p">:</span> <span class="s1">&#39;Eld&#39;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>As you will see afterwards, I needed a way to check every line for a certain statement. However, rather than hardcoding every operation, I wanted it to loop over a list of terms. This meant that I could easily go back to the code and add a few more search statements if I needed them. I ended up storing them in a dictionary as you can see above. I really like the way you can make dictionaries in Python and have every entry on a new line. It makes the code very readable and structured.</p>
<p>And this is the actual looping sequence that I ended up with:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">for</span> <span class="n">folder_name</span><span class="p">,</span> <span class="n">sub_folder</span><span class="p">,</span> <span class="n">file_names</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">walk</span><span class="p">(</span><span class="n">source</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">file_names</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">PurePath</span><span class="p">(</span><span class="n">folder_name</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s1">&#39;rt&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">my_file</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># search for games number line</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">my_file</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># find number of games and add to total games</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">regex_games</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">line</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">g</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                    <span class="n">total_games</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">g</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">f</span> <span class="o">=</span> <span class="n">my_file</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="c1"># check which runs were done by using the regex dict</span>
</span></span><span class="line"><span class="cl">                    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">regex_dict</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                        <span class="n">location</span> <span class="o">=</span> <span class="n">regex_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                        <span class="n">reg</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">location</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="c1"># if there is a match, add the numbers to the total variable</span>
</span></span><span class="line"><span class="cl">                        <span class="k">if</span> <span class="n">reg</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                            <span class="n">var_name</span> <span class="o">=</span> <span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;_total&#39;</span>
</span></span><span class="line"><span class="cl">                            <span class="nb">globals</span><span class="p">()[</span><span class="n">var_name</span><span class="p">]</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">g</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span></code></pre></div><p>This sequence loops through the folder, the subfolder, and opens each file one by one. When the file is opened it looks for the “Games: 25” line and adds the number to a variable. However, I was not only interested in the total number of games. I also wanted to get more insight in how many Pindle runs or Nihla runs I had done. So I set up another regex search and made sure that the number of games are added to a “pindle_total” or “nihla_total” variable.</p>
<h2 id="result">result</h2>
<p>When running the script in the shell, the result looks like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">mischa@MischaMacBook stats_parser % python3 total_runs.py 
</span></span><span class="line"><span class="cl">Total runs: <span class="m">7159</span>
</span></span><span class="line"><span class="cl">Pindle runs: <span class="m">6926</span>
</span></span><span class="line"><span class="cl">Eldritch + Shenk runs: <span class="m">367</span>
</span></span><span class="line"><span class="cl">Nihla runs: <span class="m">232</span>
</span></span><span class="line"><span class="cl">mischa@MischaMacBook stats_parser % 
</span></span></code></pre></div><p>Exactly what I wanted. Now I can just paste my stats files into a folder and see how many runs I’ve done. Maybe I’ll improve it by building a GUI. Another fun idea I have is to create a little pipeline where this script would be run once an hour and the stats would be uploaded to a webpage somewhere, so others could see the amount of runs of my bot. Not that anyone is interested in that, but it is a fun project for me to do. Let’s see what happens!</p>
<p>For now I am very happy with the result. It was a very satisfying experience to identify a problem that I had and to be able to come up with an automated solution. Of course it is still very rudimentary programming, and there is a long long way ahead of me, but it was fun to finally do something practical that solved a particular problem in my life.</p>
<p>The final result is in my <a href="https://github.com/mischavandenburg/diablo2">Diablo 2 GitHub repo.</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Python Project: Mad Libs</title>
      <link>https://mischavandenburg.com/python-project-mad-libs/</link>
      <pubDate>Tue, 01 Feb 2022 21:41:28 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/python-project-mad-libs/</guid>
      <description>I am currently working through the book Automate the Boring Stuff by Al Sweigart . I can already highly recommend it to anybody who is learning Python.
Chapter 9 is about reading and writing files, and there are two assignments at the end of the chapter. Here I’ll discuss my solution of the Mad Libs assignment.
here is the full assignment text: Mad Libs Create a Mad Libs program that reads in text files and lets the user add their own text anywhere the word ADJECTIVE, NOUN, ADVERB, or VERB appears in the text file.</description>
      <content:encoded><![CDATA[<p>I am currently working through the book <a href="https://automatetheboringstuff.com">Automate the Boring Stuff</a> by Al Sweigart . I can already highly recommend it to anybody who is learning Python.</p>
<p>Chapter 9 is about reading and writing files, and there are two assignments at the end of the chapter. Here I’ll discuss my solution of the Mad Libs assignment.</p>
<h3 id="here-is-the-full-assignment-text">here is the full assignment text:</h3>
<pre tabindex="0"><code>Mad Libs
Create a Mad Libs program that reads in text files and lets the user add their own text anywhere the word ADJECTIVE, NOUN, ADVERB, or VERB appears in the text file. For example, a text file may look like this:
The ADJECTIVE panda walked to the NOUN and then VERB. A nearby NOUN was
unaffected by these events.

The program would find these occurrences and prompt the user to replace them.
Enter an adjective:
silly
Enter a noun:
chandelier
Enter a verb:
screamed
Enter a noun:
pickup truck

The following text file would then be created:
The silly panda walked to the chandelier and then screamed. A nearby pickup
truck was unaffected by these events.

The results should be printed to the screen and saved to a new text file.
</code></pre><p>Looks pretty simple, right? I went into it with a lot of zeal and started writing a long list of if statements. My first attempts at the solution involved matching the words NOUN and ADJECTIVE directly, like so:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="n">word</span> <span class="o">==</span> <span class="s1">&#39;ADJECTIVE&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">inv</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s1">&#39;Enter an adjective: &#39;</span><span class="p">)</span>
</span></span></code></pre></div><p>However, this is problematic because as you can see, the sentence can contain words with a period attached, such as “VERB.” in the above example.</p>
<h2 id="no-please-no-regex">no, please no regex!</h2>
<p>I’ve understood that here is a general anxiety around regex. I have certainly noticed it in myself and some of my junior engineer friends. As soon as I read the word regex, or realise that an assignment is going to involve regex, I get a constricting feeling in my throat and a rise in my heart rate.</p>
<p>I’ve had to struggle with it quite a bit during my freeCodeCamp Front End Development Certificate, and the memories are still fresh in my mind.</p>
<p>So, like any other ‘rational’ human being, I tried to approach this relatively simple assignment with all sorts of ways trying to account for a period ending the word:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">period_check</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">letter</span> <span class="ow">in</span> <span class="n">x</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">letter</span> <span class="o">==</span> <span class="s2">&#34;.&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># loop over the array and prompt user</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">source_text</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">period</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">period_check</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">word</span> <span class="o">=</span> <span class="n">word</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&#34;.&#34;</span><span class="p">,</span> <span class="s2">&#34;&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">period</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">word</span> <span class="o">==</span> <span class="s1">&#39;ADJECTIVE&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">invoer</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s1">&#39;Enter an adjective: &#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">period</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">result_text</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">invoer</span> <span class="o">+</span> <span class="s2">&#34;.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">period</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">result_text</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">invoer</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="n">word</span> <span class="o">==</span> <span class="s1">&#39;NOUN&#39;</span><span class="p">:</span>
</span></span></code></pre></div><p>It’s quite funny to see the lengths we go through to avoid regular expressions. However, as you maybe deduce from the code above, it didn’t work, and after a couple of hours of fiddling I gave up, and like any other frustrated programmer, I started to google.</p>
<p>I quickly found solutions to this assignment and they all involved regex, and I realised I could not walk away from my fears anymore.</p>
<h2 id="an-elegant-solution">an elegant solution</h2>
<p>Eventually I ended up with the following result for the part of my assignment that needed to recognise and replace the words with the user input. Of course I heavily borrowed from my Google search results.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># set up and match the regex</span>
</span></span><span class="line"><span class="cl"><span class="n">grammar_regex</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;ADJECTIVE|NOUN|VERB|ADVERB&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">match_regex</span> <span class="o">=</span> <span class="n">grammar_regex</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">source_text</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># replace the matches with user input </span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">match_regex</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">ask_user</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s1">&#39;Please enter &#39;</span> <span class="o">+</span> <span class="n">i</span> <span class="o">+</span> <span class="s1">&#39;: &#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">source_text</span> <span class="o">=</span> <span class="n">source_text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">ask_user</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span></code></pre></div><p>When I say elegant, I mean elegant in total Python beginner terms. I’m sure there are enough Seniors out there who will burst out laughing when they see this. But to me, it was quite an eye-opening experience to see this little piece of code do exactly what I had intended to achieve with 3 different functions and long blocks of if statements.</p>
<p>Also, I was pleasantly surprised with how simple regex can be in Python. In this case there were no scary [Az ^**/!!${}aa{}aA{nF}] statements. We simply defined which words we wanted and called the findall() module to generate a list with all the matches.</p>
<p>Then we iterate over the list of matches and for each match we ask the user for the desired word, and replace it in the source_text.</p>
<h2 id="final-result">final result</h2>
<p>Having sorted out the pattern matching and replacing part, it was only a matter of implementing reading from files and writing to a new file.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Automate the Boring Stuff chapter 9</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Mad Libs assignment</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Mischa van den Burg</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">re</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># ask the user which file to open</span>
</span></span><span class="line"><span class="cl"><span class="n">file_name</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s1">&#39;Enter the filename. For example, grammar.txt: &#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># my script and .txt file are located in ~/python/automatetheboringstuff/ </span>
</span></span><span class="line"><span class="cl"><span class="n">text_file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">Path</span><span class="o">.</span><span class="n">home</span><span class="p">()</span> <span class="o">/</span> <span class="s1">&#39;python&#39;</span> <span class="o">/</span> <span class="s1">&#39;automatetheboringstuff&#39;</span> <span class="o">/</span> <span class="n">file_name</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># read the file and store in variable &amp; close</span>
</span></span><span class="line"><span class="cl"><span class="n">source_text</span> <span class="o">=</span> <span class="n">text_file</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">text_file</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># set up and match the regex</span>
</span></span><span class="line"><span class="cl"><span class="n">grammar_regex</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;ADJECTIVE|NOUN|VERB|ADVERB&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">match_regex</span> <span class="o">=</span> <span class="n">grammar_regex</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">source_text</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># replace the matches with user input </span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">match_regex</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">ask_user</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s1">&#39;Enter &#39;</span> <span class="o">+</span> <span class="n">i</span> <span class="o">+</span> <span class="s1">&#39;: &#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">source_text</span> <span class="o">=</span> <span class="n">source_text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">ask_user</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># write to the new file and print the result</span>
</span></span><span class="line"><span class="cl"><span class="n">new_file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;new_&#39;</span> <span class="o">+</span> <span class="n">file_name</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">new_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">source_text</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">new_file</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">source_text</span><span class="p">)</span>
</span></span></code></pre></div><p>I was getting into some better functionality, such as accounting for existing filenames, and making the pathing relative so it could be run from anywhere. But I decided to save that for a later assignment.</p>
<p>The assignment was clear and did not require such functionality. I need to learn to keep things simple, and I decided to do just what I was asked and not go into any other rabbit holes.</p>
<h1 id="lessons-learned">Lessons Learned</h1>
<p>All in all the assignment is pretty simple, but I learned surprisingly much from it. I decided I’ll need to change and learn to love regex rather than fear it, because it showed me how powerful it can be.</p>
<p>Also, I got some insight into my own mind and how I tend to work. I realised I have a tendency to make things much more complicated than they need to be. I need to learn to keep things simple.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Docker LEMP Stack deployed with Ansible</title>
      <link>https://mischavandenburg.com/docker-lemp-stack-deployed-with-ansible/</link>
      <pubDate>Sun, 30 Jan 2022 16:20:24 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/docker-lemp-stack-deployed-with-ansible/</guid>
      <description>In order to learn more about Docker and Ansible I am working on an assignment to take an existing application and to break it down into containers. However, in order to be able to understand this process properly, I first needed to understand more about Docker and containerisation.
I wrote a playbook that installs Docker and deploys a fully containerised LEMP stack on a virtual machine.
You can have a look at the Github repo with the result here.</description>
      <content:encoded><![CDATA[<p>In order to learn more about Docker and Ansible I am working on an assignment to take an existing application and to break it down into containers. However, in order to be able to understand this process properly, I first needed to understand more about Docker and containerisation.</p>
<p>I wrote a playbook that installs Docker and deploys a fully containerised LEMP stack on a virtual machine.</p>
<p><a href="https://github.com/mischavandenburg/lemp_docker_ansible">You can have a look at the Github repo with the result here. </a></p>
<p>The repo is using the “ansible-galaxy init” role structure. You will find the playbooks as follows: roles/your_choice/tasks/main.yml</p>
<h2 id="docker">Docker</h2>
<p>I was very excited to learn more about Docker and containerisation. I was familiar with the concept of virtualisation, which is creating virtual versions of fully functional machines on a host operating system. But the concept of containerisation was new to me.</p>
<p>As I understand it, containerisation differs drastically from virtualisation because containers are able to use resources from host directly. They do not need an entire operating system to run, and therefore they are a much more lightweight.</p>
<p>This means that resources can be used much more efficiently which eventually can mean cost reduction in your cloud infrastructure.</p>
<p>Docker is a very popular platform for building and running containers. It seemed like the best option to get started with deploying my own containers.</p>
<h2 id="lemp-stack">LEMP Stack</h2>
<p>My colleague recommended me <a href="https://tech.osteel.me/posts/docker-for-local-web-development-part-1-a-basic-lemp-stack">this tutorial</a> to become more familiar with Docker. It uses a LEMP stack as an example application. When I told friends about the fact that I was building a LEMP stack, they corrected me and said it was a LAMP stack.</p>
<p>The LAMP stack is a collection of software built out of these elements:</p>
<p>L – Linux: the operating system</p>
<p>A – Apace: webserver</p>
<p>M – MySQL: database</p>
<p>P – PHP: server scripting language</p>
<p>However, in a LEMP stack, we use NGINX as a webserver, which is pronounced “Engine X”, hence the E in LEMP stack. Therefore, LEMP is the correct way to spell it, and it is used in all the tutorials that I have been using.</p>
<p>I highly recommend <a href="https://tech.osteel.me/posts/docker-for-local-web-development-part-1-a-basic-lemp-stack">the tutorial</a> in order to learn how to deploy your first collection of containers. Deploying one container is relatively easy with Docker, but it gets a little more complicated when deploying several containers and making them communicate with each other in order to combine them into one application. But this tutorial does a great job at showing you how it’s done and it is especially good at explaining the steps along the way.</p>
<h2 id="docker-compose-vs-ansible">Docker Compose vs. Ansible</h2>
<p>Docker Compose is a tool you can use to run multi-container applications. With the help of the tutorial, it was fairly easy to understand and hit the ground running by deploying multiple containers into one network.</p>
<p>Now let’s have a look at how we actually set up the containers. In the Docker Compose file, the NGINX container was defined like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;3.8&#39;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="c">#Services</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">services</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="c">#Nginx Service</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">nginx</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">nginx:1.19</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="m">80</span><span class="p">:</span><span class="m">80</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="l">./src:/var/www/php</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="l">./.docker/nginx/conf.d:/etc/nginx/conf.d</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">depends_on</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="l">php</span><span class="w">
</span></span></span></code></pre></div><p>It looks pretty straightforward, right? Almost like pseudocode. We tell Docker which image to pull from the Docker Hub, and we tell it to route container’s port 80 to our host’s port 80. This ensures that the web server can be accessed from the outside, provided you have opened this port in the firewall.</p>
<p>Next there is the volumes section: this mounts certain directories on the host into the container so it is accessible. In this case this was necessary to transfer the web server configuration and the index.php which we wanted to serve to the outside.</p>
<p>Having successfully deployed my LEMP using Docker Compose, the next step was to automate this process by using Ansible. Ansible is a very powerful tool which enables you to automate configuration management and application deployment by writing scripts called playbooks.</p>
<h5 id="why-was-it-necessary-to-introduce-ansible">Why was it necessary to introduce Ansible?</h5>
<p>By using Docker Compose, you would need to have Docker and Docker Compose installed on the virtual machine before you could start running the containers.</p>
<p>However, Ansible gives you the power to take a completely fresh virtual machine, configure it from scratch, and install Docker and its necessary dependencies, followed by deploying the containers.</p>
<h4 id="now-lets-take-a-look-at-the-same-container-defined-in-ansible">Now let’s take a look at the same container defined in Ansible:</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl">- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">start nginx </span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">docker_container</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">nginx:1.19</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">detach</span><span class="p">:</span><span class="w"> </span><span class="kc">yes</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="m">80</span><span class="p">:</span><span class="m">80</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">networks</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">network_one</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="l">/src:/var/www/php</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="l">/.docker/nginx/conf.d:/etc/nginx/conf.d</span><span class="w">
</span></span></span></code></pre></div><p>Although there are some differences, they look very similar. Converting my Docker Compose file to an Ansible playbook was quite a natural and easy experience. It also helps that both are written in YAML and therefore use the same indentation conventions.</p>
<p>A few differences we can observe:</p>
<p>In the Ansible playbook we invoke the docker_container module, whereas they are defined as services in the Docker Compose file. Another difference is that we need to set up the network ourselves. In the Docker Compose file, we just specified the containers and Docker Compose created a network automatically and made sure that all containers were connected to it.</p>
<p>However, it isn’t very complicated in Ansible either:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl">- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">setup network</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">docker_network</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">network_one</span><span class="w">
</span></span></span></code></pre></div><p>We simply call the docker_network module and tell it to make a network called network_one. All we need to do then is make sure to set the networks: parameter to network_one in the docker_container module as we saw above.</p>
<p>The last point to note is the detach parameter. This means that the container will keep running in the background after it is started.</p>
<h3 id="result">Result</h3>
<p>After some debugging here and there and making sure all of the elements were in place, eventually we get the satisfying message that everything went according to plan:</p>
<p><img loading="lazy" src="/success.png" type="" alt="Successful play from Ansible"  /></p>
<p>The result is a webpage being served on the server ip:</p>
<p><img loading="lazy" src="/webpage.png" type="" alt="The final web page served by Nginx "  /></p>
<p>I know, it is not the prettiest or most intricate design. But remember that I am working towards becoming a DevOps Engineer, not a Front End Developer 😉</p>
<p>We can also enter the phpMyAdmin dashboard by adding port 8080 to our ip in the browser:</p>
<p><img loading="lazy" src="/php.png" type="" alt="The PHP page."  /></p>
<h2 id="conclusion">Conclusion</h2>
<p>The assignment of deploying a LEMP stack in separate containers has been very useful and I learned a lot from the process. There were a few more modules that needed to be configured in Ansible as opposed to the Docker Compose method, but the tradeoff is that Ansible is much more powerful and enables you to configure the server from scratch. You can have a look at the code in the GitHub repo to see all of the changes I needed to do.</p>
<p>The only part that I needed to do by hand is to create the VM in the Microsoft Azure portal, open the ports and configure the SSH keys. The next step in my learning process will be to learn how I can automate this step as well. This means that I will need to learn Terraform.</p>
<p>By using Terraform I will be truly deploying this stack as Infrastructure as Code, but doing all of these steps with Ansible has given me a much better understanding of Infrastructure as Code already.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Why I Chose a Bear as a Logo</title>
      <link>https://mischavandenburg.com/why-i-chose-a-bear-for-my-logo/</link>
      <pubDate>Sun, 30 Jan 2022 11:53:21 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/why-i-chose-a-bear-for-my-logo/</guid>
      <description>I always struggled with finding an online nickname for myself. I was never given one, and over the years I used a few here and there, ranging from Nordic gods to my favourite fantasy character: Gandalf the Grey.
I intended to create a tech blog to document my journey ever since I started my career change. But for months I was stopped by trying to decide on a name. (That’s me in a nutshell), because it needed to be perfect.</description>
      <content:encoded><![CDATA[<p>I always struggled with finding an online nickname for myself. I was never given one, and over the years I used a few here and there, ranging from Nordic gods to my favourite fantasy character: Gandalf the Grey.</p>
<p>I intended to create a tech blog to document my journey ever since I started my career change. But for months I was stopped by trying to decide on a name. (That’s me in a nutshell), because it needed to be perfect. Eventually I decided to just use my full name.</p>
<p>Although I have zero connections with Russia, neither genetically nor culturally, my parents decided to give me a name of Russian origin: Mischa.</p>
<p>According to <a href="https://babynames.com/name/mischa">this website</a>, Mischa has the following meaning:</p>
<p>“The name Mischa is primarily a gender-neutral name of Russian origin that means <em>Who Is Like God</em>“</p>
<p>Although I admire my parents for giving me such an ambitious name, I must confess that I turned out to be of a much more earthy and less godly nature.</p>
<p>When I was younger, an old gymnastics teacher once told me that Mischa meant “bear” in Russian, and I always liked that connotation much more. I also turned out to be more bear-like than god-like, being 190cm tall and having thick and bristly curly hair.</p>
<p>Indeed, <a href="https://www.quora.com/What-does-Misha-mean-Its-Russian-by-the-way">this post on Quora</a> confirms that my name indeed has the meaning of bear:</p>
<blockquote>
<p><em>&ldquo;In Russia Mishka sometimes also used to denote a bear, particularly a bear cub.&rdquo;</em>
<a href="https://www.quora.com/profile/Misha-Sivan">Misha Sivan</a>, Born in USSR.</p>
</blockquote>
<p>
  <img loading="lazy" src="https://upload.wikimedia.org/wikipedia/commons/a/a9/GrizzlyBearJeanBeaufort.jpg" alt=""  /></p>
<p>Although I am conveniently ignoring the “cub” part of his explanation, I thought it was pretty cool that my actual name had connotations with our ursine friends.</p>
<p>Moreover, I lived 9 years in Norway where I roamed the mountains for weeks at a time, just me and my tent and a fishing rod. I developed a very close connection with nature during those years. Not only its beauty, but also its merciless forces and awe-inspiring ingenuity.</p>
<p>Although I never encountered a bear myself, they are most definitely present in the Norwegian nature. One time the newspapers told me that there was a bear sighted 4 kilometers from where I was camping one weekend.</p>
<p>But this is what the bear represents to me: the time I was allowed to spend in the North and the resulting connection with nature. The solitary character of the male, calmly roaming for many miles in search of sustenance. A force to be reckoned with when aggravated, but in general preferring to stay at a distance.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>My Journey Into DevOps So Far</title>
      <link>https://mischavandenburg.com/my-journey-into-devops-so-far/</link>
      <pubDate>Fri, 28 Jan 2022 08:56:38 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/my-journey-into-devops-so-far/</guid>
      <description>In 2021 I had reached a certain stage in my life where I had the liberty to make a choice. I was 31 years old and had just finished an important chapter of my life, and was ready to begin a new one. After thinking about where I wanted to steer my professional career, I decided to follow my passion for programming and automation and attempt to make a career out of my interests in tech and IT.</description>
      <content:encoded><![CDATA[<p>In 2021 I had reached a certain stage in my life where I had the liberty to make a choice. I was 31 years old and had just finished an important chapter of my life, and was ready to begin a new one. After thinking about where I wanted to steer my professional career, I decided to follow my passion for programming and automation and attempt to make a career out of my interests in tech and IT.</p>
<p>I was in a fortunate situation, because there were large shortages of people on the IT job market in The Netherlands. Consequently, employers were much more willing to train their employees to perform the roles that they required to fulfill.</p>
<h2 id="bots-and-scripts">Bots and Scripts</h2>
<p>I have always loved messing around with computers, and I learned to write small and simple programs at a very young age. At the same time I was an avid gamer, and spent many hours slaying monsters in online RPG’s. One day I came across the concept of a bot: a program that plays the computer game for you, and I was hooked. This is where I developed my ‘fetish’ for automation.</p>
<p>I was very lucky that I had a friend who shared my interests, and together we built our own automation projects (called ‘botfarms’) in which we ran large amounts of bots that played a certain game for us. This army of bots generated in-game currency which we could sell for actual money. They weren’t huge profits, but it was an amazingly satisfying feeling to be the overlords of an army of automations that actually generated some income for us.</p>
<h2 id="from-bots-to-devops">From Bots to DevOps</h2>
<p>These botfarms were hosted on servers which set up ourselves. In order to save costs we rented Linux servers, and I spent many evenings figuring out how configure them via the command line. Often I would suddenly snap out of my flow at 3am and realise I had to go to work at 7 in the morning again.</p>
<p>Although I did not manage to make these personal interests into a personal career, my friend eventually became a Data Engineer. After making the decision to make a career switch to IT I needed to figure out which direction I wanted to go in, because IT covers a very broad range of topics and skills. Based on on our shared interests and previous projects, he recommended me to become a DevOps Engineer.</p>
<p>I very quickly realised that he was right on the money with his suggestion and I started to become very enthusiastic to learn how to make a living by working with computers and automation.</p>
<h2 id="traineeship">Traineeship</h2>
<p>As I mentioned before, employers in The Netherlands are now willing to train potential candidates, and I used my hobby projects as a way to demonstrate my genuine interest and affinity with IT and automation. I was offered a traineeship to become a DevOps Engineer in 2021. After a period of training I started working for the City of Amsterdam and I&rsquo;ve been part of an IT4IT operations team since.</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
