[{"content":"It\u0026rsquo;s now possible to abort long running operations on AKS clusters. It was released as Generally Available.\nFor example:\naz aks operation-abort --name myAKSCluster --resource-group myResourceGroup\nLinks: 202304270704\nhttps://azure.microsoft.com/en-us/updates/generally-available-operation-abort-in-aks/\nhttps://learn.microsoft.com/en-us/azure/aks/manage-abort-operations?tabs=azure-cli\n","permalink":"https://mischavandenburg.com/zet/aks-abort-operation/","summary":"It\u0026rsquo;s now possible to abort long running operations on AKS clusters. It was released as Generally Available.\nFor example:\naz aks operation-abort --name myAKSCluster --resource-group myResourceGroup\nLinks: 202304270704\nhttps://azure.microsoft.com/en-us/updates/generally-available-operation-abort-in-aks/\nhttps://learn.microsoft.com/en-us/azure/aks/manage-abort-operations?tabs=azure-cli","title":"You Can Abort Operations on AKS Clusters Now"},{"content":"I started running on the 13th of February 2023. I ran 1.2 kilometres in 9 minutes. Slowly I built it up to my first 5k which I ran on the 23th of March 2023.\nAfter I discovered a new potential within me now that I lost so much weight, I felt I needed some more structure towards my running training. Reddit, as always, comes to the rescue. On the r/running subreddit I discovered base training.\nI focused completely on the distance ran while slowly working my way up to 5 kilometres. Base training, however, made me focus on the duration of the runs. This was quite an eye opener and changed my perspective.\nThe idea is that the duration is much more important for beginners because they experience more stress on the body while running. An experienced runner might finish 10 kilometres in 30 minutes, but a beginner will experience a lot more stress on the body doing 30 minutes of running on an easy pace because there are more foot strikes occurring and because the body is unadapted. The beginner first needs to build up strength in the legs and resistance in the joints and tissues.\nSo far it has been a good experience. I like the idea of running more often, but not pushing myself every single run. The schedule also allows for variation. Some days are easier than others, so it feels good to have the opportunity to go for only a short 30 minute run on the days where I\u0026rsquo;m not feeling up to it.\nHere are the results of my first week:\nWeek 16 - 1 Easy Run 1 Mon 30min Easy Run 2 Wed 43min Easy Run 3 Thu 36,5min Easy Run 4 Sun 43min Long Run Fri 50min Total distance: 20.59km Goal I currently don\u0026rsquo;t have a particular goal in mind. I have a knee injury which started to flare up again when I started running. So far the body has adapted, and the knee stopped hurting. I\u0026rsquo;m going to follow this program for 6 weeks, adding 5 minutes (or more if I feel good) per week to my long run, and see where this takes me.\nLinks: 202304241704\nhttps://www.reddit.com/r/running/comments/3bckeh/base_training_a_guide_to_your_foundation_to/\n","permalink":"https://mischavandenburg.com/zet/base-training-running/","summary":"I started running on the 13th of February 2023. I ran 1.2 kilometres in 9 minutes. Slowly I built it up to my first 5k which I ran on the 23th of March 2023.\nAfter I discovered a new potential within me now that I lost so much weight, I felt I needed some more structure towards my running training. Reddit, as always, comes to the rescue. On the r/running subreddit I discovered base training.","title":"Base Training For Running: Time Instead Of Distance"},{"content":"Over the past two months I\u0026rsquo;ve increased the amount of exercise quite a lot. Over the past year or so I\u0026rsquo;ve been doing Brazilian Jiu Jitsu 1-2x a week and going for a 30-90 minute walk every day.\nAfter I started running in February I have quit BJJ and changed to running 5-6 times a week, 30-60 minutes of Ashtangha Yoga almost every day and two calisthenics workouts a week. For my calisthenics I use rings and I do squats in the gym .\nIn addition, I reduced my calories to 1700 calories a day spread over two meals, doing 20 hours of fasting a day. I have also eliminated all sugar from my diet.\nThe reason I needed to do this is because my weight loss has plateaued for several months and I have at least 3kg more fat to lose until I reach my goal of 10% body fat.\nMost people know me as someone who is very productive and frugal with his time, and that\u0026rsquo;s true. I\u0026rsquo;m always studying, writing, creating or learning in my free time. There is little to no time for girls and video games, I\u0026rsquo;m on my purpose.\nYet I am also aware of the need of rest and relaxation. I haven\u0026rsquo;t written much about it on my blog, and this will probably change in the time to come, but I am an established meditator and have a very solid mindfulness practice. I\u0026rsquo;ll notice very quickly when I get out of balance and fortunately my practice has provided me with the wisdom to know what to do to bring things back into balance again.\nHaving introduced an increased amount of exercise and mental stress due to caloric restriction, I saw I needed to take a few steps back and allow my body and mind to adjust to these changes. I was already on caloric restriction and fasting 18 hours a day, but still I noticed I needed time to adjust.\nTo adjust I allowed myself to take a break from evening tech study and homelabbing and read some fiction instead, and I increased my meditation practice. Stilling the mind with breath meditation is helping me to deal with very intense food cravings. Quitting sugar is definitely no walk in the park.\nI have made these kinds of changes many times before, and I know that after a while my body and mind are used to it and I can go back to my old routine again. Having a daily meditation practice gives me the tools to check in with myself. I am very grateful to myself that I put in the hard work over the past 6 years to get an established daily practice. I am also extremely grateful to my teacher and the Buddhist community who supported me while I spent time in a monastery in Norway.\nLinks: 202304242004\n[[buddhist-practice]]\n[[meditation]]\n","permalink":"https://mischavandenburg.com/zet/importance-rest-changes/","summary":"Over the past two months I\u0026rsquo;ve increased the amount of exercise quite a lot. Over the past year or so I\u0026rsquo;ve been doing Brazilian Jiu Jitsu 1-2x a week and going for a 30-90 minute walk every day.\nAfter I started running in February I have quit BJJ and changed to running 5-6 times a week, 30-60 minutes of Ashtangha Yoga almost every day and two calisthenics workouts a week. For my calisthenics I use rings and I do squats in the gym .","title":"Importance Of Rest And Meditation When Making Changes To Routine"},{"content":"This week I started a project which I\u0026rsquo;ve been putting off for too long. I finally started my homelab. Over the past year I\u0026rsquo;ve been collecting hardware here and there, and I\u0026rsquo;ve had the intention to start up a proper Kubernetes cluster at home. I got inspired by Rob Muhlenstein\u0026rsquo;s Homelab Init playlist on YouTube which I\u0026rsquo;m working on.\nThere are a few reasons why I haven\u0026rsquo;t started up until now:\nFocused on switching jobs and certifications Not knowing what to run on the cluster High electricity costs Now that I found a new job with a great employer, I\u0026rsquo;ve changed my focus towards doing more hands-on learning in my free time by learning Go and diving deeper into Cloud Native technology. Energy prices have come down in the meantime as well.\nI\u0026rsquo;ve reached a stage in my Go learning journey where I\u0026rsquo;m actually able to start making small deployable applications, and I want an environment where I can do that without high costs or without fearing to break something. I want to learn more about databases on Kubernetes, and I want to start writing small microservices and API\u0026rsquo;s that are able to query these databases.\nMy lab is going to be my playground, where I can deploy whatever I want and learn the technologies that interest me at that particular moment.\nHardware: Parts For some reason the Raspberry Pi has become synonymous with homelabs. I get that it\u0026rsquo;s fun to run a cluster on something that is not much larger than a 1kg pack of sugar. But I never really caught on to that whole scene yet. Maybe it\u0026rsquo;s because I\u0026rsquo;m a bit late to the party and the Pi\u0026rsquo;s have been scarce and very expensive lately?\nIn any case, I\u0026rsquo;ve been thankfully accepting old computers that friends were going to get rid of, and I\u0026rsquo;ve been keeping some of my own old hardware as well. I have enough motherboards and other parts to assemble around 3 nodes, which will probably have around 8GB RAM each, but possibilities to attach storage.\nThis is also what has been keeping me back for a while I think. There is quite a bit of work that I need to do to get these machines going, and probably I\u0026rsquo;ll have to purchase a couple of other parts. However, I also have some functional hardware.\nGaming Desktop I have an old gaming desktop with 16GB RAM, an Intel 6700K Skylake, 1070 video card and a couple of TB of storage.\nThis has been my Arch Linux desktop for the past year, but now that I switched to my new MacBook, I\u0026rsquo;m not using it as much. I want to keep it as it is right now, but I could run a few Virtual Machines on there, and maybe consider turning it into a ProxMox server.\nOld Laptops I have two old laptops. One Asus with 4GB of RAM and a Thinkpad T430 with 8GB RAM. The Thinkpad is actually surprisingly powerful. As a weekend project I installed Arch on it and I fitted it with a refurbished keyboard, and it is a very pleasant machine to work with. However, now that I have a very powerful laptop that I use as a desktop and portable device, it has become redundant.\nOld Laptops as Raspberri Pi\u0026rsquo;s Having these two old laptops lying around, it occurred to me that these machines were basically Raspberri Pi\u0026rsquo;s with a large form factor and a higher power usage. Why would I need to spend hundreds of euros on these smaller computers if I could just use these laptops as a starting point for my lab?\nUsing laptops has the following advantages:\nNo additional costs No building needed Easy to install Linux on them Built-in screen and keyboard for quick access when SSH does not work out Built-in batteries to handle short power disruptions (rare but possible) Can get going very quickly Choices and Goals Kubernetes The first goal is to get a Kubernetes cluster running. I will do bare metal kubeadm installs, and later I want to learn more about Talos. Fortunately I feel very comfortable installing Kubernetes with kubeadm. I did plenty of practice for my CKA, and I recently installed it on free Oracle VM\u0026rsquo;s. I\u0026rsquo;ve experimented a bit with K9S earlier, but I want to learn how to maintain on-prem Kubernetes.\nMy goals is to learn to maintain production-grade clusters properly.\nLinux Naturally I\u0026rsquo;ll be using Linux as my base OS. After some consideration I chose to use Ubuntu Server 22. Some notes on that choice:\nI already have years of Ubuntu Server experience Good to keep building on what I have Working with managed Kubernetes on my day job requires me to keep Linux admin skills fresh Still the most popular Linux distro Google uses Ubuntu Server Well documented and plenty of questions on StackOverflow Infrastructure as Code \u0026amp; GitOps Initially I\u0026rsquo;ll configure the servers by hand, but I want to have the server configuration as code as Ansible playbooks eventually. However, I\u0026rsquo;ll be using ArgoCD for all of my deployments on Kubernetes itself, so the server configuration is only a very small part of the setup. Just get Kubernetes running and do the rest with ArgoCD.\nPerhaps I will expand with larger servers that run multiple VM\u0026rsquo;s. Then it will be very relevant to start provisioning these with Ansible.\nNetworking I want to learn more about networking and use static IP addresses for my servers. I need to figure out how my home network works exactly. Surprisingly, I\u0026rsquo;ve never taken the effort to actually know how the devices on my network get their IP addresses and how they communicate, even though I\u0026rsquo;ve learned plenty about it for my day job and do networking in an enterprise environment daily.\nFor Kubernetes I\u0026rsquo;ll use Flannel to start out with, but I want to learn more about Cilium, Istio and other service mesh implementations.\nAnother goal is to host my own DNS server for internal name resolution, probably CoreDNS.\nDeployment I want to host my own container registry (Harbor) and use Tekton pipelines to for CI/CD, and I\u0026rsquo;m playing with the thought to host my own GitLab instance as well.\nLet\u0026rsquo;s Go! Another realization was that I don\u0026rsquo;t need to have everything figured out before I begin. The beauty of cluster computing is that you can add to it as you go. I can start with a small cluster of two nodes and build it out as my needs grow. I don\u0026rsquo;t expect to need more than a few GB of RAM in the foreseeable future, so these two laptops will be plenty to get going.\nLinks: 2023041213\n[[homelab]]\n[[homelab-network]]\n[[linux]]\n[[homelab-ubuntu-server]]\n[[kubernetes]]\n","permalink":"https://mischavandenburg.com/zet/starting-my-homelab/","summary":"This week I started a project which I\u0026rsquo;ve been putting off for too long. I finally started my homelab. Over the past year I\u0026rsquo;ve been collecting hardware here and there, and I\u0026rsquo;ve had the intention to start up a proper Kubernetes cluster at home. I got inspired by Rob Muhlenstein\u0026rsquo;s Homelab Init playlist on YouTube which I\u0026rsquo;m working on.\nThere are a few reasons why I haven\u0026rsquo;t started up until now:","title":"Starting My Homelab"},{"content":"The Tour of Go is very clear:\nGo does not have classes.\nOne benefit of learning multiple programming languages is that each language provides you with a set of “pegs” that you can use to refer to other languages. As I learned about structs in Go, I hung them to the “Python classes” peg and used that as a reference point. Using these reference points can help you to understand the object of study by looking at their differences and similarities.\nEven though Go does not have classes, my understanding of Python classes did help me to understand structs much more quickly.\nGo Structs A struct is a collection of fields, and they are accessed using a dot.\nEach data field in a struct has its own type, either user defined or built in.\ntype Person struct { name string address string } person1 := Person{\u0026#34;Mischa\u0026#34;, \u0026#34;Amsterdam\u0026#34;} fmt.Println(person1.name) fmt.Println(person1.address) When creating a struct, you can use the Name: syntax to set the values. Otherwise, you need to populate the fields in order.\ntype Computer struct { os string ram int } func main() { macBook := Computer{os: \u0026#34;MacOs\u0026#34;} fmt.Println(macBook) thinkPad := Computer{\u0026#34;Arch Linux\u0026#34;, 16} fmt.Println(thinkPad) } This produces the output:\n{MacOs 0} {Arch Linux 16} In the first line, the ram is 0 because I only set the os field, and unset fields get a 0 value by default.\nStruct Methods Classes in Python can contain methods. What about Go structs?\nStructs can have methods, but they are not contained in the struct definition, like you would see in Python. Methods are defined on types, and the type does not need to be a struct. Therefore, methods are defined after you create the struct.\ntype Computer struct { os string ram int } func (c Computer) doubleRam() string { return fmt.Sprintf(\u0026#34;If you would double your ram, you would have %v GB of ram.\u0026#34;, c.ram * 2) } func main() { macBook := Computer{\u0026#34;MacOs\u0026#34;, 32} fmt.Println(macBook.doubleRam()) } Output:\nIf you would double your ram, you would have 64 GB of ram.\nIn this example, doubleRam() is my method of the Computer type. Methods take a special receiver argument, written between the func keyword and the method name. The doubleRam method has a receiver of type Computer, named c.\nStruct Literal A struct literal is a struct which has its contents defined in the source code itself. The opposite would be to generate the contents of the struct through computation or reading memory during the execution of the program.\nConclusion In conclusion, here are the similarities and differences that stood out to me in this morning\u0026rsquo;s study of Go structs.\nSimilarities Both can have fields to hold values of different types Both can have methods Differences Go structs are used for data structures, classes in Python are used for OOP Go struct fields have static types Methods are defined separately from the struct definition Links: 202304100704\n[[go]]\n[[data-types]]\nhttps://go.dev/tour/moretypes/2\nhttps://go.dev/tour/methods/1\nhttps://articles.wesionary.team/map-vs-struct-in-golang-when-to-use-b0b66446627a\n","permalink":"https://mischavandenburg.com/zet/go-struct-python-class/","summary":"The Tour of Go is very clear:\nGo does not have classes.\nOne benefit of learning multiple programming languages is that each language provides you with a set of “pegs” that you can use to refer to other languages. As I learned about structs in Go, I hung them to the “Python classes” peg and used that as a reference point. Using these reference points can help you to understand the object of study by looking at their differences and similarities.","title":"Structs In Go - Similar To Classes In Python?"},{"content":"I\u0026rsquo;m working on my twitter CLI and as I was writing a function to format the tweet I remembered something I picked up last week. After a quick search in my notes I remembered to use the %q with printf.\nslice := []string{feed.Items[0].Title, feed.Items[0].Link} result := strings.Join(slice, \u0026#34;\\n\u0026#34;) fmt.Printf(\u0026#34;Testing printf %q\u0026#34;, result) This is very useful when formatting output. Now I can actually see whether it is inserting the new line characters correctly:\nTesting printf \u0026quot;I Made My First Tweet Using My Go Program\\nhttps://mischavandenburg.com/zet/go-first-tweet/\u0026quot;I Made My First Tweet Using My Go Program\nThis can be harder to see when printing the variable using fmt.Println or using the %v verb with Printf.\nLinks: 202304091304\n[[go]]\n[[coding]]\n","permalink":"https://mischavandenburg.com/zet/go-use-q-for-debugging/","summary":"I\u0026rsquo;m working on my twitter CLI and as I was writing a function to format the tweet I remembered something I picked up last week. After a quick search in my notes I remembered to use the %q with printf.\nslice := []string{feed.Items[0].Title, feed.Items[0].Link} result := strings.Join(slice, \u0026#34;\\n\u0026#34;) fmt.Printf(\u0026#34;Testing printf %q\u0026#34;, result) This is very useful when formatting output. Now I can actually see whether it is inserting the new line characters correctly:","title":"Use The %q Verb When Debugging In Go"},{"content":"I spent the evening learning about the Twitter API. It was not as straight forward as I thought. My goal was to do this project using only the standard library, and I hoped to get away with a few simple curls, but since the API requires OAuth 1 to create tweets, I had to revise my strategy.\nAfter struggling with Postman for a few hours to get the correct environment variables set up I managed to make my first tweet through Postman. Turns out that Twitter made some big changes in the free tier of their API, and it took me quite a while to figure out that the functions that are used as examples in the API documentation are not accessible to free accounts anymore.\nWhen I realized I would not get away with a simple curl I looked into libraries, but many were deprecated. Eventually I discovered gotwi and it didn\u0026rsquo;t take long to make my first tweet using Go.\nIt feels like cheating, but looking at the complexity of the library, it looks like I would be stuck in the weeds for quite a while if I wanted to figure all of this out by myself.\nThere is still enough to figure out in this project, such as curling my RSS feed and creating a tweet out of that, and wrapping it in a CLI, so in the end I\u0026rsquo;m glad I found a working library.\nHere\u0026rsquo;s the program I used to make my first tweet. It is a modified example from the gotwi GitHub readme.\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/michimani/gotwi\u0026#34; \u0026#34;github.com/michimani/gotwi/tweet/managetweet\u0026#34; \u0026#34;github.com/michimani/gotwi/tweet/managetweet/types\u0026#34; ) func main() { in := \u0026amp;gotwi.NewClientInput{ AuthenticationMethod: gotwi.AuthenMethodOAuth1UserContext, OAuthToken: os.Getenv(\u0026#34;TWITTER_ACCESS_TOKEN\u0026#34;), OAuthTokenSecret: os.Getenv(\u0026#34;TWITTER_ACCESS_TOKEN_SECRET\u0026#34;), } c, err := gotwi.NewClient(in) if err != nil { fmt.Println(err) return } p := \u0026amp;types.CreateInput{ Text: gotwi.String(\u0026#34;Hello World! This time I\u0026#39;m using os.Getenv in my Go program to load the credentials.\\n#go #coding #study #learning\u0026#34;), } res, err := managetweet.Create(context.Background(), c, p) if err != nil { fmt.Println(err.Error()) return } fmt.Printf(\u0026#34;[%s] %s\\n\u0026#34;, gotwi.StringValue(res.Data.ID), gotwi.StringValue(res.Data.Text)) } Links: 202304082104\nhttps://github.com/michimani/gotwi\nhttps://mischavandenburg.com/zet/go-twitter-cli-project/\n[[go]]\n[[go-twitter-cli-project]]\n","permalink":"https://mischavandenburg.com/zet/go-first-tweet/","summary":"I spent the evening learning about the Twitter API. It was not as straight forward as I thought. My goal was to do this project using only the standard library, and I hoped to get away with a few simple curls, but since the API requires OAuth 1 to create tweets, I had to revise my strategy.\nAfter struggling with Postman for a few hours to get the correct environment variables set up I managed to make my first tweet through Postman.","title":"I Made My First Tweet Using My Go Program"},{"content":"When learning a programming language it is important to start building things quickly and to begin applying the theory. I have a tendency to dive into the books and lose myself in the theory, where I should be getting hands on experience.\nOver the past few months I\u0026rsquo;ve generated a bunch of ideas for projects that I want to write, and I selected my first project today.\nhttps://github.com/mischavandenburg/twitter-cli\nhttps://twitter.com/mischa_vdburg\nTwitter CLI Programs should solve a problem. My problem has to do with Twitter. I recently created a Twitter account, and I want to make a tweet whenever I publish something new on my website. I\u0026rsquo;m currently doing this by hand, and that needs to stop, obviously.\nThere are bots out there for this, but I want to build it myself. I\u0026rsquo;ve created the following user stories for my project.\nUser Story 1 As a user, I need a command that I can run from a bash shell that will post the standard input to my Twitter account\nUser Story 2 As a user, I need a command that I can run from a bash shell that will take the latest post from the RSS feed generated by my blog and post it to Twitter\nConcepts By writing this program I\u0026rsquo;ll need to figure out the following problems in Go:\nTaking input from the command line Authenticating to the Twitter API Making a POST request to the Twitter API Curling an RSS feed Looping over / reading XML / HTML data Transforming that data to a suitable format to post to Twitter Expansion This will be a good start for my project and will keep me busy for a while. When I solved the previous problems I can use the result and expand further. Some thoughts about further expansion:\nRSS Feeds I can use the skills I learn to start crawling Reddit feeds and filter them for keywords. I can automatically generate a curated selection from Reddit which will be easier to consume and will save me time by only serving me content that I might think is interesting to me, based on keywords.\nDatabase I want to learn more about using databases on Kubernetes and how to interact with databases using Go. For this I\u0026rsquo;d like to store my RSS feed into a database and keep track of information in the database. I could track whether an article has been posted to Twitter and when.\nBot Rather than posting my latest blog post to Twitter by manually running a command, I should have a bot scanning my blog and posting to Twitter when it detects a new article. Or I could trigger the bot whenever I make a push to my blog repo.\nIn any case, I want to have an application running on a server. I\u0026rsquo;m making plans to start up a proper home lab and this will be a perfect use case to start running on my home Kubernetes cluster.\nLinks: 202304081304\nhttps://github.com/mischavandenburg/twitter-cli\nhttps://twitter.com/mischa_vdburg\n[[go]]\n[[go-twitter-cli-project]]\n","permalink":"https://mischavandenburg.com/zet/go-twitter-cli-project/","summary":"When learning a programming language it is important to start building things quickly and to begin applying the theory. I have a tendency to dive into the books and lose myself in the theory, where I should be getting hands on experience.\nOver the past few months I\u0026rsquo;ve generated a bunch of ideas for projects that I want to write, and I selected my first project today.\nhttps://github.com/mischavandenburg/twitter-cli\nhttps://twitter.com/mischa_vdburg\nTwitter CLI Programs should solve a problem.","title":"Outlining My First Go Project"},{"content":"Dopamine Listened to an interesting podcast on Dopamine by Andrew Huberman. I always enjoy his content because he is completely driven by science, not by personal opinion. He is truly objective as far as I can tell.\nDopamine is often misunderstood. It is not necessarily associated with good feelings after you pursue an activity. It is more related to motivation to engage in certain behaviours.\nRewards Scientists did an experiment where they observed children of kindergarten age, and later reproduced this in adults. The children were monitored during school days, where they had blocks of structured time and free time. During structured time their activities were managed. They had to sing or write, for example. Then they had blocks of free time where they could do whatever activity they liked.\nThey observed what activity the children liked to do in their free time, and then started to reward these children for their work. For example, if the children liked to draw in their free time, the researches started giving them gold stars and other praise for their work.\nThen the researches removed the reward for the activity, and interestingly, the children and adults started showing less interest and enjoyment in the activity because they were lacking the reward.Note that this is the activity that they liked to do in their free time prior to the experiment.\nThis all has to do with the nature of dopamine, dopamine baselines and peaks.\nHuberman explains this with a metaphor of a pool. In the pool you have waves, and the waves have peaks and troughs. At the peaks, your dopamine is high, but they are inevitably followed by troughs: dips in dopamine, which lead to low energy and low motivation.\nThen there is the baseline of dopamine: the amount of water in the pool. The baseline is the relative point to which the peaks and throughs are determined.\nSo it is important that you don\u0026rsquo;t reward yourself for activities that you are doing. Don\u0026rsquo;t reward yourself too much when you achieve a fitness or study goal. You will feel good after the event, but you will end up in a through. Being in the trough means feeling demotivated, leading to procrastination.\nGetting Out Of The Trough The good news is that the system will reset itself after experiencing highs or lows due to rewards.Getting out of the trough happens automatically, but you can influence the rate at which you climb out of it.\nLet\u0026rsquo;s say you are not motivated to study. You\u0026rsquo;ll need to do something which is more painful. Something that\u0026rsquo;s even harder to do. Whatever feels harder to do in the moment than the activity that you should be doing. Do something that makes you feel uncomfortable.\nTactics exercise for 1 minute do something that really sucks cold water anything that makes you cringe leverage something that\u0026rsquo;s painful The Holy Grail Of Motivation The holy grail of motivation: not needing any external reward, but enjoying the activity itself that is leading you towards your goal. For example, enjoying running training in pursuit of a marathon goal. Enjoying coding and doing coding exercises in pursuit of becoming a software engineer.\nI feel very fortunate that I succeeded at turning my passion into my career. The journey itself is fun. I enjoy coding, and that activity is taking me closer towards the goal of becoming a good software engineer and the ability to contribute to open source.\nLinks: 202303311903\nhttps://hubermanlab.com/leverage-dopamine-to-overcome-procrastination-and-optimize-effort/\n[[my ikigai and flow]]\n[[coding]]\n[[devops]]\n[[career]]\n","permalink":"https://mischavandenburg.com/zet/dopamine-and-motivation/","summary":"Dopamine Listened to an interesting podcast on Dopamine by Andrew Huberman. I always enjoy his content because he is completely driven by science, not by personal opinion. He is truly objective as far as I can tell.\nDopamine is often misunderstood. It is not necessarily associated with good feelings after you pursue an activity. It is more related to motivation to engage in certain behaviours.\nRewards Scientists did an experiment where they observed children of kindergarten age, and later reproduced this in adults.","title":"Podcast Notes: Dopamine and Motivation - Huberman"},{"content":"I noticed multiple times now that during my workouts my mind is generating lots of new ideas for coding projects or blog posts. It is a well known fact that walking stimulates areas in the brain that which will in turn stimulate creativity. But I\u0026rsquo;m noticing that it starts to happen with strength workouts as well.\nAlthough I don\u0026rsquo;t struggle with the motivation to get myself to the gym, I sometimes feel \u0026ldquo;guilty\u0026rdquo; for not spending that time on coding or studying. I\u0026rsquo;m realizing now that this is a very irrational train of thought.\nExercise actually stimulates my creativity and will help generating new ideas or help me understand concepts better in the unconscious. It is also massively beneficial for health and general well being. When general well-being is enhanced I\u0026rsquo;ll also be able to focus better and absorb information more quickly.\nI should re-frame my thinking and actually put exercise as priority number 1. Exercise time is never wasted time. It indirectly contributes to my productivity and coding endeavours as well.\nLinks: 202304011104\n[[exercise]]\n[[health]]\n[[focus]]\n[[productivity]]\n","permalink":"https://mischavandenburg.com/zet/exercise-stimulates-creativity-and-motivation/","summary":"I noticed multiple times now that during my workouts my mind is generating lots of new ideas for coding projects or blog posts. It is a well known fact that walking stimulates areas in the brain that which will in turn stimulate creativity. But I\u0026rsquo;m noticing that it starts to happen with strength workouts as well.\nAlthough I don\u0026rsquo;t struggle with the motivation to get myself to the gym, I sometimes feel \u0026ldquo;guilty\u0026rdquo; for not spending that time on coding or studying.","title":"Exercise Stimulates Creativity And Motivation"},{"content":"A couple of weeks ago I created a simple bash script to generate a date in format YYY-MM-DD to use with the magic !! wands in vim.\nJust now I wanted to expand this with being able to create a markdown header with this date.\n#!/bin/bash # Gendate generates the date in YYY-MM-DD format # Can be called with arguments h and number to generate a markdown heading # For example: \u0026#39;gendate h 3\u0026#39; will generate \u0026#39;### 2023-03-29\u0026#39; header=\u0026#34;\u0026#34; date=$(date +\u0026#34;%Y-%m-%d\u0026#34;) if [[ $# -gt 0 ]]; then # handling wrong arguments if [[ $# -eq 1 || \u0026#34;$1\u0026#34; != \u0026#34;h\u0026#34; ]]; then echo \u0026#34;Usage: gendate h 2 to generate with markdown heading ##\u0026#34; exit 1 fi # format markdown heading if arguments h, n are given if [[ $# -eq 2 ]]; then for i in $(seq 1 $2); do header+=\u0026#34;#\u0026#34; done header+=\u0026#34; \u0026#34;$date echo \u0026#34;$header\u0026#34; exit 1 fi fi # if no arguments given, generate the date echo \u0026#34;$date\u0026#34; Links: 202304011104\nhttps://github.com/mischavandenburg/dotfiles/tree/main/scripts\n","permalink":"https://mischavandenburg.com/zet/generate-markdown-headings-date/","summary":"A couple of weeks ago I created a simple bash script to generate a date in format YYY-MM-DD to use with the magic !! wands in vim.\nJust now I wanted to expand this with being able to create a markdown header with this date.\n#!/bin/bash # Gendate generates the date in YYY-MM-DD format # Can be called with arguments h and number to generate a markdown heading # For example: \u0026#39;gendate h 3\u0026#39; will generate \u0026#39;### 2023-03-29\u0026#39; header=\u0026#34;\u0026#34; date=$(date +\u0026#34;%Y-%m-%d\u0026#34;) if [[ $# -gt 0 ]]; then # handling wrong arguments if [[ $# -eq 1 || \u0026#34;$1\u0026#34; !","title":"Generating Markdown Headings with YYYY-MM-DD Date Format Using Bash"},{"content":"I\u0026rsquo;m still at the beginning of my Go learning journey, but I worked through a few tutorials and guides by now. I\u0026rsquo;ve gathered lots of ideas for programs that I want to write, big and small, but I have to start somewhere.\nThe best thing to do is to write little programs that solve a problem that you have.\nOne problem I needed to solve was converting sentences to title case in vim. There are plugins for this, or elaborate macros, but I thought this was a nice opportunity to write my first program from scratch. You can view the program here: my go repo.\nTitle Converting a string to title case is fairly easy:\n// Package title converts a string to title case. package title import ( \u0026#34;fmt\u0026#34; \u0026#34;strings\u0026#34; ) func Make(s string) { t := strings.Title(s) fmt.Printf(\u0026#34;%v\\n\u0026#34;, t) } This function takes a string as an argument and uses the Title function from the strings package to convert it. I use printf to format the output and to add a new line character.\nChallenge The challenge lies in taking input from Stdin. I solved this by reusing the things I learned from the greet challenge. I described how to read from standard input in go in this article.\nHere is the code for the title command:\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;github.com/mischavandenburg/go/projects/title\u0026#34; \u0026#34;github.com/mischavandenburg/go/projects/title/internal\u0026#34; ) func main() { var x string var err error if len(os.Args) \u0026gt; 1 { x = strings.Join(os.Args[1:], \u0026#34; \u0026#34;) } if x == \u0026#34;\u0026#34; { x, err = internal.ReadLine(os.Stdin) } if err != nil { log.Print(err) return } title.Make(x) } This main function checks if arguments are given to the program from the command line, and uses these arguments if they are given. If there are no arguments, it will expect input to be piped to it. It takes either the input from the arguments or from standard input and calls the Make() function that I described above.\nThis main function uses a Readline function to extract the string from standard input:\npackage internal import ( \u0026#34;bufio\u0026#34; \u0026#34;io\u0026#34; \u0026#34;strings\u0026#34; ) // ReadLine takes anything of type io.Reader and returns a trimmed string (initial // and trailing white space) or an empty string and error if any error // is encountered. func ReadLine(in io.Reader) (string, error) { out, err := bufio.NewReader(in).ReadString(\u0026#39;\\n\u0026#39;) out = strings.TrimSpace(out) return out, err } For a more detailed explanation of this function read this article I wrote.\nUsage I wrote the program so I can call it with a sentence as an argument:\nmischa@mac-beast:~ (ins)$ title hello world, this is my sentence in title case Hello World, This Is My Sentence In Title Case But the main use case is to use it as a UNIX filter inside of vim. A UNIX filter is a program that takes standard input, performs some operation on the input, and prints it to standard output.\nNow, when I\u0026rsquo;m working inside of vim, I can convert the current line to title case by typing the following command:\n!!title\nThe !! command sends the specified amount of lines to the command you specify. Exclamation marks truly are magic wands! I highly recommend reading this article by rwxrob to learn more about them.\nLinks: 202303280803\nhttps://github.com/mischavandenburg/go/tree/main/projects/title\nhttps://rwx.gg/tools/editors/vi/how/magic/\nReading from standard input in Go\n","permalink":"https://mischavandenburg.com/zet/my-first-go-program/","summary":"I\u0026rsquo;m still at the beginning of my Go learning journey, but I worked through a few tutorials and guides by now. I\u0026rsquo;ve gathered lots of ideas for programs that I want to write, big and small, but I have to start somewhere.\nThe best thing to do is to write little programs that solve a problem that you have.\nOne problem I needed to solve was converting sentences to title case in vim.","title":"I Wrote My First Go Program Today"},{"content":"Pods have containers, and limits can be set on those containers.\nRequests used by the kube-scheduler to determine where the Pod will be placed containers can use more than requested resources if it is available on node If a limit is specified, but no request, Kubernetes will use the limit value as the request value.\nLimits containers may never use more than the set limit\nenforced by kubelet and container\nhost kernel will kill processes that attempt to allocate more than limit (OOM error)\nreactively: killed when exceeded\nenforcement: system prevents container to ever exceed limit\nIf the node runs out of memory and the container exceeds its memory request, the pod will be evicted\ncontainer runtimes don\u0026rsquo;t terminate Pods or containers for excessive CPU usage\nPods The pod resource request and limit is the sum of the resource requests of the containers in the pod.\nCPU Units Defined as an absolute amount of resource. 1000m = 1 CPU.\nThis is always the same unit, regardless whether the host has 4 or 48 CPU\u0026rsquo;s.\n500m CPU = 0.5 CPU\nMemory Units Can use P, T, G, M etc.\nNote that \u0026ldquo;m\u0026rdquo; is not megabyte. 0.8m = 0.8 bytes.\nUse mebibytes Mi or megabytes M.\nDefiniton Example spec: containers: - name: nginx image: nginx resources: requests: memory: 100Mi cpu: 250m limits: memory: 200Mi cpu: 500m Scheduling The scheduler ensures that the sum of requests of the pods on the node does not exceed the available resources.\nEven if a node has low resource usage, it will not accept pods that have requests which exceed the available resources.\nNodes use k describe node to see the resource status of the node.\nLinks: 202303281903\nhttps://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\n","permalink":"https://mischavandenburg.com/zet/kubernetes-resource-management-pods-containers/","summary":"Pods have containers, and limits can be set on those containers.\nRequests used by the kube-scheduler to determine where the Pod will be placed containers can use more than requested resources if it is available on node If a limit is specified, but no request, Kubernetes will use the limit value as the request value.\nLimits containers may never use more than the set limit\nenforced by kubelet and container","title":"Kubernetes Resource Management for Pods and Containers - CPU and Memory"},{"content":"I used to make all my notes on paper, but I decided to switch to a digital note-taking system about two years ago.\nDigital note-taking provides the following advantages:\nSearchability Collected in one place Can be converted to different output formats Easier to share with others Does not take up physical space No risk of losing your notes in case of fire or other disaster I\u0026rsquo;ve gone through several iterations of my note-taking systems. I started on Google Docs, moved on to Notion and eventually landed on Obsidian. However, as I became more proficient with UNIX systems and vim, I realized I did not need all of that distracting functionality, and I switched over to using Neovim and a few bash scripts. I occasionally open up Obsidian to look at my graph view or to make use of the Anki plugin, but I enjoy the trimmed-down version that I built myself. I don\u0026rsquo;t need to leave the command line to read my notes or to create a new one.\nIn this article I\u0026rsquo;ll show you the system I\u0026rsquo;m currently using for note-taking. My note-taking system is part of a larger system which I call my \u0026ldquo;Second Brain\u0026rdquo;, a term coined by Tiago Forte. This is an overarching system I\u0026rsquo;ve build to manage my time, projects, tasks and bookmarks. Explaining this entire system is beyond the scope of this article, but I plan to write about my Second Brain in the future as well.\nMy system is based on the Zettelkasten method, but I\u0026rsquo;ve adjusted it to my own needs. After a few years of experimentation I feel that my system has reached a \u0026ldquo;mature\u0026rdquo; state, but it will always remain continuosly under development, also because my needs may change.\nWhat is a Zettelkasten? Zettelkasten is a system of note taking and personal knowledge management. Zettelkasten means \u0026ldquo;slip box\u0026rdquo; in German. Traditionally it is a physical box of slips of paper or index cards containing smaller notes. Each of these cards have a unique identifier, and these identifiers are used to create links between the cards.\nNiklas Luhmann, who was one of the most prolific scholars in history, used it extensively. He wrote 70 books and nearly 400 academic articles, and he credited the Zettelkasten with making his productivity possible. It contained around 90,000 index cards.\nI first came across this concept by reading the book How to take Smart Notes by Sönke Ahrens.\nWhy Take Notes? \u0026ldquo;Your professional success and quality of life depend directly on your ability to manage information effectively.\u0026rdquo;\nTiago Forte, Building A Second Brain\nIn the modern age we consume very large amounts of information, much more than we can remember. It is therefore very important to manage your intake of information, but also the retention of that information. I find that taking notes on the topics that may be of interest to me at any given time helps me understand the subject much better, because it forces me to clearly formulate my thoughts and convert them to written form.\nOver time a large collection is built up which can be used to generate new ideas.\nWhat Do I Take Notes On? Firstly, I keep a daily journal where I write down my thoughts, my feelings and the activities that I undertake. Some days may be several pages worth, and other days will be only \u0026ldquo;Had a nice day today\u0026rdquo;. But I make a point to write something down every day.\nSecondly, I write about the topics I study. I\u0026rsquo;m always learning something new. I\u0026rsquo;m a DevOps Cloud Engineer, and new developments are made constantly in the Cloud Native landscape. There is always more to learn.\nMy notes are not only work related. I also keep notes of the research I do for all of my interests, such as health, longevity, exercise, diet, literature, yoga and meditation. I made a habit out of always writing a note when I read a book, watch a documentary or listen to a podcast. These notes can be a short summary, a few thoughts that come to mind, or a few bullet points that capture the main takeaways.\nThese activities are usually accompanied by taking notes:\nReading books Watching video courses Watching educational YouTube videos Researching hobbies and interests Coding Reflection Interesting conversations Listening to podcasts An Overview of My System My system is a modified Zettelkasten system that incorporates daily notes for journaling. The Zettelkasten method emphasizes small atomic notes containing only your own thoughts and links to other notes. However, I choose to also make longer notes. For example, when I did the Fundamentals of Bicep course, I made one large note that contains all of the information I wished to remember or refer to later. In this particular use case, I think it\u0026rsquo;s much better to keep everything collected in one note, rather than breaking this note up in smaller notes on parameters and variables, for example.\nAnother area where I diverge from the traditional Zettelkasten is categorization of notes. Traditionally the Zettelkasten is one big repository of notes, but I like to keep them loosely collected. When I create a new note, my system places that note in an inbox directory. Once a week I go through my inbox and revisit all of my notes. Sometimes I will throw some notes away because I don\u0026rsquo;t think they\u0026rsquo;re necessary after all. When I decide to keep it, I put it in a directory that I find suitable. I have a directory structure based on the PARA method, but I also have a large Zettelkasten directory where I put anything that does not necessary belong to a category. I mostly keep some notes collected in a directory because I expect to use them as a collection at some point, or because I want to run scripts on the files.\nEvery day a daily note is created that contains a few checkboxes for habit tracking and sometimes I\u0026rsquo;ll include an inspirational quote in my template for a while.\nThis is what a daily note looks like in my editor:\nLocal Text Files I think it is important to use locally stored markdown text files. This is mainly why I moved away from Notion. My notes are stored on my local machine rather than with a third party. This gives me complete control over my data.\nBecause my notes are stored as markdown files, I can use various tools to write or edit my notes. Moreover, storing my notes as local files allows me to run scripts on my notes and customize my workflow to my heart\u0026rsquo;s content. This can be especially useful for automating tasks, streamlining my work, or making bulk updates.\nI have my collection backed up in several places and on GitHub.\nI recommend this article about the benefits and merits of using markdown for note-taking.\nVim, My Preferred Text Editor I use Vim, or technically, Neovim for all of my text editing. It took a while to get used to, but my productivity related to text editing has increased significantly. I also find it very enjoyable to work on the command line, not having to leave my tmux window to manage my entire Zettelkasten or keep my journal.\nMy vim configuration is always subject to constant change, but I only use three plugins that are related to my note-taking and Zettelkasten.\n1. Telescope I cannot live without this plugin any longer. It is a fuzzy searcher integrated into my editor. I can search for files but also search within the files using ripgrep, and preview the files as I\u0026rsquo;m searching.\nHeres what a search for Neovim looks like:\nI can navigate through all the search results blazingly fast using only the keyboard, and I can preview the files by scrolling up and down in the window on the right hand side.\n2. Marksman LSP Marksman is a markdown language server which helps me format my documents and to create links between notes.\nWhen I start to make a link it will search through my entire Zettelkasten and look for matches based on the filename or markdown headers. Here it shows me suggestions related to markdown.\n3. Pandoc According to Wikipedia, \u0026ldquo;Pandoc is a free-software document converter, widely used as a writing tool (especially by scholars) and as a basis for publishing workflows.\u0026rdquo;\nI use the Pandoc plugin for vim to export my notes to different formats when I need to.\nHowever, the thing I like most about it is that it renders my notes very nicely as I\u0026rsquo;m writing them. I really like how headings look, and that it will make text bold while editing, which did not work when I used nvim-markdown.\nBash To facilitate my workflow I wrote a few bash scripts.\nWhen I want to take a note, I run the zet command from the command line. This command is a bash script I wrote which creates a file in my inbox, adds a template to it, and opens the file in Neovim. I can either provide a filename to it as an argument or it will ask me to provide one.\nI have another script named blog which I use to create notes that I intend on publishing on my website. These files are stored in a different location and use a different template. The blog script also has a pub function that will publish the script directly to my website in a few seconds after I\u0026rsquo;m done writing.\nThese scripts can be found in my dotfiles repo.\nIdentifiers and Filenames The Zettelkasten method assigns unique identifiers to notes. I generate a unique identifier from the current date and time, but I never really use these identifiers. However, since I\u0026rsquo;ve been doing this consistently from the beginning, I\u0026rsquo;ll keep generating them to make my system future proof. Who knows, maybe they will become useful someday when I suddenly find a use case for it.\nI\u0026rsquo;ve seen other people use unique identifiers in filenames as well, but this is a big no for me. I want to be able to discern what the contents might be from the filename whenever I\u0026rsquo;m navigating around my directory structure. Using the filename as the note identifier is a much better solution in my opinion. There is a small chance that you will choose a filename that already exists, but you can easily add a number to it or change a word and you have solved that problem.\nMoreover, I use the marksman LSP server to create links between my notes in Neovim. The LSP server is based on headings and filenames, so I\u0026rsquo;m really glad that I chose to use filenames such as \u0026ldquo;fundamentals-of-bicep.md\u0026rdquo; right from the beginning.\nTags and Links If you are as deep into note-taking systems as I am, you might have come across the problem of using tags versus using links for your note-taking system.\nI primarily use [[markdown links]] to create links in my notes. This is also why you will sometimes see these things in my public zettelkasten. The links between notes are rendered beatifully by Obsidian in the graph view. This is also why I\u0026rsquo;ll always try to keep my system compatible with Obsidian, even though I\u0026rsquo;ve moved away from the application almost entirely.\nHere is a graph view of my Zettelkasten:\nFor the notes in my Zettelkasten which are intended to be published on my website, I use tags in the YAML frontmatter. These are used by my static website generator, Hugo, to create tag pages on my website which I think will be useful to my readers.\nHere is an example of the YAML front matter of one of these notes:\n--- title: \u0026#34;I\u0026#39;m In Love with my Work: Lessons from a Japanese Sushi Master\u0026#34; date: 2022-10-08 tags: - Career - Personal - Article --- Conclusion That\u0026rsquo;s about it. My system is a minimalistic set of tools utilized for maximum productivity. It\u0026rsquo;s Neovim with a few plugins and a couple of helper scripts I wrote in bash that store markdown files in a directory structure that I find meaningful. After using many different applications and solutions, I\u0026rsquo;m extremely satisfied with the system I built for myself which is very basic, free from distractions and tailored to my own needs.\nWhen I want to reflect on a certain topic or to write an article, I go through my collection of notes and link them together. Very often I find that the creation of these links will stimulate even more new connections and associations, and I end up with new ideas and more topics of study I want to explore and write about. I use my Zettelkasten as a vehicle for reflection, learning and creativity.\nI love the simplicity of my system. It enables me to capture notes very quickly while I\u0026rsquo;m working or studying with very little effort. By storing my note collection on my iCloud drive and in GitHub, they can be accessed from all of my devices across all different operating systems.\nI hope that this article may give you some inspiration to start building your own note collection. You don\u0026rsquo;t need to build your own system like I have done, any app that satisfies your needs can be used. Keeping notes on the things I encounter in life is one of the most enriching habits I\u0026rsquo;ve acquired. It will be interesting to see how large my collection grows over the years!\nLinks: 202303270703\nhttps://fortelabs.com/blog/basboverview/\nhttps://fortelabs.com/blog/para/\nhttps://rwx.gg/lang/md/\nhttps://github.com/mischavandenburg/dotfiles/tree/main/scripts\n","permalink":"https://mischavandenburg.com/zet/neovim-zettelkasten/","summary":"I used to make all my notes on paper, but I decided to switch to a digital note-taking system about two years ago.\nDigital note-taking provides the following advantages:\nSearchability Collected in one place Can be converted to different output formats Easier to share with others Does not take up physical space No risk of losing your notes in case of fire or other disaster I\u0026rsquo;ve gone through several iterations of my note-taking systems.","title":"My Neovim Zettelkasten: How I Take Notes in Markdown Using Vim and Bash"},{"content":"I\u0026rsquo;m working through the \u0026ldquo;greet\u0026rdquo; challenge by rwxrob. It is amazing how such a relatively simple and small challenge can lead down to so many rabbit holes.\nThe program should take input from the user and print it out. I worked through the challenge together with Rob in his video but I\u0026rsquo;m going to talk (write) myself through these functions to fully understand what\u0026rsquo;s going on.\nWe have the following function in main.go:\nfunc main() { var name string var err error if len(os.Args) \u0026gt; 1 { name = strings.Join(os.Args[1:], \u0026#34; \u0026#34;) } if name == \u0026#34;\u0026#34; { fmt.Println(\u0026#34;Hello there, what\u0026#39;s your name?\u0026#34;) name, err = internal.ReadLine(os.Stdin) if err != nil { log.Print(err) return } } greet.Hi(name) } If the number of arguments passed to the program is greater than 1, we set name to a joined string created from the provided arguments. Args[0] would be the path to the program, so we don\u0026rsquo;t want that to be included. As a result, greet mischa will pass mischa to the greet.Hi() function, as defined in greet.go, and print out Hello, mischa.\nHowever, if no arguments are passed to the greet program, discovered in case name is empty, we ask the user for input. We capture the input by os.Stdin and pass it to the ReadLine() function, which is located at internal/readline.go.\n// ReadLine takes any io.Reader and returns a trimmed string (initial // and trailing white space) or an empty string and error if any error // is encountered. func ReadLine(in io.Reader) (string, error) { out, err := bufio.NewReader(in).ReadString(\u0026#39;\\n\u0026#39;) out = strings.TrimSpace(out) return out, err } } ReadLine has a parameter in of type io.Reader, which is an interface. Next, we determine that ReadLine should return two values of type string and error. I\u0026rsquo;m saving to learn about interfaces for another day, I\u0026rsquo;m just going to work through this function now.\nWe assign the output of bufio.NewReader(in).ReadString('\\n') to two new variables named out and err using the \u0026ldquo;walrus operator\u0026rdquo; := which detects the types automatically. We can do it like this because ReadString returns (string, error).\nWe take the in argument of type io.Reader which was passed to the ReadLine function, which in this case is the io.Stdin that came from our main function, and pass it on to bufio.NewReader(in). Then we are able to read the string until the newline character \\n in the string, and trim off the whitespace from the beginning and the end of the string by calling TrimSpace on the out variable.\nThen we return the trimmed string back to our original main function, which will pass it on to the Hi() function.\nHowever, if the bufio.NewReader(in).ReadString('\\n') should return an error, it is caught by this code in the main function:\nif err != nil { log.Print(err) return } This is a standard way of handling errors in Go. If the error is anything else than nil, we will print the error and end the function with the return keyword.\nThoughts I\u0026rsquo;m really glad I took the time to talk / write myself through this program. I think I\u0026rsquo;m going to make a habit of this as I\u0026rsquo;m learning Go. It made everything much clearer when I sat down and traced the arguments from function to function and describing every step in my own words.\nI\u0026rsquo;ll post this note in the YouTube comments, maybe somebody will find it useful as well.\nLinks: This page goes a lot deeper in what stdin and stdout actually do in this context. Very interesting reading:\nhttps://stackoverflow.com/questions/12363030/read-from-initial-stdin-in-go\nThe code in my repo:\nhttps://github.com/mischavandenburg/go/tree/main/rwxrob/boost2022/challenges/greet\n[[go]]\n[[go-rwxrob]]\n[[coding]]\n[[functions]]\n","permalink":"https://mischavandenburg.com/zet/go-reading-stdinput-cmdline/","summary":"I\u0026rsquo;m working through the \u0026ldquo;greet\u0026rdquo; challenge by rwxrob. It is amazing how such a relatively simple and small challenge can lead down to so many rabbit holes.\nThe program should take input from the user and print it out. I worked through the challenge together with Rob in his video but I\u0026rsquo;m going to talk (write) myself through these functions to fully understand what\u0026rsquo;s going on.\nWe have the following function in main.","title":"Go - Reading from Standard Input Provided by User"},{"content":"Beginner Boost Week 17 and 18 Notes Link to video\nDon\u0026rsquo;t forget to set GOBIN=~/.local/bin, GOPRIVATE, CGO_ENABLED=0 Go Testing - Example Tests func ExampleFoo() { foo() // Output: // Foo } The ExampleFoo indicates the test here. It needs to match the name exactly after Example. But it is capitalized.\nIt runs that function and will compare the output to what is specified.\nIt says \u0026ldquo;see if the program generates this output in std out\u0026rdquo;.\nThese are called example tests.\nTo export a function the first letter should be a capital. Everything that has a capital as first letter is exported.\nWhen you are writing example tests you are providing readable automatic documentation to your end users.\nYou can use // Unordered Output if you don\u0026rsquo;t care about the order.\nPrintf use printf %q to see all of the characters. %q escapes all of the invisible characters. So you can see \\r or \\n for example. This is the only way to check for empty values in Example tests.\nHow to Learn Go Don\u0026rsquo;t go to the books. Stay with the spec, write your own projects, and find one or two or 10 Go projects and study the crap out of them.\nStudy the code bases. Kind is a good codebase.\nRead the Go codebase!\nhttps://youtu.be/9hEnzD-bNy4?t=8467\nRead other people\u0026rsquo;s code and see if your code looks like that.\n\u0026ldquo;Good artists copy, great artists steal.\u0026rdquo; - quote accredited to Steve Jobs but it has its origin in T.S. Eliot and even further back.\nStart with the tests! Read the tests first.\nSearching Go Documentation Always search for golang when searching on the net. Preferably text based searching.\nThen you use go doc os.Stderr - for example.\nVariable Names Long variable names are frowned upon in the Go community. They should be as short as they need to be.\nIf using in a tight scope, a single letter is fine. In a block two or three characters is more than enough. It distracts from reading the code if they have very long names.\nThe more remote the variable is, the more descriptive it should be.\nThe Art of Coding breaking everything down into small 1 task functions don\u0026rsquo;t repeat yourself See stuttering\nTop Level Libraries It\u0026rsquo;s not common to make a single utility at a top-level GitHub repo.\nIt\u0026rsquo;s very likely that you are going to want to reuse code somewhere.\nYou should start thinking of things as composition. \u0026ldquo;How is this code going to be used by other people\u0026rdquo;.\nGo is different than bash: you will use this code elsewhere.\nWhat is the function of what I\u0026rsquo;m creating? Create these as small composable blocks So you can use them later Example Based Testing https://pkg.go.dev/testing\nThe package also runs and verifies example code. Example functions may include a concluding line comment that begins with \u0026ldquo;Output:\u0026rdquo; and is compared with the standard output of the function when the tests are run. (The comparison ignores leading and trailing space.) These are examples of an example:\nfunc ExampleHello() { fmt.Println(\u0026#34;hello\u0026#34;) // Output: hello } func ExampleSalutations() { fmt.Println(\u0026#34;hello, and\u0026#34;) fmt.Println(\u0026#34;goodbye\u0026#34;) // Output: // hello, and // goodbye } Searching Information Being able to look up information quickly and taking notes about them is just as important as the coding itself.\nPackages or Libraries You can either have a command, which will be a main package, or you can have an importable library named .go. Should have any other name besides main.\nYou rarely want your top level of your module to be a package.\nConvention: cmd directory.\nThis is specifically for separate commands.\nStuttering greet.Greet() is called stuttering in go. Don\u0026rsquo;t do this.\nModules A module can be defined as \u0026ldquo;a GitHub repo with a go.mod\u0026rdquo; in it.\n\u0026ldquo;A collection of Go packages stored in a file tree with a go.mod file at its root\u0026rdquo;\nThe go.mod contains the import path of the module.\nWhat Module Does Greet Belong To? In the case of our greet command, it belongs to the greet module located at:\ngithub.com/mischavandenburg/go/rwxrob/boost2022/challenges/greet\nGreet has a main package which is part of the greet module.\nNow we will add another command with another main package to this module.\nCLI Stuff that has to do with interacting with the user on the command line should never be in the package library. The package library should be written to be solid no matter what.\nThe \u0026ldquo;name\u0026rdquo; for our greet program should be able to come to anywhere.\nAvoid Interactive Input You generally want to avoid interactive input. Prefer arguments, env variables or file inputs.\nIf you call your program from another script it will stall if it waits for interactive input.\nA good example is forgetting \u0026ldquo;-y\u0026rdquo; when running apt-get.\nAn interactive story game is a different use case than a CLI tool.\nUNIX filters are specifically designed to read input and generate an output based on the input. Here it is expected behaviour to stall if no input is given.\nRunes A rune is a single Unicode point.\nThe same people who created Go, created the Unicode standard.\nGo has the best Unicode support.\nWriting Documentation Convention is to always start with the name of the thing you are documenting. When documenting a function, always start with the function name. For example // ReadLine reads from standard input\nUse go doc --all to see a print of all the documentation in your package.\nThe same goes for packages. Always start with \u0026ldquo;Package internal does bla bla\u0026rdquo;\nLinks 202303261403\nhttps://www.youtube.com/watch?v=WMH5ENF_Xvo\n[[go]]\n[[coding]]\n[[rwxrob]]\n","permalink":"https://mischavandenburg.com/zet/go-rwxrob-boost-week-17-18/","summary":"Beginner Boost Week 17 and 18 Notes Link to video\nDon\u0026rsquo;t forget to set GOBIN=~/.local/bin, GOPRIVATE, CGO_ENABLED=0 Go Testing - Example Tests func ExampleFoo() { foo() // Output: // Foo } The ExampleFoo indicates the test here. It needs to match the name exactly after Example. But it is capitalized.\nIt runs that function and will compare the output to what is specified.\nIt says \u0026ldquo;see if the program generates this output in std out\u0026rdquo;.","title":"Go - Skillstak Beginner Boost Week 17 and 18 Notes"},{"content":"To read the news free from distractions and ads I use Newsboat as a reader for RSS feeds.\nHowever, one thing that annoyed me was that it would span across my entire screen in the terminal. When you read blogs or news pages in the browser, you\u0026rsquo;ll notice that the text is always located in a middle column of the window, so you don\u0026rsquo;t have to move your neck while reading. At least, this is the case with well designed websites that serve text content.\nVim has a plugin that achieves this and I use it extensively. It is called Zenmode. In Neovim I use a similar plugin called No Neckpain.\nTo achieve a similar configuration for Newsboat I used tmux. I wrote the following bash script:\n#!/bin/bash # nb opens a new pane and runs newsboat in it. I want to read from a centered column in my screen. tmux split-pane -h \\; resize-pane -x 130\\; send -t 2 \u0026#34;newsboat\u0026#34; Enter\\; send -t 1 \u0026#34;clear\u0026#34; Enter Note that this needs to be run from within an existing tmux window with no split panes.\nIt splits the current window in to two panes, resizes the new pane to a width of 130 pixels and sends the \u0026ldquo;newsboat\u0026rdquo; command to the new pane, and the \u0026ldquo;clear\u0026rdquo; command to the old (left) pane to keep it nice and clean.\nIn my ~/.newsboat/config file I added the following setting:\ntext-width 72\nThis will limit the text on the right hand side of the screen.\nThe end result looks like this:\nLinks: 202303260903\nhttps://github.com/folke/zen-mode.nvim\nhttps://github.com/shortcuts/no-neck-pain.nvim\n","permalink":"https://mischavandenburg.com/zet/newsboat-in-zenmode/","summary":"To read the news free from distractions and ads I use Newsboat as a reader for RSS feeds.\nHowever, one thing that annoyed me was that it would span across my entire screen in the terminal. When you read blogs or news pages in the browser, you\u0026rsquo;ll notice that the text is always located in a middle column of the window, so you don\u0026rsquo;t have to move your neck while reading.","title":"How to Run Newsboat in Zenmode"},{"content":"I found an excellent video by Rob Muhlenstein explaining the differences between Cloud, Cloud Native and DevOps. Here are the notes I wrote.\nCloud These are primarily cloud services. The external cloud.\n\u0026ldquo;Something as a Service\u0026rdquo;.\nAmazon Azure GCP Cloud Native This is Cloud Native: The CNCF Landscape\nCloud Native is the technology that makes the cloud possible, and all the technology dependent on those services.\nComputing Edge Computing\nHigh Performance Computing\nEncapsulates all of the technologies that are involved with containerization of work, jobs and nodes\nDeployment of compute resources as nodes\nThis is why Google\u0026rsquo;s Borg was called Borg\nComputers are drones of a larger collective\nEvery node puts all the resources into the collective.\nThe collective is all the nodes combined, and Kubernetes is the Borg that orchestrates everything. It sees available resources and allocates the work that needs to be done.\nBorg is the internal system developed at Google to run their infrastructure. You can read about it in the Site Reliability Engineering books and I highly recommend them.\nKubernetes is /proc for the cloud\nRob Muhlenstein\nMost Important Technologies Docker, Dockerfiles\nKubernetes\nHelm\nHarbor\nDifferent registries, harbor, quay\nIt is a lot of Python and POSIX shell\nGo for infrastructure application development\nKubernetes and Helm have won the game\nContainers: Size Matters Size matters (again) in the cloud The smaller your container the better, because it takes less resources and less costs DevOps DevOps is not the same as Cloud Native. It is one piece of it, a specific set of practices and actions that can be done within Cloud Native.\nHow you write software and release it CI/CD Focused on getting the software out GitLab has become the one stop shop Purpose is to write software and get it published fast GitOps Summary In summary, \u0026ldquo;cloud\u0026rdquo; stands for the services offered by cloud providers such as AWS, Azure and GCP. Cloud Native stands for all of the technology that makes these cloud services possible. DevOps is part of Cloud Native, but definitely not the same thing. DevOps is concerned with how software is written and released.\nLinks: 202303262003\nhttps://youtu.be/gyjRriOyw-k\nhttps://landscape.cncf.io/\nhttps://sre.google/books/\n[[rwxrob]]\n[[devops]]\n[[kubernetes]]\n[[cloud-native]]\n[[cncf]]\n","permalink":"https://mischavandenburg.com/zet/cloud-cloudnative-devops/","summary":"I found an excellent video by Rob Muhlenstein explaining the differences between Cloud, Cloud Native and DevOps. Here are the notes I wrote.\nCloud These are primarily cloud services. The external cloud.\n\u0026ldquo;Something as a Service\u0026rdquo;.\nAmazon Azure GCP Cloud Native This is Cloud Native: The CNCF Landscape\nCloud Native is the technology that makes the cloud possible, and all the technology dependent on those services.\nComputing Edge Computing\nHigh Performance Computing","title":"The Difference Between DevOps, Cloud and Cloud Native"},{"content":"It happens that I want to share my notes with friends that just simply want a pdf instead of a markdown file.\nThis morning I figured out a quick way to convert markdown to pdf on a M2 MacBook running MacOS Ventura.\nYou need the pandoc and wkhtmltopdf packages.\nbrew install pandoc wkhtmltopdf To convert:\npandoc 00-zettelkasten/Fundamentals\\ of\\ Bicep.md --pdf-engine=wkhtmltopdf -o /tmp/test.pdf This will output a pdf to my /tmp/ directory and it looks pretty good.\nTo convert all markdown files in a directory you can use a wildcard:\npandoc 00-zettelkasten/*.md --pdf-engine=wkhtmltopdf -o /tmp/test.pdf Fun fact: converting my entire zettelkasten took a few seconds and generated a document of 80 pages with a font size of 10. Pretty fun to see it in a more regular format instead of the terminal.\n","permalink":"https://mischavandenburg.com/zet/pandoc-markdown-pdf-macos/","summary":"It happens that I want to share my notes with friends that just simply want a pdf instead of a markdown file.\nThis morning I figured out a quick way to convert markdown to pdf on a M2 MacBook running MacOS Ventura.\nYou need the pandoc and wkhtmltopdf packages.\nbrew install pandoc wkhtmltopdf To convert:\npandoc 00-zettelkasten/Fundamentals\\ of\\ Bicep.md --pdf-engine=wkhtmltopdf -o /tmp/test.pdf This will output a pdf to my /tmp/ directory and it looks pretty good.","title":"Converting markdown to PDF on MacOS"},{"content":"Yesterday I reached a significant milestone. I ran my first 5K!\nI\u0026rsquo;ve always been on the heavy side. I did a lot of weightlifting from age 18 to around 27 and I was always rather bulky, but very strong. This meant I was very good at working in the woods or helping people move house, but I\u0026rsquo;ve never been able to do any running or endurance because my knees would start protesting very quickly. However, I always admired endurance athletes and I had the desire to be able to run more than a few minutes without feeling the urge to die on the spot.\nOver the past 1,5 years I lost 20kg (~44 pounds) so I decided to give it yet another try.\nI\u0026rsquo;ve never been this lightweight so I decided to give running another try. I started small with only 10 minutes but over the course of two months I built it up gradually.\nLast week I ran 4km and it felt pretty good, so I just decided to push a little bit more and try for my goal of 5k, and it went well! It feels like quite an accomplishment to me.\nNow that I lost all this weight running actually feels good, and it is liberating to discover a whole new area of exercise that I haven\u0026rsquo;t explored yet: endurance. Looking back to my progress over a couple of months, I\u0026rsquo;m curious how far I\u0026rsquo;ll come and what my limits are.\n","permalink":"https://mischavandenburg.com/zet/ran-5km/","summary":"Yesterday I reached a significant milestone. I ran my first 5K!\nI\u0026rsquo;ve always been on the heavy side. I did a lot of weightlifting from age 18 to around 27 and I was always rather bulky, but very strong. This meant I was very good at working in the woods or helping people move house, but I\u0026rsquo;ve never been able to do any running or endurance because my knees would start protesting very quickly.","title":"I ran 5K for the first time in my life!"},{"content":"Did some website housekeeping today. Spent the entire morning on a few tasks that I intended to do for a while. I added a search page and reorganized the menu, and I added a \u0026ldquo;Start Here\u0026rdquo; page.\nI hope that the new \u0026ldquo;Start Here\u0026rdquo; page will do a better job of explaining the how and why of my website, and that the search function will help you nagivate my website better.\n","permalink":"https://mischavandenburg.com/zet/website-housekeeping/","summary":"Did some website housekeeping today. Spent the entire morning on a few tasks that I intended to do for a while. I added a search page and reorganized the menu, and I added a \u0026ldquo;Start Here\u0026rdquo; page.\nI hope that the new \u0026ldquo;Start Here\u0026rdquo; page will do a better job of explaining the how and why of my website, and that the search function will help you nagivate my website better.","title":"Made Some Website Improvements"},{"content":"For weeks I\u0026rsquo;ve been debating with myself whether I should open up more on my blog. I spend my energy on two main pillars of my life: work and health, and up until now I\u0026rsquo;ve only been writing about work on my blog.\nwork I spend most of my waking hours sitting at my keyboard working on the command line: working, studying, coding, note taking. My life is focused around my career and I spend at at least 20 hours a week studying or learning new skills related to my career. When I made my career change I started to write this blog to document the process and share my learning with anyone who might find it useful\nhealth The other main pillar of my life is health. In 2017 my father suffered two very serious hard attacks in close succession, and nearly losing him really woke me up and made me focus on my health.\nI started eating a plant based diet and slowly reducing all the habits that were detrimental to my health, and increasing the activities that would promote my health. I quit drinking alcohol and I\u0026rsquo;ve been sober for nearly 5 years now.\nmental health I\u0026rsquo;m also very interested in mental health. I\u0026rsquo;ve done extensive studies of meditation and did several retreats. I\u0026rsquo;ve had a daily meditation practice going for 6 years now. I have a teacher and enjoy conversing about this topic with friends that have similar interests. I also practice yoga several times a week.\ndiet I spend a lot of time reading and researching what the optimal diet is for me. I\u0026rsquo;ve done a lot of experimentation. Not only what I eat, but also when I eat has been a subject of scrutiny for years now. I\u0026rsquo;ve been on some form of intermittent fasting protocol since 2015, and I\u0026rsquo;m currently eating only one meal a day.\nCaloric restriction has been shown to extend lifespan in many different organisms and I find it very intriguing. I\u0026rsquo;ve been on a weight loss journey since late 2021 and I find that restricting my calories gives me all sorts of benefits.\nsleep My life changed when I read Matt Walker\u0026rsquo;s book \u0026ldquo;Why We Sleep\u0026rdquo; and I\u0026rsquo;ve focused on sleep hygiene ever since. My average sleep over the last year is 7 hours a night (logged by my iWatch).\nexercise I recently ran my first 5k and I\u0026rsquo;m about to embark on a whole new exercise journey where I\u0026rsquo;ll be exploring my endurance limits.\nblog All of the topics above are interests of mine and I spent a lot of time researching them. I listen to podcasts, read books, discuss them with friends. And most importantly: I keep notes about them.\nSo if I\u0026rsquo;m happy to publish all of my work related notes with the world, why should I not share my other interests as well? I spend so much time gathering all this information and writing notes about them, and I feel that it might be useful to others who don\u0026rsquo;t wish to plough through all of the material and would like to read summaries of the information.\nThere are a few reasons why I\u0026rsquo;ve been hesitant to do so. seo I used to have websites that marketed products, and from those years of optimizing for sales I\u0026rsquo;ve gained some ingrained habits. These include keeping websites centered around one topic and repeating keywords throughout articles.\nThis blog was never intended for any sales or marketing, so I haven\u0026rsquo;t been writing with any SEO in mind which has been liberating, frankly. But I find that these ingrained habits have stopped me from broadening my scope for the blog.\ncentralization But keeping the website centered focused on one area has been a sticking point. I\u0026rsquo;ve always had the reader in mind, but also for selfish reasons. Colleagues and managers have pointed out that my blog is an excellent supplement to my CV. When I switched jobs recently, one hiring manager even offered me a job with a good salary solely based on my CV, my blog and one phone conversation.\nReaders of my blog might not necessarily be interested reading about my opinions and notes on health and exercise.\nIt\u0026rsquo;s my blog! In the end I decided to start writing more about my personal life and health interests. The whole point of starting my own website was to have the freedom of writing about what I want without limitations. I could have just written all my stuff on LinkedIn on other social media, but then you don\u0026rsquo;t own any of the content. What happens if you get banned for some reason? Or hacked? All your work is gone, which is why I chose my own website written in markdown files stored locally on my disk and hosted on GitHub.\nFrom the beginning I made sure to use a system where readers can filter on tags if they are looking for specific content. And honestly, who reads my stuff anyway? I know of one person who regularly reads my stuff (Hi, F!), but my blog is mostly a relaxing creative activity. I like scrolling through it and seeing the thoughts I captured, and to see it grow over time. And it helps me tremendously with learning information, because writing about topics is a great way to test whether I really understand the matter.\nIncluding my notes on other topics such as health and exercise will make my blog less focused and centralized around DevOps and tech, but it will reveal more about me as a person, and it will be fun to look back in the future and see my progress in these other areas as well. And I think that summaries of the research I do on these topics can actually be of use to others.\nSo I\u0026rsquo;ve decided to remove all limitations on topics for my blog and to just write about everything that I feel like publishing and sharing with the world.\n","permalink":"https://mischavandenburg.com/zet/articles/write-about-health-personal/","summary":"For weeks I\u0026rsquo;ve been debating with myself whether I should open up more on my blog. I spend my energy on two main pillars of my life: work and health, and up until now I\u0026rsquo;ve only been writing about work on my blog.\nwork I spend most of my waking hours sitting at my keyboard working on the command line: working, studying, coding, note taking. My life is focused around my career and I spend at at least 20 hours a week studying or learning new skills related to my career.","title":"Writing about Health and Personal Life"},{"content":"I do all of my writing in markdown. I keep my diaries in my Second Brain, I constantly write notes on the topics I\u0026rsquo;m studying, and my entire blog is written in markdown.\nSome of the writing is done in Obsidian but I\u0026rsquo;m moving away from Obsidian step by step. I\u0026rsquo;ll still keep my second brain compatible with it, but I want to be able to do all of my writing in (neo)vim.\nI\u0026rsquo;ve been using nvim-markdown for a while now and I was quite happy with it, but it was bothering me that bold text was not rendered in my editor.\nPandoc had been on my radar for a while because it uses the Commonmark markdown spec and it can be used to convert to many different documentation types. It\u0026rsquo;s an interesting thought to keep my entire collection of notes and writings compatible with something that can be converted into anything.\nI noticed rwxrob uses pandoc as one of his few vim plugins.\nIt took me an hour to make it compatible with neovim and to disable the spelling and folding. I couldn\u0026rsquo;t just set the provided variables because they are still in vimscript and I haven\u0026rsquo;t figured out how to set them yet because they use a hashtag in their name #\nSo far I like it, but there are a few things that I miss from my nvim-markdown\nautomatic bulleted list nested bulleted lists with nvim-markdown these lists would continue Here I have to indent each line myself and add a bullet navigation between headers I didn\u0026rsquo;t use this much to be honest, but I haven\u0026rsquo;t figured it out in pandoc yet. Maybe it is possible? I find myself using bullete lists and nested lists a lot in my notes, but I wonder if it is better to step away from those and make more use of markdown headings.\n","permalink":"https://mischavandenburg.com/zet/trying-out-pandoc/","summary":"I do all of my writing in markdown. I keep my diaries in my Second Brain, I constantly write notes on the topics I\u0026rsquo;m studying, and my entire blog is written in markdown.\nSome of the writing is done in Obsidian but I\u0026rsquo;m moving away from Obsidian step by step. I\u0026rsquo;ll still keep my second brain compatible with it, but I want to be able to do all of my writing in (neo)vim.","title":"Trying out pandoc for vim"},{"content":"[[neovim]]\nI find myself quoting words very often in vim when I\u0026rsquo;m writing bash code. I used to do this by simply navigating around the word and typing them, but I knew there had to be a better way.\nI found this vim command:\nciw\u0026quot;\u0026quot;\u0026lt;Esc\u0026gt;P\n\u0026ldquo;c\u0026rdquo; deletes into register and enters insert mode. \u0026ldquo;iw\u0026rdquo; stands for \u0026ldquo;inner word\u0026rdquo; and selects the word.\nSo we delete the entire word and enter insert mode. Then we type two quotes, and we press \u0026ldquo;P\u0026rdquo; to paste the register (containing the word) before the cursor.\nVoila, the word is surrounded by quotes.\nTo make it even easier, I added this to my keymaps, and I\u0026rsquo;ll add a few more for parentheses and brackets.\nvim.keymap.set(\u0026quot;n\u0026quot;, \u0026quot;\u0026lt;leader\u0026gt;wsq\u0026quot;, 'ciw\u0026quot;\u0026quot;\u0026lt;Esc\u0026gt;P', { desc = \u0026quot;Word Surround Quotes\u0026quot; })\nhttps://vi.stackexchange.com/questions/21113/vimscript-surround-word-under-cursor-with-quotes\n","permalink":"https://mischavandenburg.com/zet/surround-word-quotes-neovim/","summary":"[[neovim]]\nI find myself quoting words very often in vim when I\u0026rsquo;m writing bash code. I used to do this by simply navigating around the word and typing them, but I knew there had to be a better way.\nI found this vim command:\nciw\u0026quot;\u0026quot;\u0026lt;Esc\u0026gt;P\n\u0026ldquo;c\u0026rdquo; deletes into register and enters insert mode. \u0026ldquo;iw\u0026rdquo; stands for \u0026ldquo;inner word\u0026rdquo; and selects the word.\nSo we delete the entire word and enter insert mode.","title":"How to Surround a Word with Quotes in Vim"},{"content":"A module is generally associated with a single git repo.\nYou can have a module with multiple packages, and each package would get its own subdirectory.\nYou should always name your main file main.go\ncreating a module Use the go mod init {{your path here}} command to initiate a module.\nmultiple modules I was running into some trouble with this because I want to have one big repo where I will store all my go projects.\nMy gopls LSP in Neovim would start throwing errors when I added multiple projects in my repo.\nThe fix is to create a separate directory for each project. For example:\n/go/hello/main.go\n/go/hi/main.go\nHello and Hi are each separate projects.\nNow I enter each of these directories and run go mod init hello\nI\u0026rsquo;m sure this isn\u0026rsquo;t good practice for production code, but it serves its purpose to collect all my learning code in one place.\nhttps://www.youtube.com/watch?v=9hEnzD-bNy4\n","permalink":"https://mischavandenburg.com/zet/package-module-go/","summary":"A module is generally associated with a single git repo.\nYou can have a module with multiple packages, and each package would get its own subdirectory.\nYou should always name your main file main.go\ncreating a module Use the go mod init {{your path here}} command to initiate a module.\nmultiple modules I was running into some trouble with this because I want to have one big repo where I will store all my go projects.","title":"What is the difference between a Go module and a package?"},{"content":"Today was meal-prepping day and I cut up some vegetables for the coming week. I’m on a strict caloric restriction regimen and need to meticulously track and plan all the food that I consume.\nI cut up three kinds of vegetables and wrote down how many grams of each I cut up so I could divide them by three and add them to my calorie tracking application.\nAs I took out my phone to pick up my calculator to divide each number, my inner engineer started complaining about the fact that I had to do three calculations and that it would be much better to loop over an array of these values.\nI had my laptop nearby with a terminal open and wrote this instead:\nfor i in 287 252 321; do echo \u0026#34;$i / 3\u0026#34; | bc; done 95 84 107 After turning my passion into my career, I love the fact that I’m starting to think like an engineer in all my other areas of life. I’m also happy to see my progress in command line work. I chose to go the hard way, using bash instead of zsh and doing all my editing in vim and using Linux as much as possible, and it is paying off because I don’t have to think much about these little operations anymore.\nThen again, I would be much better off training my brain to become better at doing math without the aid of paper or calculators 🤓\nIf you\u0026rsquo;re curious about what a shokunin is, check out this article I wrote: https://mischavandenburg.com/zet/articles/jiro-sushi/\n","permalink":"https://mischavandenburg.com/zet/cli-shokunin-moment/","summary":"Today was meal-prepping day and I cut up some vegetables for the coming week. I’m on a strict caloric restriction regimen and need to meticulously track and plan all the food that I consume.\nI cut up three kinds of vegetables and wrote down how many grams of each I cut up so I could divide them by three and add them to my calorie tracking application.\nAs I took out my phone to pick up my calculator to divide each number, my inner engineer started complaining about the fact that I had to do three calculations and that it would be much better to loop over an array of these values.","title":"Had a CLI Shokunin Moment Today"},{"content":"I do all my coding and note taking in the terminal using tmux and neovim. I picked up a nice trick from Rob Muhlenstein today.\nYou can use this command in a split window to keep running a Go file. It will update when you save the file.\nentr -c bash -c \u0026quot;go run main.go\u0026quot; \u0026lt;\u0026lt;\u0026lt; main.go\nEntr runs commands when files change. Here we are feeding it only one file, but you can also feed it a directory like so:\nfind src/ | entr -s 'make'\nSuper handy to see the outcome of your code changes in real time.\nTo run all the files in the directory, use the following:\nentr -c bash -c \u0026quot;go run . \u0026quot; \u0026lt; \u0026lt;(find .)\nI picked this up while going through Rob\u0026rsquo;s Beginner Boost of 2022:\nhttps://youtu.be/kwrN3jbv4sE\n","permalink":"https://mischavandenburg.com/zet/running-go-on-change/","summary":"I do all my coding and note taking in the terminal using tmux and neovim. I picked up a nice trick from Rob Muhlenstein today.\nYou can use this command in a split window to keep running a Go file. It will update when you save the file.\nentr -c bash -c \u0026quot;go run main.go\u0026quot; \u0026lt;\u0026lt;\u0026lt; main.go\nEntr runs commands when files change. Here we are feeding it only one file, but you can also feed it a directory like so:","title":"How to continuously run a Go file while coding in the terminal"},{"content":"Deploying to subscriptions and management groups To tell Bicep which scope to deploy to, use the targetScope keyword, for example, managementGroup.\nYou\u0026rsquo;re not specifying which management group exactly, this is done during deployment of the template file.\ntargetScope can be set to resourceGroup, subscription, managementGroup or tenant.\nIf it is not set, Bicep assumes resourceGroup.\ncreate a resource group targetScope = \u0026#39;subscription\u0026#39; resource resourceGroup \u0026#39;Microsoft.Resources/resourceGroups@2021-01-01\u0026#39; = { name: \u0026#39;example-resource-group\u0026#39; location: \u0026#39;westus\u0026#39; } To deploy you use az deployment group create for resource groups, but you use az deployment sub create for subscriptions, mg for management group and tenant for tenant.\ndeployment scripts deploymentScripts resources are either PowerShell or Bash scripts that run in a Docker container as part of your template deployment. The default container images have either the Azure CLI or Azure PowerShell available. These scripts run during the processing of the ARM template, so you can add custom behavior to the deployment process.\nHere is an example of a deployment script with some comments:\nresource myFirstDeploymentScript \u0026#39;Microsoft.Resources/deploymentScripts@2020-10-01\u0026#39; = { name: \u0026#39;myFirstDeploymentScript\u0026#39; location: resourceGroup().location // can be AzurePowershell or Azure CLI kind: \u0026#39;AzurePowerShell\u0026#39; // the script will be run in a container. We need to provide a Managed Identity to give the script the required permissions identity: { type: \u0026#39;UserAssigned\u0026#39; userAssignedIdentities: { \u0026#39;/subscriptions/01234567-89AB-CDEF-0123-456789ABCDEF/resourcegroups/deploymenttest/providers/Microsoft.ManagedIdentity/userAssignedIdentities/myscriptingid\u0026#39;: {} } } properties: { azPowerShellVersion: \u0026#39;3.0\u0026#39; // in Bicep we use \u0026#39;\u0026#39;\u0026#39; to indicate a multi line string scriptContent: \u0026#39;\u0026#39;\u0026#39; $output = \u0026#39;Hello Learner!\u0026#39; Write-Output $output // the $DeploymentScriptOutputs variable is created to return output back to the Bicep template // It needs to be a hash table $DeploymentScriptOutputs = @{} $DeploymentScriptOutputs[\u0026#39;text\u0026#39;] = $output \u0026#39;\u0026#39;\u0026#39; retentionInterval: \u0026#39;P1D\u0026#39; } } output scriptResult string = myFirstDeploymentScript.properties.outputs.text You can also write deployment scripts in Bash. To create outputs from a Bash script, you need to create a JSON file in a location specified by the AZ_SCRIPTS_OUTPUT_PATH environment variable.\nTo include a script file, use the following:\nproperties: { azPowerShellVersion: \u0026#39;3.0\u0026#39; scriptContent: loadTextContent(\u0026#39;myscript.ps1\u0026#39;) retentionInterval: \u0026#39;P1D\u0026#39; } deploying a managed identity and assigning a role var userAssignedIdentityName = \u0026#39;configDeployer\u0026#39; var roleAssignmentName = guid(resourceGroup().id, \u0026#39;contributor\u0026#39;) var contributorRoleDefinitionId = resourceId(\u0026#39;Microsoft.Authorization/roleDefinitions\u0026#39;, \u0026#39;b24988ac-6180-42a0-ab88-20f7382dd24c\u0026#39;) resource userAssignedIdentity \u0026#39;Microsoft.ManagedIdentity/userAssignedIdentities@2018-11-30\u0026#39; = { name: userAssignedIdentityName location: resourceGroup().location } resource roleAssignment \u0026#39;Microsoft.Authorization/roleAssignments@2020-04-01-preview\u0026#39; = { name: roleAssignmentName properties: { roleDefinitionId: contributorRoleDefinitionId principalId: userAssignedIdentity.properties.principalId principalType: \u0026#39;ServicePrincipal\u0026#39; } } template specs When you have a lot of reusable templates, you can use Template Specs to enable your entire organization to deploy them.\nYou can convert a Bicep file to a template spec. The template spec is then deployed to Azure as a resource, and anybody with the right access and do deployments with the template spec from the portal or Azure CLI. Azure will handle the version control.\nYou will lose any comments and whitespace.\nBicep modules are intended to be combined into larger deployments. Template specs are for sets of resources with a certain configuration.\nTemplate specs can be used as a Bicep module. You use the following code to import it:\nmodule storageAccountTemplateSpec \u0026#39;ts:f0750bbe-ea75-4ae5-b24d-a92ca601da2c/sharedTemplates/StorageWithoutSAS:1.0\u0026#39; = { name: \u0026#39;storageAccountTemplateSpec\u0026#39; } ","permalink":"https://mischavandenburg.com/zet/advanced-bicep/","summary":"Deploying to subscriptions and management groups To tell Bicep which scope to deploy to, use the targetScope keyword, for example, managementGroup.\nYou\u0026rsquo;re not specifying which management group exactly, this is done during deployment of the template file.\ntargetScope can be set to resourceGroup, subscription, managementGroup or tenant.\nIf it is not set, Bicep assumes resourceGroup.\ncreate a resource group targetScope = \u0026#39;subscription\u0026#39; resource resourceGroup \u0026#39;Microsoft.Resources/resourceGroups@2021-01-01\u0026#39; = { name: \u0026#39;example-resource-group\u0026#39; location: \u0026#39;westus\u0026#39; } To deploy you use az deployment group create for resource groups, but you use az deployment sub create for subscriptions, mg for management group and tenant for tenant.","title":"Notes: Advanced Bicep"},{"content":"The past few days I\u0026rsquo;ve been trying out a few options to run Docker containers and a Kubernetes clusters on my new MacBook Pro M2.\nUnfortunately you can\u0026rsquo;t just run brew install docker and expect it to work. Additionally, Docker desktop requires that you purchase a license if you use it for work purposes.\nMinikube works fine as well, but the networking driver for qemu is not fully supported yet, and I haven\u0026rsquo;t tried any of the other alternatives because I found something better.\nRancher Desktop provides everything that you need. It sets up a local VM where it will run a Kubernetes cluster using k3s. It will configure the containerd container engine for you which you can interact with using nerdctl.\nTo install:\nbrew install rancher #after installing rancher, start it up and wait for it to boot the VM. alias docker=nerdctl docker run hello-world And you\u0026rsquo;re good to go. Rancher will add the rancher-desktop to your kube context.\nTo test your Kubernetes cluster:\nk get pods k get nodes # test running a pod k run nginx --image=nginx k expose pod nginx --port=80 --type=NodePort # inspect your services and look for 80:31066/TCP under PORT(S) k get svc curl localhost:31066 Or visit localhost:31066 in your browser. Replace 31066 with the port you found listed under your services.\n","permalink":"https://mischavandenburg.com/zet/docker-kubernetes-on-mac-m2/","summary":"The past few days I\u0026rsquo;ve been trying out a few options to run Docker containers and a Kubernetes clusters on my new MacBook Pro M2.\nUnfortunately you can\u0026rsquo;t just run brew install docker and expect it to work. Additionally, Docker desktop requires that you purchase a license if you use it for work purposes.\nMinikube works fine as well, but the networking driver for qemu is not fully supported yet, and I haven\u0026rsquo;t tried any of the other alternatives because I found something better.","title":"Running Docker and Kubernetes on Mac M2"},{"content":"Today I finished the Intermediate Bicep module. Here are my notes.\nChild and Extension You can also use Bicep to refer to resources that were created outside the Bicep file itself. For example, you can refer to resources that your colleagues have created manually by using the Azure portal, or from within another Bicep template or module, even if they\u0026rsquo;re in a different resource group or subscription. By using these features of Bicep, you can unlock the ability to create powerful templates that deploy all aspects of your Azure infrastructure.\nSome resources are only deployed in context of their parent. For example:\nVirtual network subnets Microsoft.Network/virtualNetworks/subnets App Service configuration Microsoft.Web/sites/config SQL databases Microsoft.Sql/servers/databases Virtual machine extensions Microsoft.Compute/virtualMachines/extensions Storage blob containers Microsoft.Storage/storageAccounts/blobServices/containers Azure Cosmos DB containers\nIt does not make sense for a container to exist without a storage account.\nDifference between child and extension In summary, you define extensions with the scope keyword, and child resources are defined by nesting them or by using the parent keyword.\nAn extension resource is a resource that modifies another resource. For example, assigning a role to a resource.\nA child resource is a resource that exists only within the context of another resource, such as a subnet existing only within a vnet.\nNested resource resource vm \u0026#39;Microsoft.Compute/virtualMachines@2020-06-01\u0026#39; = { name: vmName location: location properties: { // ... } resource installCustomScriptExtension \u0026#39;extensions\u0026#39; = { name: \u0026#39;InstallCustomScript\u0026#39; location: location properties: { // ... } } } Here the extension resource is within the vm resource. The fully qualified domain name is Microsoft.Compute/virtualMachines/extensions, but it is not necessary because it inherits it from the parent. Therefore we only need to specify \u0026rsquo;extensions\u0026rsquo; here.\nNo API version is specified either, this is also inherited.\nYou can refer to a nested resource by using the :: operator. For example, you could create an output that will return the full resource ID of the extension:\noutput childResourceId string = vm::installCustomScriptExtension.id Parent property This is the second way to declare a child resource.\nresource vm \u0026#39;Microsoft.Compute/virtualMachines@2020-06-01\u0026#39; = { name: vmName location: location properties: { // ... } } resource installCustomScriptExtension \u0026#39;Microsoft.Compute/virtualMachines/extensions@2020-06-01\u0026#39; = { parent: vm name: \u0026#39;InstallCustomScript\u0026#39; location: location properties: { // ... } } dependsOn use dependsOn to indicate a dependency.\ndependsOn: [ vm ] Extension resources Extension resources are always attached to other Azure resources. They extend them with extra functionality. Some examples are role assignments, locks, and policy assignments.\nIt doesn\u0026rsquo;t make sense to deploy a lock by itself. It always has to be deployed to another resource, because it prevents deletion or modification of a resource.\nResources are defined almost the same way as normal resources, but you add the scope property to tell Bicep that it is attached to another resource in the template.\nresource cosmosDBAccount \u0026#39;Microsoft.DocumentDB/databaseAccounts@2020-04-01\u0026#39; = { name: cosmosDBAccountName location: location properties: { // ... } } resource lockResource \u0026#39;Microsoft.Authorization/locks@2016-09-01\u0026#39; = { scope: cosmosDBAccount name: \u0026#39;DontDelete\u0026#39; properties: { level: \u0026#39;CanNotDelete\u0026#39; notes: \u0026#39;Prevents deletion of the toy data Cosmos DB account.\u0026#39; } } Extensions have slightly different resource ID\u0026rsquo;s. They consist of the parent resource ID, the separator /providers/, and the extension resource ID.\nIf you see a resource ID that starts with a normal resource ID and then adds /providers/ and another resource type and name, it means that you\u0026rsquo;re looking at an extension resource ID.\nExisting resources Bicep files often need to refer to resources that have been already created elswewhere. They might have been created in the portal or by another Bicep file.\nHere you use the existing keyword in Bicep. You are defining a resource that already exists, and therefore you are telling Bicep that it shouldn\u0026rsquo;t try to deploy it. Think of it as a placeholder resource.\nYou can do the same for nested or child resources.\nresource storageAccount \u0026#39;Microsoft.Storage/storageAccounts@2019-06-01\u0026#39; existing = { name: \u0026#39;toydesigndocs\u0026#39; } Existing resources outside of the resource group and subscription resource vnet \u0026#39;Microsoft.Network/virtualNetworks@2020-11-01\u0026#39; existing = { scope: resourceGroup(\u0026#39;f0750bbe-ea75-4ae5-b24d-a92ca601da2c\u0026#39;, \u0026#39;networking-rg\u0026#39;) name: \u0026#39;toy-design-vnet\u0026#39; } You can refer to these as long as they are within your Azure AD tenant.\nAdd child and extension resources to an existing resource resource server \u0026#39;Microsoft.Sql/servers@2020-11-01-preview\u0026#39; existing = { name: serverName } resource database \u0026#39;Microsoft.Sql/servers/databases@2020-11-01-preview\u0026#39; = { parent: server name: databaseName location: location sku: { name: \u0026#39;Standard\u0026#39; tier: \u0026#39;Standard\u0026#39; } } Use the existing keyword to refer to the resource, and then you add the child by specifying the parent property.\nFinally, to deploy an extension resource to an existing resource, use the scope keyword:\nresource storageAccount \u0026#39;Microsoft.Storage/storageAccounts@2019-06-01\u0026#39; existing = { name: \u0026#39;toydesigndocs\u0026#39; } resource lockResource \u0026#39;Microsoft.Authorization/locks@2016-09-01\u0026#39; = { scope: storageAccount name: \u0026#39;DontDelete\u0026#39; properties: { level: \u0026#39;CanNotDelete\u0026#39; notes: \u0026#39;Prevents deletion of the toy design documents storage account.\u0026#39; } } Referring to an existing resource\u0026rsquo;s properties Define the resource and you can refer to its properties if the prperty isn\u0026rsquo;t secure.\nresource applicationInsights \u0026#39;Microsoft.Insights/components@2018-05-01-preview\u0026#39; existing = { name: applicationInsightsName } resource functionApp \u0026#39;Microsoft.Web/sites@2020-06-01\u0026#39; = { name: functionAppName location: location kind: \u0026#39;functionapp\u0026#39; properties: { siteConfig: { appSettings: [ // ... { name: \u0026#39;APPINSIGHTS_INSTRUMENTATIONKEY\u0026#39; value: applicationInsights.properties.InstrumentationKey } ] } } } When you need to access secure data, use the listKeys() function.\nresource storageAccount \u0026#39;Microsoft.Storage/storageAccounts@2019-06-01\u0026#39; existing = { name: storageAccountName } resource functionApp \u0026#39;Microsoft.Web/sites@2020-06-01\u0026#39; = { name: functionAppName location: location kind: \u0026#39;functionapp\u0026#39; properties: { siteConfig: { appSettings: [ // ... { name: \u0026#39;StorageAccountKey\u0026#39; value: storageAccount.listKeys().keys[0].value } ] } } } The VScode extension will show hints to help you understand the data this function returns.\nYou need to have sufficient permissions to use the listKeys function.\nChild and extension In this example we are attaching an extension to a child resource.\nresource storageAccount \u0026#39;Microsoft.Storage/storageAccounts@2019-06-01\u0026#39; existing = { name: storageAccountName resource blobService \u0026#39;blobServices\u0026#39; existing = { name: \u0026#39;default\u0026#39; } } /* Note: Here we are attaching to blobServices which itself is a child resource. So we are attaching an extension to a child resource. */ resource storageAccountBlobDiagnostics \u0026#39;Microsoft.Insights/diagnosticSettings@2017-05-01-preview\u0026#39; = { scope: storageAccount::blobService name: storageAccountBlobDiagnosticSettingsName properties: { workspaceId: logAnalyticsWorkspace.id logs: [ { category: \u0026#39;StorageRead\u0026#39; enabled: true } { category: \u0026#39;StorageWrite\u0026#39; enabled: true } { category: \u0026#39;StorageDelete\u0026#39; enabled: true } ] } } Structuring Bicep for Collaboration configuration maps Using many different parameters can be confusing to the user of the template you\u0026rsquo;re writing. One way of solving this is by creating a config map:\n@allowed([ \u0026#39;Production\u0026#39; \u0026#39;Test\u0026#39; ]) param environmentType string = \u0026#39;Test\u0026#39; var environmentConfigurationMap = { Production: { appServicePlan: { sku: { name: \u0026#39;P2V3\u0026#39; capacity: 3 } } storageAccount: { sku: { name: \u0026#39;ZRS\u0026#39; } } } Test: { appServicePlan: { sku: { name: \u0026#39;S2\u0026#39; capacity: 1 } } storageAccount: { sku: { name: \u0026#39;LRS\u0026#39; } } } } resource appServicePlan \u0026#39;Microsoft.Web/serverfarms@2020-06-01\u0026#39; = { name: appServicePlanName location: location sku: environmentConfigurationMap[environmentType].appServicePlan.sku } Here we are taking a parameter for the environment, but create an object that contains the settings for that particular environment. Note that we are accessing it like we\u0026rsquo;d access a dictionary in python: sku: environmentConfigurationMap[environmentType].appServicePlan.sku\nNaming In Bicep, you ordinarily use camelCase capitalization style for the names of parameters, variables, and resource symbolic names.\nResource names cannot be renamed after they\u0026rsquo;re deployed in Azure.\nComments use // for single line comments, and /* */ for multi-line comments.\nWhen adding comments to JSON files, you might have to save the file as jsonc to let the code editor know that comments are allowed.\n","permalink":"https://mischavandenburg.com/zet/intermediate-bicep/","summary":"Today I finished the Intermediate Bicep module. Here are my notes.\nChild and Extension You can also use Bicep to refer to resources that were created outside the Bicep file itself. For example, you can refer to resources that your colleagues have created manually by using the Azure portal, or from within another Bicep template or module, even if they\u0026rsquo;re in a different resource group or subscription. By using these features of Bicep, you can unlock the ability to create powerful templates that deploy all aspects of your Azure infrastructure.","title":"Notes: Intermediate Bicep"},{"content":"I\u0026rsquo;ll be working with Bicep during my next contract, so I\u0026rsquo;m working through the Bicep modules on Microsoft Learn to prepare. I must say that these modules are particularly helpful. They are well structured and they provide you with free sandbox environments to practice deploying the templates you create.\nWhy Bicep? Resources in Azure are deployed by the Azure Resource Manager (ARM). These resources are JSON objects under the covers, and ARM templates are a way to generate these JSON objects. However, JSON is not really meant to be edited by humans, and the ARM templates are not very suitable for editing either. Thus, Bicep was developed to allow for a better editing experience and better readability and reusability.\nBicep templates are transpiled into JSON objects, which are sent to the Azure API to create resources with the Azure Resource Manager.\nFundamentals of Bicep Notes A parameter lets you bring in values from outside the template file. For example, if someone is manually deploying the template by using the Azure CLI or Azure PowerShell, they\u0026rsquo;ll be asked to provide values for each parameter. They can also create a parameter file, which lists all of the parameters and values they want to use for the deployment. If the template is deployed from an automated process like a deployment pipeline, the pipeline can provide the parameter values.\nA variable is defined and set within the template. Variables let you store important information in one place and refer to it throughout the template without having to copy and paste it.\ngenerating unique names Bicep has another function called uniqueString() that comes in handy when you\u0026rsquo;re creating resource names. When you use this function, you need to provide a seed value, which should be different across different deployments but consistent across all deployments of the same resources.\nparam storageAccountName string = uniqueString(resourceGroup().id) Every time you deploy the same resources, they\u0026rsquo;ll go into the same resource group. The uniqueString() function will return the same value every time. If you deploy into two different resource groups in the Azure subscription, the resourceGroup().id value will be different, because the resource group names will be different. The uniqueString() function will give different values for each set of resources. If you deploy into two different Azure subscriptions, even if you use the same resource group name, the resourceGroup().id value will be different because the Azure subscription ID will be different. The uniqueString() function will again give different values for each set of resources. combining strings Can use string interpolation to generate a unique string with a recognizable hardcoded part:\nparam storageAccountName string = 'toylaunch${uniqueString(resourceGroup().id)}'\nThis can also be handy for generating correct names. For example, storage accounts may not begin with a number.\nparameter decorators allowed parameters @allowed([ \u0026#39;nonprod\u0026#39; \u0026#39;prod\u0026#39; ]) param environmentType string The template cannot be deployed unless the nonprod or prod values are provided.\n@allowed is a parameter decorator: it gives Bicep information on what the parameter\u0026rsquo;s value needs to be.\nYou can also specify the allowed length of the parameter by using the following decorators:\n@minLength(5) @maxLength(24) param storageAccountName string You can apply multiple decorators to a parameter by putting each on a separate line.\nThese min and maxLength decorators can also be used to limit the length of an array.\nTo limit int values:\n@minValue(1) @maxValue(10) param appServicePlanInstanceCount int Finally, you can add descriptions to your parameters with the @description decorator:\n@description(\u0026#39;The locations into which this Cosmos DB account should be configured. This parameter needs to be a list of objects, each of which has a locationName property.\u0026#39;) param cosmosDBAccountLocations array if statements var storageAccountSkuName = (environmentType == \u0026#39;prod\u0026#39;) ? \u0026#39;Standard_GRS\u0026#39; : \u0026#39;Standard_LRS\u0026#39; var appServicePlanSkuName = (environmentType == \u0026#39;prod\u0026#39;) ? \u0026#39;P2V3\u0026#39; : \u0026#39;F1\u0026#39; Let\u0026rsquo;s unpack this:\n? is a ternary operator and evaluates an if/then statement. The value after ? is used if the expression is true. If it\u0026rsquo;s false, the value after : is used.\nSo here, if the environmentType is prod, the SKU is set to Standard_GRS\nObjects in Bicep You can use objects within resource definitions, within variables, or within expressions in your Bicep file.\nObjects are the same as dictionaries in python:\nparam appServicePlanSku object = { name: \u0026#39;F1\u0026#39; tier: \u0026#39;Free\u0026#39; capacity: 1 } These are called \u0026ldquo;properties\u0026rdquo; of type string and int. Note that they are line separated, not comma separated like in python.\nWhen referencing the parameter in the template, you can use dot notation to access the object properties:\nresource appServicePlan \u0026#39;Microsoft.Web/serverfarms@2022-03-01\u0026#39; = { name: appServicePlanName location: location sku: { name: appServicePlanSku.name tier: appServicePlanSku.tier capacity: appServicePlanSku.capacity } } [!important] Keep in mind that you don\u0026rsquo;t specify the type of each property within an object. However, when you use a property\u0026rsquo;s value, its type must match what\u0026rsquo;s expected. In the previous example, both the name and the tier of the App Service plan SKU must be strings.\nExample: tags param resourceTags object = { EnvironmentName: \u0026#39;Test\u0026#39; CostCenter: \u0026#39;1000100\u0026#39; Team: \u0026#39;Human Resources\u0026#39; } resource appServicePlan \u0026#39;Microsoft.Web/serverfarms@2022-03-01\u0026#39; = { name: appServicePlanName location: location tags: resourceTags sku: { name: \u0026#39;S1\u0026#39; } } resource appServiceApp \u0026#39;Microsoft.Web/sites@\u0026#39; = { name: appServiceAppName location: location tags: resourceTags kind: \u0026#39;app\u0026#39; properties: { serverFarmId: appServicePlan.id } } Here we take the tags for all the resources of the template as parameters. But we easily reuse all the tags for each resource by referencing the entire object.\nArrays Arrays are not typed in Bicep. You cannot specify that it must contain strings.\nExample:\nparam cosmosDBAccountLocations array = [ { locationName: \u0026#39;australiaeast\u0026#39; } { locationName: \u0026#39;southcentralus\u0026#39; } { locationName: \u0026#39;westeurope\u0026#39; } ] This is an array of objects, which have an locationName property each.\nAnd you would access it by:\nresource account \u0026#39;Microsoft.DocumentDB/databaseAccounts@2022-08-15\u0026#39; = { name: accountName location: location properties: { locations: cosmosDBAccountLocations } } Specifying parameter values When deploying a template file there are three options:\ndefault values command line parameter file Parameter file This is a json file. To deploy a template with a paramter file, use:\naz deployment group create \\ --template-file main.bicep \\ --parameters main.parameters.json priority The order of priority is this, from high to low priority:\nParameters specified on the command line Parameter file Default values in template Securing parameters It is best to use Managed Identities for Azure, but if you need to supply secret values to a deployment, use the @secure() decorator. These values aren\u0026rsquo;t available in the deployment logs, and they won\u0026rsquo;t be displayed on the screen when entered in the terminal.\nLoops Defined with the for keyword. Usually you iterate over an array to create multiple instances of a resource.\nCopy loops param storageAccountNames array = [ \u0026#39;saauditus\u0026#39; \u0026#39;saauditeurope\u0026#39; \u0026#39;saauditapac\u0026#39; ] resource storageAccountResources \u0026#39;Microsoft.Storage/storageAccounts@2021-09-01\u0026#39; = [for storageAccountName in storageAccountNames: { name: storageAccountName location: resourceGroup().location kind: \u0026#39;StorageV2\u0026#39; sku: { name: \u0026#39;Standard_LRS\u0026#39; } }] Notice that bicep requires \u0026ldquo;[\u0026rdquo; before the for, and a closing bracket.\ncount loops resource storageAccountResources \u0026#39;Microsoft.Storage/storageAccounts@2021-09-01\u0026#39; = [for i in range(1,4): { name: \u0026#39;sa${i}\u0026#39; location: resourceGroup().location kind: \u0026#39;StorageV2\u0026#39; sku: { name: \u0026#39;Standard_LRS\u0026#39; } }] The range function takes two arguments. The first one specifies the starting value, and the second tells Bicep the number of values you want.\nIf you use range(3,4), you will get 3, 4, 5 and 6.\naccessing the index param locations array = [ \u0026#39;westeurope\u0026#39; \u0026#39;eastus2\u0026#39; \u0026#39;eastasia\u0026#39; ] resource sqlServers \u0026#39;Microsoft.Sql/servers@2021-11-01-preview\u0026#39; = [for (location, i) in locations: { name: \u0026#39;sqlserver-${i+1}\u0026#39; location: location properties: { administratorLogin: administratorLogin administratorLoginPassword: administratorLoginPassword } }] The first value is zero, so you can add 1 to i if you want your names to be sqlserver-1, sqlserver-2 etc.\ni is used here, but you can use any value you want.\nFiltering with loops param sqlServerDetails array = [ { name: \u0026#39;sqlserver-we\u0026#39; location: \u0026#39;westeurope\u0026#39; environmentName: \u0026#39;Production\u0026#39; } { name: \u0026#39;sqlserver-eus2\u0026#39; location: \u0026#39;eastus2\u0026#39; environmentName: \u0026#39;Development\u0026#39; } { name: \u0026#39;sqlserver-eas\u0026#39; location: \u0026#39;eastasia\u0026#39; environmentName: \u0026#39;Production\u0026#39; } ] resource sqlServers \u0026#39;Microsoft.Sql/servers@2021-11-01-preview\u0026#39; = [for sqlServer in sqlServerDetails: if (sqlServer.environmentName == \u0026#39;Production\u0026#39;) { name: sqlServer.name location: sqlServer.location properties: { administratorLogin: administratorLogin administratorLoginPassword: administratorLoginPassword } tags: { environment: sqlServer.environmentName } }] This will deploy -we and -eas, but not -eus2, because the environmentName does not match Production.\nControlling loop execution By default all the iterations of a loop are executed simultaneously. However, you don\u0026rsquo;t always want this to be happening.\nTo control the amount you can use the @batchSize decorator.\n@batchSize(2) resource appServiceApp \u0026#39;Microsoft.Web/sites@2021-03-01\u0026#39; = [for i in range(1,3): { name: \u0026#39;app${i}\u0026#39; // ... }] Here bicep will wait for the first two to be fully completed before it moves to the next.\nTo loop sequentially, meaning one at a time in order, use @batchSize(1)\nVariable loops You can use loops to create arrays that you can use in the Bicep template.\nvar items = [for i in range(1, 5): \u0026#39;item${i}\u0026#39;] This produces an array containing the values item1, item2 up to 5 stored in the items variable.\nReminds me of list comprehensions in python.\nHere is an example:\nparam addressPrefix string = \u0026#39;10.10.0.0/16\u0026#39; param subnets array = [ { name: \u0026#39;frontend\u0026#39; ipAddressRange: \u0026#39;10.10.0.0/24\u0026#39; } { name: \u0026#39;backend\u0026#39; ipAddressRange: \u0026#39;10.10.1.0/24\u0026#39; } ] var subnetsProperty = [for subnet in subnets: { name: subnet.name properties: { addressPrefix: subnet.ipAddressRange } }] resource virtualNetwork \u0026#39;Microsoft.Network/virtualNetworks@2021-08-01\u0026#39; = { name: \u0026#39;teddybear\u0026#39; location: resourceGroup().location properties:{ addressSpace:{ addressPrefixes:[ addressPrefix ] } subnets: subnetsProperty } } The content of the subnetsProperty array would look like this:\n[ { name: \u0026#39;frontend\u0026#39;, properties: { addressPrefix: \u0026#39;10.10.0.0/24\u0026#39; } }, { name: \u0026#39;backend\u0026#39;, properties: { addressPrefix: \u0026#39;10.10.1.0/24\u0026#39; } } ] Output loops To output the contents of the array:\nvar items = [ \u0026#39;item1\u0026#39; \u0026#39;item2\u0026#39; \u0026#39;item3\u0026#39; \u0026#39;item4\u0026#39; \u0026#39;item5\u0026#39; ] output outputItems array = [for i in range(0, length(items)): items[i]] Modules You can create modules in Bicep so the code becomes reusable. You can share the modules with other teams and use them for different outcomes.\nGenerally, it\u0026rsquo;s not a good practice to create a module for every resource that you deploy. A good Bicep module typically defines multiple related resources. However, if you have a particularly complex resource with a lot of configuration, it might make sense to create a single module to encapsulate the complexity. This approach keeps your main templates simple and uncluttered.\nSo for example it would make sense to write a networking module and a database module that handles these resources.\nModules can be nested, but it can quickly become very complex.\nTo call a module in a template:\nmodule appModule \u0026#39;modules/app.bicep\u0026#39; = { name: \u0026#39;myApp\u0026#39; params: { location: location appServiceAppName: appServiceAppName environmentType: environmentType } } The modules are stored in the modules folder in your root directory.\nParameters Modules will take parameters, but it is good practice to leave out default values for parameters in modules. In templates it\u0026rsquo;s good practice to add defaults wherever you can. Therefore it is best to leave them out in modules because templates usually have their own default values. This can get confusing if you have similar default values in the templates and modules.\nModule dependency Bicep will figure out automatically if there is a dependency between modules. For example:\n@description(\u0026#39;Username for the virtual machine.\u0026#39;) param adminUsername string @description(\u0026#39;Password for the virtual machine.\u0026#39;) @minLength(12) @secure() param adminPassword string module virtualNetwork \u0026#39;modules/vnet.bicep\u0026#39; = { name: \u0026#39;virtual-network\u0026#39; } module virtualMachine \u0026#39;modules/vm.bicep\u0026#39; = { name: \u0026#39;virtual-machine\u0026#39; params: { adminUsername: adminUsername adminPassword: adminPassword subnetResourceId: virtualNetwork.outputs.subnetResourceId } } Here the virtualMachine module takes the subnetResourceId from the virtualNetwork module outputs.\nBecause it is defined like this, Bicep will wait with deploying the virtualMachine modul until the virtualNetwork module is finished, and pass in the required parameter.\nIt is important to note that this means that it will wait until the virtualNetwork module is completely finished. If it takes a long time to deploy the previous module, all the subsequent modules will have to wait until it\u0026rsquo;s finished.\n","permalink":"https://mischavandenburg.com/zet/articles/fundamentals-of-bicep/","summary":"I\u0026rsquo;ll be working with Bicep during my next contract, so I\u0026rsquo;m working through the Bicep modules on Microsoft Learn to prepare. I must say that these modules are particularly helpful. They are well structured and they provide you with free sandbox environments to practice deploying the templates you create.\nWhy Bicep? Resources in Azure are deployed by the Azure Resource Manager (ARM). These resources are JSON objects under the covers, and ARM templates are a way to generate these JSON objects.","title":"Notes: Fundamentals of Bicep"},{"content":"The CTO of my new company recommended the Udemy course \u0026ldquo;Go: The Complete Developer\u0026rsquo;s Guide (Golang)\u0026rdquo;. I started today and here are some notes I made.\nHello World in Go We start by writing a Hello World and studying all the elements.\npackage main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;Hello World!\u0026#34;) } How do we run code? go run main.go runs the program go build main.go compiles it to an executable\nWhat does package main mean? package main\nA package is a collection of common source code files.\nOne app is a package. If you have multiple files in a folder, such as helper.go or support.go, they should have package main to indicate that they belong to package main.\n*Why do we call it main?\nThere are two types of packages.\nexecutable generates file that can be run reusable used as \u0026ldquo;helpers\u0026rdquo; reusable logic When you call the package main, you are telling the compiler it needs to be compiled as an executable. If it has a different name, it won\u0026rsquo;t generate an executable. Main is sacred.\nAny other name is a reusable or dependency type package (helper code).\nAnother important point is that whenever you create an executable package, it must always have a func called \u0026lsquo;main\u0026rsquo;.\nWhat does import fmt mean? The import statement is used to give our package access to code written in another package. You are saying \u0026ldquo;give access to all code in fmt\u0026rdquo;. Fmt is a standard library package included in Go. Short for format. Used to print out information to the terminal.\nOther packages included in the standard library of go are debug, math, encoding, crypto, io.\ngolang.org/pkg for documentation on standard library packages for Go.\nA lot of learning go is learning the standard packages and how they work.\nOrganizing the main.go file It is the same for every go file, just like the code example at the top of the page. Package main, import fmt, and func main.\nvariable declarations var card string = \u0026quot;Ace of Spades\u0026quot;\nvar: we are about to create a new variable\ncard: name\nstring = telling the go compiler that only strings will be assigned to this variable\nAlternatively: card := \u0026quot;Ace of Spades\u0026quot;\nHere you are relying on the compiler to figure out what type it is.\nCompiler will infer the type.\nWe only use this := assignment for new variables\nIf you want to assign a value to a variable after it is declared, you just do card = \u0026quot;Five of Diamonds\u0026quot;\nGo types Go is a statically typed language.\nJavascript, python are dynamically typed language. We don\u0026rsquo;t care what value is assigned to a variable.\nYou always a assign a type to a variable in Go.\nBasic go types:\nbool string into float64 : a number with a decimal after it. ","permalink":"https://mischavandenburg.com/zet/go-day-1/","summary":"The CTO of my new company recommended the Udemy course \u0026ldquo;Go: The Complete Developer\u0026rsquo;s Guide (Golang)\u0026rdquo;. I started today and here are some notes I made.\nHello World in Go We start by writing a Hello World and studying all the elements.\npackage main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;Hello World!\u0026#34;) } How do we run code? go run main.go runs the program go build main.go compiles it to an executable","title":"Learning Go Day 1: Notes"},{"content":"For month\u0026rsquo;s I considered to learn Go, and today I finally started.\nI doubted for a long time, because I figured it would be better to dive deeper into Python. However, when I reflected on it, I realized I\u0026rsquo;m able to do the things I want to do in Python. I can create scripts to manipulate data, and I can string different tools and libraries together if I need to. So how deep do I actually need to go as a DevOps Engineer?\nThe breakthrough came when I discovered this website: Good First Issue. It\u0026rsquo;s a collection of issues in open source projects which are suitable for people who are starting out. Scrolling through the available projects for Python and Go, I noticed I got a lot more energy from the things I saw in the Go list: Grafana, ArgoCD, Kubernetes. I probably have a long way to go before I can make any meaningful contributions, but my goal is to be able to contribute to open source in the future.\nAdditionally, I\u0026rsquo;m going to focus my career in the direction of Kubernetes and CNCF, where Go is mostly used.\nI have a week off before I start my new job so I\u0026rsquo;m going to spend a few hours a day to get a good start, and when I\u0026rsquo;m working full time again, my plan is to spend at least one pomodoro (25 minutes) a day on Go. Small increments will add up to something big over time.\nI created a new GitHub repo where I\u0026rsquo;ll be uploading all my practice code and projects.\n","permalink":"https://mischavandenburg.com/zet/start-learning-go/","summary":"For month\u0026rsquo;s I considered to learn Go, and today I finally started.\nI doubted for a long time, because I figured it would be better to dive deeper into Python. However, when I reflected on it, I realized I\u0026rsquo;m able to do the things I want to do in Python. I can create scripts to manipulate data, and I can string different tools and libraries together if I need to. So how deep do I actually need to go as a DevOps Engineer?","title":"I started to learn Go!"},{"content":"I\u0026rsquo;m a little obsessed with controlling everything with my keyboard. That\u0026rsquo;s why I loved AwesomeWM so much on my Arch Linux setup, I hardly used my mouse anymore.\nOne thing I loved about my setup was the ability to control my music from the keyboard from anywhere. This is a feature I picked up from the awesome-copycats theme for AwesomeWM. This was one of the first things I missed when I made my switch to MacOS.\nI started using Apple Music as my music app but it does not have any global hotkeys, and it makes you use a widget with the mouse.\nskhd I solved the problem using skhd. This is a free hotkey daemon for MacOS. To install:\nbrew install koekeishiya/formulae/skhd brew services start skhd Then I added the following to my skhd config file:\n# Control apple music globally # Based on Aesome-copycats theme for AesomeWM ctrl + cmd + fn - up : osascript -e \u0026#39;tell app \u0026#34;Music\u0026#34; to playpause\u0026#39; ctrl + cmd + fn - left : osascript -e \u0026#39;tell app \u0026#34;Music\u0026#34; to back track\u0026#39; ctrl + cmd + fn - right : osascript -e \u0026#39;tell app \u0026#34;Music\u0026#34; to play next track\u0026#39; links https://github.com/koekeishiya/skhd\n","permalink":"https://mischavandenburg.com/zet/apple-music-hotkeys/","summary":"I\u0026rsquo;m a little obsessed with controlling everything with my keyboard. That\u0026rsquo;s why I loved AwesomeWM so much on my Arch Linux setup, I hardly used my mouse anymore.\nOne thing I loved about my setup was the ability to control my music from the keyboard from anywhere. This is a feature I picked up from the awesome-copycats theme for AwesomeWM. This was one of the first things I missed when I made my switch to MacOS.","title":"Controlling Apple Music with hotkeys from anywhere on MacOS"},{"content":"My new employer kindly sponsored me with a new 16-inch MacBook Pro M2. I chose 32GB Ram and the base model with 12 CPU and 19 GPU cores and 1TB of hard disk. These specifications will suit my needs well.\nI\u0026rsquo;ve been daily driving Linux for a year now, and it has been such an incredible journey. Building my own Arch Linux system taught me much about Linux and what it means to maintain a system yourself. I will undoubtedly miss my custom AwesomeWM configuration and the freedom that Arch provides.\nTechnically I could have chosen a Windows laptop and installed Linux on it. But I went for a Mac for the following reasons:\nCompatibility. I\u0026rsquo;ll be doing contracts for various customers, so I\u0026rsquo;ll likely run into environments where I\u0026rsquo;ll need to use some software, such as a VPN that is not supported on Linux. MacOS will always be compatible everywhere. Portability. I\u0026rsquo;ll probably be going to the office multiple times a week, and using a MacBook as a daily driver will allow me to take my full setup with me everywhere. I also plan to spend some time abroad, and using a MacBook as a daily driver will allow me to work remotely for extended periods. Stability. Even though my Linux systems never really broke on me, there is always a possibility. I don\u0026rsquo;t have to worry about that with MacOS. Even though I\u0026rsquo;ll be switching to my MacBook as my primary workstation, I will continue using Linux for my hobby projects and home lab. So there will be plenty of opportunity to keep growing in that area in the future.\n","permalink":"https://mischavandenburg.com/zet/move-to-macos/","summary":"My new employer kindly sponsored me with a new 16-inch MacBook Pro M2. I chose 32GB Ram and the base model with 12 CPU and 19 GPU cores and 1TB of hard disk. These specifications will suit my needs well.\nI\u0026rsquo;ve been daily driving Linux for a year now, and it has been such an incredible journey. Building my own Arch Linux system taught me much about Linux and what it means to maintain a system yourself.","title":"New Laptop: Moving to MacOS"},{"content":"Last week I accepted a job offer! I\u0026rsquo;ve been hired as a DevOps Engineer by Fullstaq.\nFullstaq specialises in Kubernetes and Cloud Native, so I\u0026rsquo;m really happy I could join their team. I\u0026rsquo;ll be doing projects for a variety of clients, and I think it will be a great place to learn and gain more experience in the Kubernetes and CNCF space.\nFullstaq has been delightfully welcoming and great to work with so far. The future looks bright!\n","permalink":"https://mischavandenburg.com/zet/new-job-2023/","summary":"Last week I accepted a job offer! I\u0026rsquo;ve been hired as a DevOps Engineer by Fullstaq.\nFullstaq specialises in Kubernetes and Cloud Native, so I\u0026rsquo;m really happy I could join their team. I\u0026rsquo;ll be doing projects for a variety of clients, and I think it will be a great place to learn and gain more experience in the Kubernetes and CNCF space.\nFullstaq has been delightfully welcoming and great to work with so far.","title":"New job!"},{"content":"This guide uses the official getting started guide with a few modifications. This installation is only for lab purposes. Running ArgoCD in a production environment requires more configuration.\nInstall argocd and argocd cli kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml My VM is running on arm architecture, so I need these commands to install the argocd cli on ubuntu.\ncurl -sSL -o argocd-linux-arm64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-arm64 sudo install -m 555 argocd-linux-arm64 /usr/local/bin/argocd rm argocd-linux-arm64 Change the service type to LoadBalancer\nkubectl patch svc argocd-server -n argocd -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;LoadBalancer\u0026#34;}}\u0026#39; Retrieve your passsword\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d; echo Find out which port argocd-server is running on\nk get svc -A Look for the argocd-server and see where port 80 is mapped to. In my case, it is 80:31372.\nOpen this port in your network security group for your VM, and you should be able to log in on ArgoCD in the browser by entering the VM ip followed by the port:\nhttp://143.44.179.11:31372\nLinks https://argo-cd.readthedocs.io/en/stable/getting_started/\n","permalink":"https://mischavandenburg.com/zet/articles/lab-vm-install-argocd/","summary":"This guide uses the official getting started guide with a few modifications. This installation is only for lab purposes. Running ArgoCD in a production environment requires more configuration.\nInstall argocd and argocd cli kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml My VM is running on arm architecture, so I need these commands to install the argocd cli on ubuntu.\ncurl -sSL -o argocd-linux-arm64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-arm64 sudo install -m 555 argocd-linux-arm64 /usr/local/bin/argocd rm argocd-linux-arm64 Change the service type to LoadBalancer","title":"Lab VM project - Install ArgoCD to your Kubernetes cluster"},{"content":"You can get a free 24GB ram VM from Oracle. What better place for your own Kubernetes lab that is always available? See this article to create your VM.\nHere are the steps I took to install a single node kubernetes cluster on the Ubuntu VM.\nInstallation sudo apt-get update sudo apt install apt-transport-https curl Install containerd\nsudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get update sudo apt-get install containerd.io Remove the default containerd configuration, because it creates errors when running kubeadm init.\nsudo rm -f /etc/containerd/config.toml sudo systemctl status containerd.service Install Kubernetes\nsudo curl -fsSLo /etc/apt/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg echo \u0026#34;deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\u0026#34; | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo apt install kubeadm kubelet kubectl kubernetes-cni Avoid the error \u0026ldquo;/proc/sys/net/bridge/bridge-nf-call-iptables does not exist\u0026rdquo; on kubeinit (reference https://github.com/kubernetes/kubeadm/issues/1062).\nsudo modprobe br_netfilter sudo echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward Start the cluster Initialize the Kubernetes cluster for use with Flannel\nsudo kubeadm init --pod-network-cidr=10.244.0.0/16 Copy to config as kubadm command says\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Usually you wouldn\u0026rsquo;t run pods on your control-plane node. However, since we are running a lab environment on a single VM, it\u0026rsquo;s ok. To be able to schedule pods on the control-plane node, we need to remove the NoSchedule taint:\nkubectl taint node instance-20230205-0909 node-role.kubernetes.io/control-plane:NoSchedule- Add a Container Networking Interface Install Flannel to the cluster (reference https://github.com/flannel-io/flannel)\nkubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml Configure the server firewall We use Uncomplicated Firewall. Run these commands:\nsudo ufw allow 22 sudo ufw allow 6443/tcp sudo ufw allow 2379:2380/tcp sudo ufw allow 10250/tcp sudo ufw allow 10259/tcp sudo ufw allow 10257/tcp sudo ufw enable sudo ufw status Set up bashrc Next, edit your bashrc with vim ~/.bashrc and add these lines:\nsource \u0026lt;(kubectl completion bash) alias k=kubectl complete -o default -F __start_kubectl k Then run source ~/.bashrc\nThis configures autocompletion for kubectl, and sets up \u0026ldquo;k\u0026rdquo; as an alias for kubectl.\nLet\u0026rsquo;s run a pod! To see all pods running on your cluster:\nk get pods -A\nNow let\u0026rsquo;s run a simple nginx pod and expose it:\nk run nginx --image=nginx k expose pod nginx --port=80 --type=NodePort To find out which port it\u0026rsquo;s running on, run k get service. In the PORT(S) column, there will be an nginx service exposing port 80 to a random port on the node in the range of 30000-32767.\nIn my case, it says \u0026ldquo;80:31878/TCP\u0026rdquo;\nTo see if we can reach the container, run:\ncurl localhost:31878\nIf everything went well, you will get back the HTML of the default index page served by NGINX:\nubuntu@instance-20230205-0909:~$ curl localhost:31878 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; To reach the pod from the browser, open your port in the security group configured for the subnet of your VM.\nGood luck with your new lab environment!\nLinks https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/\nhttps://kubernetes.io/docs/concepts/services-networking/service/\nhttps://github.com/flannel-io/flannel\n","permalink":"https://mischavandenburg.com/zet/articles/simple-cluster-on-ubuntu-vm/","summary":"You can get a free 24GB ram VM from Oracle. What better place for your own Kubernetes lab that is always available? See this article to create your VM.\nHere are the steps I took to install a single node kubernetes cluster on the Ubuntu VM.\nInstallation sudo apt-get update sudo apt install apt-transport-https curl Install containerd\nsudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.","title":"Setting up a Kubernetes cluster on an Ubuntu 20.04 VM with containerd and flannel"},{"content":"A few weeks ago someone gave me a tip. Oracle actually has a really good free tier offering.\nYou can host a 4CPU 24GB VM for free!\nThis is perfect for a lab environment.\nI spent my evening creating the VM and setting up a kubernetes cluster from scratch.\nUse this video to claim your free vm:\nhttps://www.youtube.com/watch?v=NKc3k7xceT8\n","permalink":"https://mischavandenburg.com/zet/free-oracle-vm/","summary":"A few weeks ago someone gave me a tip. Oracle actually has a really good free tier offering.\nYou can host a 4CPU 24GB VM for free!\nThis is perfect for a lab environment.\nI spent my evening creating the VM and setting up a kubernetes cluster from scratch.\nUse this video to claim your free vm:\nhttps://www.youtube.com/watch?v=NKc3k7xceT8","title":"Get a free 4 CPU 24GB Ram VM on from Oracle"},{"content":"Today I added a harddisk I had lying around because I needed some more space. On my Arch Linux system I have all my drives encrypted like a good boy. It can be a bit tricky when you are adding them because you need to configure a few different files and add different UUID\u0026rsquo;s in each of them.\nHere are the steps I follow to add a new disk. Note that this how to assumes that you already have set up your system with dm-crypt.\nList out the disks with lsblk:\n(ins)[mischa@arch-beast ~]$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 223.6G 0 disk └─sda1 8:1 0 223.6G 0 part └─games 254:0 0 223.6G 0 crypt /games sdb 8:16 0 931.5G 0 disk └─sdb1 8:17 0 931.5G 0 part └─data-hdd 254:2 0 931.5G 0 crypt /data-hdd sdc 8:32 0 931.5G 0 disk └─sdc1 8:33 0 931.5G 0 part └─data-hdd2 254:3 0 931.5G 0 crypt /data-hdd2 sdd 8:48 0 465.8G 0 disk ├─sdd1 8:49 0 300M 0 part /boot └─sdd2 8:50 0 465.5G 0 part └─root 254:1 0 465.5G 0 crypt / sde 8:64 0 931.5G 0 disk └─sde1 8:65 0 931.5G 0 part I will be adding /dev/sde to my system. As you see, I already created a partition on it, named sde1. The mountpoint for the disk will be /data-hdd3.\nIf you still need to add your partition, use sudo gdisk /dev/sde to write a new table and partition.\nencryption First I create the mount point I\u0026rsquo;ll use and set the appropriate permisssions:\nsudo mkdir /data-hdd3 sudo chown mischa:mischa /data-hdd3 Now we create a LUKS header and an encrypted filesystem on the disk. Note that I\u0026rsquo;m using the notation convention from the Arch Wiki where the \u0026ldquo;#\u0026rdquo; indicates that the command should be run as root.\n# cryptsetup -y -v luksFormat /dev/sde1 # cryptsetup open /dev/sde1 data-hdd3 # mkfs.ext4 /dev/mapper/data-hdd3 # mount /dev/mapper/data-hdd3 /data-hdd3 Verify that it worked and the new encrypted partition is mounted:\narch-beast# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 223.6G 0 disk └─sda1 8:1 0 223.6G 0 part └─games 254:0 0 223.6G 0 crypt /games sdb 8:16 0 931.5G 0 disk └─sdb1 8:17 0 931.5G 0 part └─data-hdd 254:2 0 931.5G 0 crypt /data-hdd sdc 8:32 0 931.5G 0 disk └─sdc1 8:33 0 931.5G 0 part └─data-hdd2 254:3 0 931.5G 0 crypt /data-hdd2 sdd 8:48 0 465.8G 0 disk ├─sdd1 8:49 0 300M 0 part /boot └─sdd2 8:50 0 465.5G 0 part └─root 254:1 0 465.5G 0 crypt / sde 8:64 0 931.5G 0 disk └─sde1 8:65 0 931.5G 0 part └─data-hdd3 254:4 0 931.5G 0 crypt /data-hdd3 Auto mounting at boot We\u0026rsquo;ll need to add this disk to the kerenel parameters, /etc/crypttab and /etc/fstab. I haven\u0026rsquo;t gotten round to switching to systemd boot yet, but I will do so very soon.\nOpen tmux and split the pane. In the bottom pane, run lsblk -f to have all the UUIDs listed. Then open the grub configuration file with sudoedit /etc/default/grub\nYou can discern which uuid to add from the listed examples. For my new disk, I needed to add the following:\nrd.luks.name=3169af6c-a129-448e-b451-d7767866f607 data-hdd3=/dev/mapper/data-hdd3\nThen run sudo grub-mkconfig -o /boot/grub/grub.cfg to update grub with the new settings. Adjust the path if you use a different path for your boot partition!\nNext, we add it to /etc/crypttab\nTo mount the new encrypted partition at boot, we add it to /etc/fstab.\nNote that this time we need to use the UUID of the partition located at /dev/mapper/data-hdd3\nUse sudo findmnt --verify to check if there is antyhing wrong with the file.\nNow you should be able to reboot and your new encrypted disk should be mounted automatically.\n(ins)[mischa@arch-beast ~]$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 223.6G 0 disk └─sda1 8:1 0 223.6G 0 part └─games 254:1 0 223.6G 0 crypt /games sdb 8:16 0 931.5G 0 disk └─sdb1 8:17 0 931.5G 0 part └─data-hdd 254:0 0 931.5G 0 crypt /data-hdd sdc 8:32 0 931.5G 0 disk └─sdc1 8:33 0 931.5G 0 part └─data-hdd2 254:4 0 931.5G 0 crypt /data-hdd2 sdd 8:48 0 465.8G 0 disk ├─sdd1 8:49 0 300M 0 part /boot └─sdd2 8:50 0 465.5G 0 part └─root 254:2 0 465.5G 0 crypt / sde 8:64 0 931.5G 0 disk └─sde1 8:65 0 931.5G 0 part └─data-hdd3 254:3 0 931.5G 0 crypt /data-hdd3 links https://wiki.archlinux.org/title/Dm-crypt/Encrypting_an_entire_system#LUKS_on_a_partition\nhttps://wiki.archlinux.org/title/GRUB\n","permalink":"https://mischavandenburg.com/zet/articles/new-luks-encrypted-disk/","summary":"Today I added a harddisk I had lying around because I needed some more space. On my Arch Linux system I have all my drives encrypted like a good boy. It can be a bit tricky when you are adding them because you need to configure a few different files and add different UUID\u0026rsquo;s in each of them.\nHere are the steps I follow to add a new disk. Note that this how to assumes that you already have set up your system with dm-crypt.","title":"Setting up a new LUKS encrypted disk with dm-crypt in Arch Linux"},{"content":"For the past few months I\u0026rsquo;ve been stuyding every hour of free time that I had. Now that I reached my certification goals for now, I finally had some time to do a chore I had been meaning to do for a long time.\nMy Arch Linux system is fully encrypted, and I make backups. But I was still doing it a bit haphazardly, usually every Friday.\nI wanted to automate this for a long time now, but I never got round to it. Today I made the first steps, but it is still in progress.\nNaturally, I could use a tool like Timeshift or something similar to schedule my backups. However, I want to do it myself using rsync because I want to fully understand what I am backing up, when, and where. Rsync is also used in our environment at work, so I assume it is more common in enterprise and production environments.\nfull system backup Before I was making a full system backup every Friday using this command:\nsudo rsync -aAXH --info=stats1,progress2 --exclude={\u0026#34;/dev/*\u0026#34;,\u0026#34;/proc/*\u0026#34;,\u0026#34;/sys/*\u0026#34;,\u0026#34;/tmp/*\u0026#34;,\u0026#34;/run/*\u0026#34;,\u0026#34;/mnt/*\u0026#34;,\u0026#34;/media/*\u0026#34;,\u0026#34;/lost+found\u0026#34;,\u0026#34;/home/*/.cache/*\u0026#34;,\u0026#34;/data-hdd/\u0026#34;,\u0026#34;/games/\u0026#34;,\u0026#34;/var/lib/docker/*\u0026#34;,\u0026#34;/home/mischa/music/*\u0026#34;,\u0026#34;/swapfile\u0026#34;, \u0026#34;/data-hdd2/\u0026#34;, \u0026#34;/data-hdd3/\u0026#34;} / /data-hdd/backups/arch-beast/01-01-23 This command creates a full backup of my entire root filesystem, and it should be possible to restore my entire system by just reversing the target and destination in the end.\nHowever, as I was coming up with my new strategy, I thought this was overkill.\nslimming down All I really need to back up is my home directory and it would be nice to have my /etc directory backed up as well.\nSo I wrote a simple shell script to do this:\n#!/bin/bash BACKUPS_DESTINATION=\u0026#34;/data-hdd/backups/arch-beast\u0026#34; # format: # rsync -a --delete --quiet /path/to/backup /location/of/backup # stop the script if an error occurs set -e rsync -a --delete --quiet --exclude=\u0026#34;{\u0026#34;/home/*/.cache/*\u0026#34;}\u0026#34; /home/mischa $BACKUPS_DESTINATION/home rsync -a --delete --quiet /etc $BACKUPS_DESTINATION echo \u0026#34;Made backups on: $(date)\u0026#34; \u0026gt;\u0026gt; /var/log/backup.log -a flag from man page:\n\u0026ldquo;This is equivalent to -rlptgoD. It is a quick way of saying you want recursion and want to preserve almost everything.\u0026rdquo;\n\u0026ndash;delete: means files deleted on the source are to be deleted on the backup as well\nautomation I have a few scripts running in cronjobs on my system. I have a goal of putting them all in systemd timers, but I haven\u0026rsquo;t gotten round to it yet. For now, I will just add my backup scripts to my existing cronjobs setup.\nTo make my backups every day, I added this to my crontab:\n0 12 * * * /bin/bash /home/mischa/git/lab/bash/backup\nEvery day it will make a backup to the same directory and update the changed files, or delete the files I deleted from my system.\nI also wanted to have a weekly backup happening on Monday.\nI will make a more elaborate script to make a weekly directory, and rotate it with a new directory every week. But for now, I just chose a quick solution by creating a weekly version of my script and running it every Monday.\nThe only difference is the path:\nBACKUPS_DESTINATION=\u0026quot;/data-hdd/backups/arch-beast/weekly\u0026quot;\nIn the crontab:\n0 10 * * 1 /bin/bash /home/mischa/git/lab/bash/backup-weekly\nto do set up weekly backup in the same script create error handling and improve logging set up in systemd timers instead of crontab ","permalink":"https://mischavandenburg.com/zet/arch-backup-setup-1/","summary":"For the past few months I\u0026rsquo;ve been stuyding every hour of free time that I had. Now that I reached my certification goals for now, I finally had some time to do a chore I had been meaning to do for a long time.\nMy Arch Linux system is fully encrypted, and I make backups. But I was still doing it a bit haphazardly, usually every Friday.\nI wanted to automate this for a long time now, but I never got round to it.","title":"Setting up automated backups on my Arch Linux system with rsync and bash"},{"content":"I\u0026rsquo;m typing this 30 minutes after I passed my AZ-400 exam. I\u0026rsquo;m sitting in a lovely cafe on Leidseplein in Amsterdam and feel relieved. Another significant certification bites the dust. This one took about 70 hours of study.\nI started preparing immediately after passing my AZ-104 exam, which was a good move. The AZ-400 requires you to know many details about Azure services and how to access them. For example, Shared Access Signatures are only used for accessing storage accounts, but they came up quite often as alternative answers to the questions.\nThe exam itself was difficult, but the AZ-104 was harder. The AZ-104 exam was more challenging because the questions were complicated and required you to simultaneously balance many different factors in the mind. The AZ-400 was difficult because the answer alternatives that are provided are incredibly similar to each other, and they make you very insecure about what the right choice might be. As a result, I changed my answers many times.\nI will do another study guide for this certification soon and publish my notes and Anki deck too. Now it\u0026rsquo;s time to celebrate and relax a little.\n","permalink":"https://mischavandenburg.com/zet/passed-az-400/","summary":"I\u0026rsquo;m typing this 30 minutes after I passed my AZ-400 exam. I\u0026rsquo;m sitting in a lovely cafe on Leidseplein in Amsterdam and feel relieved. Another significant certification bites the dust. This one took about 70 hours of study.\nI started preparing immediately after passing my AZ-104 exam, which was a good move. The AZ-400 requires you to know many details about Azure services and how to access them. For example, Shared Access Signatures are only used for accessing storage accounts, but they came up quite often as alternative answers to the questions.","title":"I passed the AZ-400 DevOps Expert today"},{"content":"The PAT (Personal Access Token) often comes up during practice tests for the AZ-400.\nOne way to remember when to use a PAT is that these are only for authenticating into Azure DevOps, never to external services.\nFor example, you might get a question on connecting your Azure DevOps project with a GitHub account from Azure DevOps, and PAT will show up as one of the alternative answers. By remembering that PATs are only for authenticating into ADO, you can elminate this alternative, and make your choice easier.\nPersonal Access Tokens are an alternative to passwords but should be treated in exactly the same way.\nhttps://learn.microsoft.com/en-us/azure/devops/organizations/accounts/use-personal-access-tokens-to-authenticate?view=azure-devops\u0026amp;tabs=Windows\n","permalink":"https://mischavandenburg.com/zet/azure-personal-access-tokens/","summary":"The PAT (Personal Access Token) often comes up during practice tests for the AZ-400.\nOne way to remember when to use a PAT is that these are only for authenticating into Azure DevOps, never to external services.\nFor example, you might get a question on connecting your Azure DevOps project with a GitHub account from Azure DevOps, and PAT will show up as one of the alternative answers. By remembering that PATs are only for authenticating into ADO, you can elminate this alternative, and make your choice easier.","title":"Azure DevOps Personal Access Tokens are always for authenticating into ADO"},{"content":"I did a lot of studying last year, and I achieved a few tough certifications. I\u0026rsquo;ve always been good at studying and never struggled with getting decent grades in university. As a result, I never felt the need to use particular techniques to pass my tests. However, now that I need to do my studies combined with a full-time job, I did some optimization and looked into study techniques.\nOne technique I\u0026rsquo;ve become very fond of is the Pomodoro Technique. I don\u0026rsquo;t have any problems focusing for long periods, but I still decided to try it. I use the standard 25-minute study with a 5-minute break routine, and after four cycles, I take a 30-minute break.\nThe Pomodoro technique has been a way to force myself to take breaks, which I wasn\u0026rsquo;t used to. I used to chip away at a specific task for hours. However, I discovered that when I take a break, walk around for five minutes, and apply myself to the task again, my mind is in a fresh state and much more receptive to the information. Perhaps the time I spend studying after a break is actually more productive because the mind had a little rest.\nThe technique also pushed my limits a bit more. I study more hours a day, considering that I also work full time. There is this moment where I want to quit studying, but I ask myself, \u0026ldquo;do I have another Pomodoro in me?\u0026rdquo;\nNow that I\u0026rsquo;ve gotten used to breaking things up into 25-minute chunks of time, I started using the Pomodoro technique for other areas in life as well, such as blog writing or coding projects.\nYou can use any tool you like to start using the Pomodoro technique and pick any break schedule that suits you. I\u0026rsquo;ll link some resources below. All you need is some sort of timer. You can use a timer on your computer or a physical timer. I use the Forest app on my iPhone because it integrates with the iOs \u0026ldquo;do not disturb\u0026rdquo; and \u0026ldquo;focus\u0026rdquo; modes, so I don\u0026rsquo;t get any distracting notifications when I\u0026rsquo;m on a Pomodoro.\nhttps://science.nichd.nih.gov/confluence/display/newsletter/2020/05/07/The+Pomodoro+Technique%3A+An+Effective+Time+Management+Tool\nhttps://www.youtube.com/watch?v=5WRO79zuJ4U\n","permalink":"https://mischavandenburg.com/zet/pomodoro/","summary":"I did a lot of studying last year, and I achieved a few tough certifications. I\u0026rsquo;ve always been good at studying and never struggled with getting decent grades in university. As a result, I never felt the need to use particular techniques to pass my tests. However, now that I need to do my studies combined with a full-time job, I did some optimization and looked into study techniques.\nOne technique I\u0026rsquo;ve become very fond of is the Pomodoro Technique.","title":"The Pomodoro technique has won me over"},{"content":"Last modified: 2023-01-10\nIn this evening\u0026rsquo;s studies I came across this bash script in a tutorial by Rob Muhlenstein:\n!#/bin/bash echo -e ${PATH//:/\\\\n} I could not make heads or tails of all these slashes and curly braces, since the output clearly indicated that search and replacement was being performed. I\u0026rsquo;m used to the sed / vim syntax: s/foo/bar\nAfter some research I learned that \u0026lsquo;//\u0026rsquo; is a global search and replace syntax of several text processing programs. It is known as parameter expansion in bash.\nExample:\nfoo=\u0026#34;1234567890\u0026#34; echo \u0026#34;${foo//[0-9]/x}\u0026#34; This replaces all the digits in the $foo variable with \u0026lsquo;x\u0026rsquo;, so the output would be xxxxxxxxxx\nTo do this with sed, you would do:\necho \u0026#34;$foo\u0026#34; | sed \u0026#39;s/[0-9]/x/g\u0026#39; For more info:\nman bash\n/parameter expansion\n","permalink":"https://mischavandenburg.com/zet/slash-syntax-replace/","summary":"Last modified: 2023-01-10\nIn this evening\u0026rsquo;s studies I came across this bash script in a tutorial by Rob Muhlenstein:\n!#/bin/bash echo -e ${PATH//:/\\\\n} I could not make heads or tails of all these slashes and curly braces, since the output clearly indicated that search and replacement was being performed. I\u0026rsquo;m used to the sed / vim syntax: s/foo/bar\nAfter some research I learned that \u0026lsquo;//\u0026rsquo; is a global search and replace syntax of several text processing programs.","title":"Using parameter expansion as search and replace"},{"content":"My friend gave me a nice tip for customizing the readme on my personal GitHub page. I discovered there is a whole world of plugins and customizations out there.\nI set up this one for my GitHub homepage. It uses a workflow to update the readme in my personal GitHub repo with the most recent posts from this blog, based on the RSS feed. Neat!\nIt was very easy to set up. If you don\u0026rsquo;t have your own blog, you could configure it with a different RSS feed. Hacker News for example.\nhttps://github.com/gautamkrishnar/blog-post-workflow\nhttps://github.com/abhisheknaiidu/awesome-github-profile-readme\n","permalink":"https://mischavandenburg.com/zet/adding-posts-github-readme/","summary":"My friend gave me a nice tip for customizing the readme on my personal GitHub page. I discovered there is a whole world of plugins and customizations out there.\nI set up this one for my GitHub homepage. It uses a workflow to update the readme in my personal GitHub repo with the most recent posts from this blog, based on the RSS feed. Neat!\nIt was very easy to set up.","title":"Automatically adding my recent blog posts to my GitHub Readme"},{"content":"I\u0026rsquo;ve used zsh for nearly two years now. I have a custom setup with autocompletion and a good looking prompt.\nRecently I\u0026rsquo;ve been diving deeper into bash scripting, following tutorials by rwxrob. He emphasizes all the time that it is much better to stick to bash instead of zsh.\nAdvantages of using bash:\nthe default Linux shell available on any Linux system full documentation available anywhere at all times with man bash free software less dependent on external plugins and configurations more portable practice by working on the command line The fact that working on the commandline is already coding convinced me to leave my beloved customized prompt behind (for now) and go back to the basics.\nI want to improve my bash scripting, and working in the bash shell will improve that just by virtue of doing my daily tasks on the command line.\nAlso I noticed I\u0026rsquo;ve gotten used to zsh\u0026rsquo;s excellent autocompletion and menu navigation. When I log in to servers at work, there is always this little moment of \u0026ldquo;oh, I don\u0026rsquo;t have that here\u0026rdquo;. I want to get better at bash so I\u0026rsquo;m not dependent on these external crutches anymore.\nAlso, I\u0026rsquo;m going to port my zsh configuration to bash. My current zsh configuration loads a bunch of plugins, and it is more of a hassle to get set up on a new system.\nI want to be able to pull my dotfiles repo and do very few steps to configure my environment.\nBut I\u0026rsquo;m going to miss that good-looking prompt with all the lovely icons!\n","permalink":"https://mischavandenburg.com/zet/back-to-bashics/","summary":"I\u0026rsquo;ve used zsh for nearly two years now. I have a custom setup with autocompletion and a good looking prompt.\nRecently I\u0026rsquo;ve been diving deeper into bash scripting, following tutorials by rwxrob. He emphasizes all the time that it is much better to stick to bash instead of zsh.\nAdvantages of using bash:\nthe default Linux shell available on any Linux system full documentation available anywhere at all times with man bash free software less dependent on external plugins and configurations more portable practice by working on the command line The fact that working on the commandline is already coding convinced me to leave my beloved customized prompt behind (for now) and go back to the basics.","title":"Back to Bas(h)ics: leaving zsh for now"},{"content":"I wanted to build an application from a Dockerfile and deploy it to a VM. I used a default Svelte setup as an example app.\nNaturally, Azure prefers that you deploy containers to services such as Azure Container Instances or App Services, so they don\u0026rsquo;t provide modules for the pipelines to deploy to docker servers as far as I could tell.\nI searched for a long time but I could not find a solution. In the end I just ran shell commands from the pipeline to run the container on on the server.\nsteps: - script: | sudo docker stop svelte-test sudo docker rm svelte-test sudo docker run --name svelte-test -p 8080:80 -d mischavandenburg/svelte-test:$(Build.BuildId) You can find the full pipeline code, the app and Dockerfile in my lab repo:\nhttps://github.com/mischavandenburg/lab/tree/main/azure-pipelines/docker-to-azure-vm\n","permalink":"https://mischavandenburg.com/zet/docker-to-azure-vm/","summary":"I wanted to build an application from a Dockerfile and deploy it to a VM. I used a default Svelte setup as an example app.\nNaturally, Azure prefers that you deploy containers to services such as Azure Container Instances or App Services, so they don\u0026rsquo;t provide modules for the pipelines to deploy to docker servers as far as I could tell.\nI searched for a long time but I could not find a solution.","title":"How to build and deploy a Docker container to an Azure VM using Azure Pipelines"},{"content":"To reach a VM from Azure Pipelines, you need to set up an environment.\nCreate your Linux VM in Azure.\nIn Azure DevOps, click envirnoments, new, and select the Virtual Machine option.\nA command is generated for you. SSH into your VM and run the command.\nNow the VM should show up under environments in Azure DevOps.\nSet up a repo with an azure-pipelines.yml with these contents to test. under environment, set the same name as you did in Azure DevOps for your environment.\ntrigger: - main pool: vmImage: ubuntu-latest jobs: - deployment: VMDeploy displayName: Deploy to VM environment: name: dev resourceType: VirtualMachine strategy: runOnce: deploy: steps: - script: echo \u0026#34;Hello world\u0026#34; You can see it when the deploy runs on the VM:\n","permalink":"https://mischavandenburg.com/zet/azure-pipelines-deploy-vm/","summary":"To reach a VM from Azure Pipelines, you need to set up an environment.\nCreate your Linux VM in Azure.\nIn Azure DevOps, click envirnoments, new, and select the Virtual Machine option.\nA command is generated for you. SSH into your VM and run the command.\nNow the VM should show up under environments in Azure DevOps.\nSet up a repo with an azure-pipelines.yml with these contents to test. under environment, set the same name as you did in Azure DevOps for your environment.","title":"How to deploy to a Linux VM in Azure with Azure Pipelines"},{"content":"For a project I\u0026rsquo;m setting up my environment with Terraform.\nI used this tutorial, but modified the code to make it simpler and easier to understand for beginners. The original uses a random module to generate random names, and generates a new SSH key. Also, this tutorial uses expensive VM tiers and Premium storage, which are not necessary when you are learning.\nI also thought the SSH configuration was overcomplicated. My version just takes an SSH keypair stored at ~/.ssh/id_rsa.pub\nTo run:\nterraform init terraform plan terraform apply The scripts prints the public IP of the newly created VM. You should be able to SSH to it:\nssh azureuser@the_printed_ip_address\nYou can find the code in my \u0026ldquo;lab\u0026rdquo; repo on GitHub.\nhttps://github.com/mischavandenburg/lab/tree/main/terraform/azure-simple-linux-vm\nhttps://learn.microsoft.com/en-us/azure/virtual-machines/linux/quick-create-terraform\nhttps://learn.microsoft.com/en-us/azure/developer/terraform/authenticate-to-azure?source=recommendations\u0026amp;tabs=bash#authenticate-to-azure-via-a-microsoft-account\n","permalink":"https://mischavandenburg.com/zet/terraform-linux-vm/","summary":"For a project I\u0026rsquo;m setting up my environment with Terraform.\nI used this tutorial, but modified the code to make it simpler and easier to understand for beginners. The original uses a random module to generate random names, and generates a new SSH key. Also, this tutorial uses expensive VM tiers and Premium storage, which are not necessary when you are learning.\nI also thought the SSH configuration was overcomplicated. My version just takes an SSH keypair stored at ~/.","title":"Deploying a Linux VM to Azure with Terraform"},{"content":"I use the Obsidian app, but I mostly write and search my notes with neovim. I added my zet directory from this blog repo into the Obsidian vault as a symbolic link, but I soon discovered that these files were not being searched.\nTelescope.nvim uses ripgrep (rg) to do the live grepping in its search, and ripgrep does not follow symbolic links by default. You need to pass the -L flag to it.\nTo pass the -L flag, and some other flags, I added the following to my telescope config file:\n-- Custom ripgrep configuration: local telescope = require(\u0026#34;telescope\u0026#34;) local telescopeConfig = require(\u0026#34;telescope.config\u0026#34;) -- Clone the default Telescope configuration local vimgrep_arguments = { unpack(telescopeConfig.values.vimgrep_arguments) } -- I want to search in hidden/dot files. table.insert(vimgrep_arguments, \u0026#34;--hidden\u0026#34;) -- I don\u0026#39;t want to search in the `.git` directory. table.insert(vimgrep_arguments, \u0026#34;--glob\u0026#34;) table.insert(vimgrep_arguments, \u0026#34;!**/.git/*\u0026#34;) -- I want to follow symbolic links table.insert(vimgrep_arguments, \u0026#34;-L\u0026#34;) telescope.setup({ defaults = { -- `hidden = true` is not supported in text grep commands. vimgrep_arguments = vimgrep_arguments, }, pickers = { find_files = { -- `hidden = true` will still show the inside of `.git/` as it\u0026#39;s not `.gitignore`d. find_command = { \u0026#34;rg\u0026#34;, \u0026#34;--files\u0026#34;, \u0026#34;--hidden\u0026#34;, \u0026#34;--glob\u0026#34;, \u0026#34;!**/.git/*\u0026#34;, \u0026#34;-L\u0026#34; }, }, }, }) Based on the configuration examples found on the project\u0026rsquo;s GitHub page.\nhttps://github.com/nvim-telescope/telescope.nvim\n","permalink":"https://mischavandenburg.com/zet/neovim-telescope-follow-symlinks/","summary":"I use the Obsidian app, but I mostly write and search my notes with neovim. I added my zet directory from this blog repo into the Obsidian vault as a symbolic link, but I soon discovered that these files were not being searched.\nTelescope.nvim uses ripgrep (rg) to do the live grepping in its search, and ripgrep does not follow symbolic links by default. You need to pass the -L flag to it.","title":"How to follow symbolic links while searching with Telescope in neovim"},{"content":"When you have an Ansible language server installed, you might find that your yaml LSP will attach to your current buffer, but the ansible language server won\u0026rsquo;t attach.\nYou can fix this by setting the correct file type for the current buffer:\n:set ft=yaml.ansible\nYou could also adjust the Ansible LSP so it attaches to all yaml files. However, this does not work out for me, because I edit different yaml files for different purposes every day. Not all yaml files are to be used with Ansible.\nThere is logic for the Ansible language server to figure out if you are working on Ansible yaml files based on the directory structure you\u0026rsquo;re working in.\nSo setting the filetype when I needed works well for me.\nhttps://www.reddit.com/r/neovim/comments/tbd7g0/lsp_ansiblels_wont_attach_anymore/\n","permalink":"https://mischavandenburg.com/zet/ansible-lsp-fix/","summary":"When you have an Ansible language server installed, you might find that your yaml LSP will attach to your current buffer, but the ansible language server won\u0026rsquo;t attach.\nYou can fix this by setting the correct file type for the current buffer:\n:set ft=yaml.ansible\nYou could also adjust the Ansible LSP so it attaches to all yaml files. However, this does not work out for me, because I edit different yaml files for different purposes every day.","title":"Attaching the Ansible Language Server to yaml files in neovim (LSP)"},{"content":"Make sure to have pip installed.\nRun pip install python-openstackclient\nPip will install a binary called \u0026ldquo;openstack\u0026rdquo; in ~/.local/bin\nIf the openstack command is not available in your session, you might need to add it to your PATH:\nexport PATH=\u0026quot;$HOME/.local/bin:$PATH\u0026quot;\nAdd this to your ~/.zshrc or ~/.bashrc to make sure this happens for each shell session.\nDon\u0026rsquo;t forget to source your updated ~/.zshrc if you chose to add it:\nsource ~/.zshrc\nhttps://docs.openstack.org/newton/user-guide/common/cli-install-openstack-command-line-clients.html\n","permalink":"https://mischavandenburg.com/zet/install-openstack-cli/","summary":"Make sure to have pip installed.\nRun pip install python-openstackclient\nPip will install a binary called \u0026ldquo;openstack\u0026rdquo; in ~/.local/bin\nIf the openstack command is not available in your session, you might need to add it to your PATH:\nexport PATH=\u0026quot;$HOME/.local/bin:$PATH\u0026quot;\nAdd this to your ~/.zshrc or ~/.bashrc to make sure this happens for each shell session.\nDon\u0026rsquo;t forget to source your updated ~/.zshrc if you chose to add it:\nsource ~/.","title":"How to install the Openstack CLI on Linux"},{"content":" Download the Openstack RC file from the Openstack portal. Click your username in the top right corner to find it. Source the RC file to make the environment variables avaialable to your current session: source ~/my_openstack.sh Find the instance ID of your VM from the portal. Run openstack server set --root-password be3xxxx5-8348-418b-xxxb-c4xxxx575cd You will be prompted for the new password which will be set on the virtual machine. ","permalink":"https://mischavandenburg.com/zet/openstack-root-password/","summary":" Download the Openstack RC file from the Openstack portal. Click your username in the top right corner to find it. Source the RC file to make the environment variables avaialable to your current session: source ~/my_openstack.sh Find the instance ID of your VM from the portal. Run openstack server set --root-password be3xxxx5-8348-418b-xxxb-c4xxxx575cd You will be prompted for the new password which will be set on the virtual machine. ","title":"How to Reset a VM Root Password using the Openstack CLI"},{"content":"When you install a pip package which is meant to be run from the command line as a command, you might find that it is not available to you after installation.\nIf this happens, it might be that the path is missing from your PATH variable. Therefore, the shell does not source these binaries when initiated, and does not know that these executables exist.\nYou can find the location of your binaries by running pip show package_name\nUsually the binaries will be located in ~/.local/bin on a UNIX based system.\nTo add this to your path, run:\nexport PATH=\u0026quot;$HOME/.local/bin:$PATH\u0026quot;\nAdd this to your ~/.zshrc or ~/.bashrc to make sure this happens for each shell session.\nDon\u0026rsquo;t forget to source your updated ~/.zshrc if you chose to add it:\nsource ~/.zshrc\nhttps://stackoverflow.com/questions/29980798/where-does-pip-install-its-packages\n","permalink":"https://mischavandenburg.com/zet/run-installed-pip-packages/","summary":"When you install a pip package which is meant to be run from the command line as a command, you might find that it is not available to you after installation.\nIf this happens, it might be that the path is missing from your PATH variable. Therefore, the shell does not source these binaries when initiated, and does not know that these executables exist.\nYou can find the location of your binaries by running pip show package_name","title":"How to run installed pip packages as binaries"},{"content":"Rob Muhlenstein makes an interesting point that using bash on the command line is already coding. When you are running commands in the terminal, you are coding one line at a time.\nWhen you put these commands in a file you have a bash script. Therefore, he argues that bash should be your first language.\nI think this is such an interesting point. I\u0026rsquo;ve been using Linux and working on the command line for years but it never dawned on me that I, in fact, was coding while working on the command line. However, when I was writing bash scripts, I did consider myself to be coding. There is literally no difference. A bash script is just a string of commands that you would enter manually anyway.\nhttps://rwx.gg/\n","permalink":"https://mischavandenburg.com/zet/bash-cmdline-is-coding/","summary":"Rob Muhlenstein makes an interesting point that using bash on the command line is already coding. When you are running commands in the terminal, you are coding one line at a time.\nWhen you put these commands in a file you have a bash script. Therefore, he argues that bash should be your first language.\nI think this is such an interesting point. I\u0026rsquo;ve been using Linux and working on the command line for years but it never dawned on me that I, in fact, was coding while working on the command line.","title":"Working on the command line is already coding"},{"content":"Sed, it\u0026rsquo;s so powerful. I remember I struggled with finding practical uses for it when I did my LPIC-1 certification. But now I find myself using it several times a week. It is so powerful to edit multiple files at a time. I use it for work, but also for making changes to my entire second brain in Obsidian with one command.\nToday I needed to update my /articles/ links to /zet/articles/ links because I\u0026rsquo;m restructuring my website. Here is the sed expression that is executed for every markdown file that is found by fd:\nsed -i \u0026#39;s/\\/articles\\//\\/zet\\/articles\\//g\u0026#39; $(fd .md) The result:\ndiff --git a/content/zet/move-to-zet.md b/content/zet/move-to-zet.md index 1e37283..3b817e3 100644 --- a/content/zet/move-to-zet.md +++ b/content/zet/move-to-zet.md @@ -6,7 +6,7 @@ tags: --- -I\u0026#39;ve transitioned my note taking system towards a Zettelkasten system. I still use directories for folders and make copious links, but more often than not I put them in the larger generic 00-zettelkasten directory in my [Obsidian](/articles/obsidian-introduction/) vault. +I\u0026#39;ve transitioned my note taking system towards a Zettelkasten system. I still use directories for folders and make copious links, but more often than not I put them in the larger generic 00-zettelkasten directory in my [Obsidian](/zet/articles/obsidian-introduction/) vault. These sites are super useful to help you formulate your expressions:\nhttps://sed.js.org/\nhttps://regex101.com/\n","permalink":"https://mischavandenburg.com/zet/fall-in-love-with-sed/","summary":"Sed, it\u0026rsquo;s so powerful. I remember I struggled with finding practical uses for it when I did my LPIC-1 certification. But now I find myself using it several times a week. It is so powerful to edit multiple files at a time. I use it for work, but also for making changes to my entire second brain in Obsidian with one command.\nToday I needed to update my /articles/ links to /zet/articles/ links because I\u0026rsquo;m restructuring my website.","title":"Fall in love with sed."},{"content":"Telemetry is the collection of measurements or other data at remote points, and transmitting that data to a receiver for monitoring.\nSampling is used to reduce telemetry traffic and costs for storage and data in Application Insights.\nFor small and medium sized applications sampling is generally not necessary.\nAdvantages of sampling:\nThrottling data when the application suddenly sends a high volume of telemetry in a short time This saves costs! Keeping a pricing tier quota Reduce network traffic from telemetry collection Three different kinds of sampling:\nadaptive sampling automatically adjusts volume of telemetry from ASP.NET or Azure Functions only for these two fixed-rate sampling rate is set by the administrator use when you have a clear idea of the appropriate sampling percentage reduces volume from ASP.NET or ASP.NET Core server Java server Python applications User browsers ingestion sampling used when monthly quota is often met reduces amount of processed and retained traffic by Application Insights less processing = less cost doesn\u0026rsquo;t reduce telemetry traffic sent from the app happens at Applications Insight service endpoint disabled if SDK samples telemetry can set sampling rate without redeploying the app only applies when no other sampling is in effect supports all Application Insights SDK\u0026rsquo;s ","permalink":"https://mischavandenburg.com/zet/application-insights-sampling/","summary":"Telemetry is the collection of measurements or other data at remote points, and transmitting that data to a receiver for monitoring.\nSampling is used to reduce telemetry traffic and costs for storage and data in Application Insights.\nFor small and medium sized applications sampling is generally not necessary.\nAdvantages of sampling:\nThrottling data when the application suddenly sends a high volume of telemetry in a short time This saves costs! Keeping a pricing tier quota Reduce network traffic from telemetry collection Three different kinds of sampling:","title":"Application Insights: Telemetry Sampling"},{"content":"This term can be confusing. Initially I thought it meant monitoring of the pipelines themselves. However, in the context of Azure Release Pipelines, continuous monitoring refers to something else.\nContinuous monitoring leverages metrics from other services such as Application Insights. You can set up release gates based on these metrics. For example, you can set up a release gate to roll back the deployment if an alert is being fired for high CPU usage in the application.\nYou can set up several of these checks. If all these checks pass, the pipeline can proceed.\n","permalink":"https://mischavandenburg.com/zet/pipelines-continuous-monitoring/","summary":"This term can be confusing. Initially I thought it meant monitoring of the pipelines themselves. However, in the context of Azure Release Pipelines, continuous monitoring refers to something else.\nContinuous monitoring leverages metrics from other services such as Application Insights. You can set up release gates based on these metrics. For example, you can set up a release gate to roll back the deployment if an alert is being fired for high CPU usage in the application.","title":"Pipelines: Continuous Monitoring"},{"content":"Debugging is done using call stacks in monolithic applications. Nowadays it is more common to deploy an application using a microservices architecture. Microservices make it easier to update certain parts of the application, and allow for more frequent deployments.\nUsing microservices does have a disadvantage: you cannot use the local call stack for debugging, because calls are sent to different microservices.\nDistributed tracing is an implementation of the call stack in the cloud. It is usually implemented by adding an agent, SDK, or library to the service. In Azure you can enable distributed tracing via Application Insights through auto-instrumentation or SDKs.\nUnified cross-component transaction diagnostics\n","permalink":"https://mischavandenburg.com/zet/distributed-tracing/","summary":"Debugging is done using call stacks in monolithic applications. Nowadays it is more common to deploy an application using a microservices architecture. Microservices make it easier to update certain parts of the application, and allow for more frequent deployments.\nUsing microservices does have a disadvantage: you cannot use the local call stack for debugging, because calls are sent to different microservices.\nDistributed tracing is an implementation of the call stack in the cloud.","title":"Distributed Tracing"},{"content":"A software development kit (SDK) is a set of tools provided by the manufacturer of (usually) a hardware platform, operating system (OS), or programming language.\nSDKs contain all the tools you need to get started. They typically contain a compiler, a debugger and an API. But they can also contain documentation and testing tools.\n","permalink":"https://mischavandenburg.com/zet/sdk/","summary":"A software development kit (SDK) is a set of tools provided by the manufacturer of (usually) a hardware platform, operating system (OS), or programming language.\nSDKs contain all the tools you need to get started. They typically contain a compiler, a debugger and an API. But they can also contain documentation and testing tools.","title":"What is a SDK?"},{"content":"All of the content on this blog is licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0).\nYou are free to: Share — copy and redistribute the material in any medium or format\nUnder the following terms: Attribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\nNonCommercial — You may not use the material for commercial purposes.\nNoDerivatives — If you remix, transform, or build upon the material, you may not distribute the modified material.\nhttps://creativecommons.org/licenses/by-nc-nd/4.0/\n","permalink":"https://mischavandenburg.com/zet/articles/copyright-license/","summary":"All of the content on this blog is licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0).\nYou are free to: Share — copy and redistribute the material in any medium or format\nUnder the following terms: Attribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.","title":"Copyright and License"},{"content":"As I alluded to in my article about Obsidian, I am very fond of editing my text in neovim. Naturally, if you want to edit in neovim, you need to have your text as local files. I keep all of my personal notes in markdown.\nPreviously I was using WordPress, but the editing and writing experience became torture which I could not endure any longer. I looked for a different solution that would allow me to edit my files locally instead of in the browser.\nI discovered Hugo and I fell in love with it immediately.\nHugo is a static site generator based on markdown files. My entire blog is written in markdown files which are stored in a GitHub repo.. I write my blog posts in vim and when I\u0026rsquo;m done I use Hugo to generate the updated website.\nThe result is what you see in the \u0026ldquo;public\u0026rdquo; directory in the GitHub repo. This public directory is pushed to a different repo which is hooked up to my hosting provider. My hosting provider uses Plesk, and with Plesk I have the option to connect the GitHub repo to the web server with a webhook. When I push to my hosting repo, the contents are gathered by the webserver and served as public web content.\nMy complete writing and publishing workflow looks like this:\nCreate a new markdown file Write the note or article Save the file and run the \u0026ldquo;hugo\u0026rdquo; command to regenerate the website Run the \u0026ldquo;publish\u0026rdquo; script. This is a custom script I wrote that takes the contents of the \u0026ldquo;public\u0026rdquo; directory to my hosting repo Push the newly generated website to the hosting repo And we\u0026rsquo;re live! 🚀 🎉 It is such a smooth and convenient process. I can literally have a new note published to the interet within a few minutes, and it is all done from the command line using my favorite tools.\nBlog GitHub repo\n","permalink":"https://mischavandenburg.com/zet/articles/how-this-blog-is-created/","summary":"As I alluded to in my article about Obsidian, I am very fond of editing my text in neovim. Naturally, if you want to edit in neovim, you need to have your text as local files. I keep all of my personal notes in markdown.\nPreviously I was using WordPress, but the editing and writing experience became torture which I could not endure any longer. I looked for a different solution that would allow me to edit my files locally instead of in the browser.","title":"How This Blog is Created, Written and Hosted"},{"content":"When you call a function, the system sets aside space in memory for the function to do its work. Those chunks are called \u0026ldquo;stack frames\u0026rdquo; or \u0026ldquo;function frames.\u0026rdquo;\nThese frames are arranged in a stack. The frame for the most recently called function is always at the top of the stack. When a new function is called, it becomes the active frame, and it is on top of the stack.\nThe function that is actually doing something at the moment is on top of the stack and is known as the \u0026ldquo;active frame.\u0026rdquo;\nWhen the function finishes its work, the frame is popped off of the stack. The frame in second place becomes the active frame. It had been paused in the meantime, and now it is active again, because it is on top.\nFunctions that are not on top, are not running.\nThis video explains it well.\n","permalink":"https://mischavandenburg.com/zet/call-stacks/","summary":"When you call a function, the system sets aside space in memory for the function to do its work. Those chunks are called \u0026ldquo;stack frames\u0026rdquo; or \u0026ldquo;function frames.\u0026rdquo;\nThese frames are arranged in a stack. The frame for the most recently called function is always at the top of the stack. When a new function is called, it becomes the active frame, and it is on top of the stack.","title":"Call Stacks"},{"content":"I\u0026rsquo;m starting a project with a friend. Developing an application. We make a good team, he\u0026rsquo;s great at coding and knows the backend too.\nHe\u0026rsquo;ll do the development, I\u0026rsquo;m in charge of hosting. We\u0026rsquo;re setting everything up in Azure DevOps, so it is a great way to practice my Azure skills and apply the things I\u0026rsquo;ve learned in my recently obtained AZ-104 Azure Administrator certification.\nEven though it is a small scale hobby project, I still plan to approach it as if it was an enterprise production application. I\u0026rsquo;ll set up a full CI/CD pipeline with testing in a secure manner. Credentials stored in an Azure key vault and images pushed to a private registry.\nThis is going to be fun!\n","permalink":"https://mischavandenburg.com/zet/starting-a-project/","summary":"I\u0026rsquo;m starting a project with a friend. Developing an application. We make a good team, he\u0026rsquo;s great at coding and knows the backend too.\nHe\u0026rsquo;ll do the development, I\u0026rsquo;m in charge of hosting. We\u0026rsquo;re setting everything up in Azure DevOps, so it is a great way to practice my Azure skills and apply the things I\u0026rsquo;ve learned in my recently obtained AZ-104 Azure Administrator certification.\nEven though it is a small scale hobby project, I still plan to approach it as if it was an enterprise production application.","title":"Starting a Project"},{"content":"I\u0026rsquo;ve transitioned my note taking system towards a Zettelkasten system. I still use directories for folders and make copious links, but more often than not I put them in the larger generic 00-zettelkasten directory in my Obsidian vault.\nThe concept of \u0026ldquo;atomic notes\u0026rdquo; is also very important in Zettelkasten methods. Notes should be small and concise.\nUp until this point I\u0026rsquo;ve been publishing full articles on my blog. I came across Rob Muhlestein yesterday and I was very inspired by his setup and public zettelkasten. I think I\u0026rsquo;ll move to a similar approach. Still planning to write and publish full articles as well, but also including atomic notes and personal status updates.\n","permalink":"https://mischavandenburg.com/zet/move-to-zet/","summary":"I\u0026rsquo;ve transitioned my note taking system towards a Zettelkasten system. I still use directories for folders and make copious links, but more often than not I put them in the larger generic 00-zettelkasten directory in my Obsidian vault.\nThe concept of \u0026ldquo;atomic notes\u0026rdquo; is also very important in Zettelkasten methods. Notes should be small and concise.\nUp until this point I\u0026rsquo;ve been publishing full articles on my blog. I came across Rob Muhlestein yesterday and I was very inspired by his setup and public zettelkasten.","title":"Going to Publish Smaller, and More Often"},{"content":"As a DevOps or Cloud engineer, you work with many different technologies daily. Therefore, a good engineer needs a solid foundation in Linux, a lot of knowledge about your cloud solution, networking, CI/CD, at least one programming language, and many other topics.\nNot only are there many of these categories, but within these categories, there are several alternatives. For example, in the category of infrastructure management, there is Ansible, Puppet, or Chef, each with its particular approach and configuration methods.\nIt can be challenging to keep everything memorized at all times. However, when I started my journey to become a DevOps engineer, I kept meticulous notes of everything I learned, and this practice has been highly beneficial so far. This blog is a result of the notes I keep every day.\nNote-taking is an essential part of the DevOps engineer\u0026rsquo;s toolkit. It allows you to stay organized, track changes, document processes, and keep track of important information. Using a powerful note-taking app like Obsidian can streamline your note-taking process and work more efficiently.\nObsidian for DevOps engineers Having a reliable and efficient system in place for managing your notes and documentation is crucial. That\u0026rsquo;s where Obsidian comes in. Obsidian is a powerful note-taking app that can help you organize and manage your notes more efficiently.\nOne of the main features of Obsidian is its use of \u0026ldquo;vaults.\u0026rdquo; A vault is a folder containing your notes as markdown text files. Your notes are stored on your machine rather than in the cloud. This gives you complete control over your data. You always have access to your local text files and can interact with them or back them up as you see fit.\nBecause your notes are stored as markdown files, you can use different tools to write or edit your notes. I mostly use neovim for editing, but I use the Obsidian application for making new links and visualization. Moreover, storing your notes as files allows you to run python scripts on your notes and customize your workflow as needed. This can be especially useful for automating tasks, streamlining your work, or making bulk updates.\nLinks and graph view In addition to its local storage capabilities, Obsidian also offers several other valuable features for DevOps engineers. For example, you can use the \u0026ldquo;graph view\u0026rdquo; to visualize your notes and see how they\u0026rsquo;re related to one another. This can be particularly useful for understanding complex systems and tracking changes over time.\nThese relations between notes are created by \u0026ldquo;[[markdown links]]\u0026rdquo;. When a note receives many links, its dot size will increase on the graph view, and in this manner, it is easy to see which notes or topics are significant in your vault and play an important role in your life.\nThis is the local graph view, showing all the linked notes to my Linux note.\nConclusion Many note-taking apps, such as Evernote, Notion, or Roam research, are available. I tried a few of these, but Obsidian was the best solution for me. Mainly because your notes are stored as markdown files on your machine and because they offer a syncing service with end-to-end encryption. Their graph view provides an interesting way to navigate your notes and discover unexpected connections.\nGive it a try. It\u0026rsquo;s free.\nObsidian website\n","permalink":"https://mischavandenburg.com/zet/articles/obsidian-introduction/","summary":"As a DevOps or Cloud engineer, you work with many different technologies daily. Therefore, a good engineer needs a solid foundation in Linux, a lot of knowledge about your cloud solution, networking, CI/CD, at least one programming language, and many other topics.\nNot only are there many of these categories, but within these categories, there are several alternatives. For example, in the category of infrastructure management, there is Ansible, Puppet, or Chef, each with its particular approach and configuration methods.","title":"Obsidian: A Note Taking App For DevOps Engineers"},{"content":"When you learn about DevOps, you will come across the concept of a container early on. This is a \u0026ldquo;Mischa Explains\u0026rdquo; article where I attempt to explain a concept in my own words as simply as possible. I use the Feynman technique and pretend to explain it to a 12-year-old.\nVirtualization To understand containers, we need to understand virtualization. Virtualization is the process of creating \u0026ldquo;fake computers\u0026rdquo; or \u0026ldquo;virtual computers\u0026rdquo; on a physical computer.\nOn your desk, you have a laptop or a desktop PC. This machine has hardware such as a motherboard, a hard disk, and a video card. To run programs on your computer, it needs an Operating System. Usually, Windows, macOS, or Linux are used.\nLet\u0026rsquo;s say you have a computer running Windows, but you want to run a program that can only run on Linux. One solution is to buy another laptop and put it beside your Windows laptop on your desk. So now you have two computers with two different operating systems.\nFortunately, there are other solutions. We can use virtualization to make a Virtual Machine. A virtual machine is created by software to imitate a fully functional running computer inside your current operating system. You can create a virtual machine that runs Linux on your Windows computer. Your Windows computer running the Linux virtual machine is known as the **host.\nNow you don\u0026rsquo;t need to buy another computer to run your Linux program. Instead, you can boot up your Linux virtual machine and run your program when needed. If you have a powerful computer, you could run ten or more virtual machines, each of which has its own operating system and custom environment.\nContainers Every time you create a virtual machine, the virtual machine needs a complete operating system to work. So, first, the software creates a virtual processor, virtual video card, and a virtual network interface. Then, it runs a fully functional operating system on that virtual hardware. This takes up a lot of resources.\nContainers are lightweight packages of software. They are designed to do a very specific task, and therefore they only contain the resources they need to do that task. Nothing more.\nContainers use the operating system of the physical computer to run. They have a very minimal, lightweight operating system inside them, but it only contains the elements they need to do their specific task. Therefore, containers are very easy to distribute, and you can run them very quickly.\nContainers are like newspapers Containers are like newspapers. Newspapers have a particular task: providing you with the day\u0026rsquo;s news. You cannot use newspapers to study for your mathematics exam. You use your math book to study for your math exam. If you want to be informed of the day\u0026rsquo;s news, you use a newspaper. This is what I mean by containers having a specific task.\nNext, newspapers are printed on a specific kind of paper. When you buy an expensive book, it will have a sturdy and durable cover, and the pages are made of nice thick paper that will last a long time. The pages don\u0026rsquo;t tear very quickly, and when the book gets wet, it can withstand it. This thick cover and high-quality papers are like the operating system of a virtual machine.\nNewspapers, on the other hand, are printed on very thin paper. Because they are designed to distribute the news to you effectively, newspapers do not need to be stored forever or do any other tasks. If you used thick, expensive paper for newspapers, they would become costly, and no one would buy them anymore. The paper is optimized to bring the news to you.\nIn the same way, the container only comes with the components it needs to do its specific task. Therefore, the container is optimized for its purpose. As a result, they can be distributed more quickly and do not take up a lot of resources when running.\nThere are other benefits to containers, such as improving the ability to autoscale your application, but I will expand on those in a future blog post.\nFurther study To learn more about containers, you can use the following resources:\nContainers \u0026amp; Friends from John Savill\u0026rsquo;s DevOps Masterclass\nDocker Documentation\nDocker Tutorial for Beginners\n","permalink":"https://mischavandenburg.com/zet/articles/what-are-containers/","summary":"When you learn about DevOps, you will come across the concept of a container early on. This is a \u0026ldquo;Mischa Explains\u0026rdquo; article where I attempt to explain a concept in my own words as simply as possible. I use the Feynman technique and pretend to explain it to a 12-year-old.\nVirtualization To understand containers, we need to understand virtualization. Virtualization is the process of creating \u0026ldquo;fake computers\u0026rdquo; or \u0026ldquo;virtual computers\u0026rdquo; on a physical computer.","title":"What Are Containers?"},{"content":"I\u0026rsquo;ve been using neovim for over a year now, and I\u0026rsquo;m very happy that I went through the initial difficulty of learning vim. One of the best perks of using neovim is that you can customize your entire editing experience and workflow. However, it can be a daunting experience to start with an empty configuration and set up everything from scratch.\nI started with an empty vanilla vim config and slowly added the plugins as I went along. Videos by content creators such as ThePrimagen were also helpful in getting inspiration on which plugins I might like for my setup. But this might not be suitable for everyone. I was only editing yaml files and writing simple Python scripts at the time, whereas you might be looking for an IDE experience out of the box.\nRecently I discovered kickstart.nvim by neovim core maintainer TJdeVries.\nI decided to give it a try, and I was pleasantly surprised. It is a great starting setup for a beginner. It is simple and does not overwhelm you with thousands of features.\nI\u0026rsquo;ve completely rewritten my config based on kickstart.nvim, and I am delighted with the result. Especially the LSP setup is very well thought out, and it works much better than the setup I came up with on my own.\nTJ DeVries also made a video introducing kickstart.nvim and going through the initial setup.\nI highly recommend kickstart.nvim if you are interested in using neovim and are looking for a sane place to start.\nlinks kickstart.nvim\nKickstart video by TJdeVries\n","permalink":"https://mischavandenburg.com/zet/articles/kickstart-nvim/","summary":"I\u0026rsquo;ve been using neovim for over a year now, and I\u0026rsquo;m very happy that I went through the initial difficulty of learning vim. One of the best perks of using neovim is that you can customize your entire editing experience and workflow. However, it can be a daunting experience to start with an empty configuration and set up everything from scratch.\nI started with an empty vanilla vim config and slowly added the plugins as I went along.","title":"Getting Started with Neovim: kickstart.nvim"},{"content":"TLDR It took me 80 hours of studying to gain this certification. Here are my notes and Anki deck: GitHub repo\nIntroduction When I did my English degree at University, exams were usually handwritten essays that needed to be done within a 3-hour timeframe. Sometimes we had multiple-choice tests, and I would always consider them a guaranteed pass because multiple-choice was much easier in my experience.\nThat opinion has changed since I\u0026rsquo;ve obtained a few IT certifications. These tests are hard! I\u0026rsquo;m typing this while sitting on the bus on my way home from my AZ-104 exam. I passed with an 860 score of 1000, where 700 or higher is a pass. But it was an astonishingly tough exam. Usually, I finish quickly and spend at least half an hour reviewing my answers. I had only 5 minutes to review my questions this time because I had used up all of the available time. The questions required intense concentration and were time-consuming because I needed to compare many options which were very similar to each other. There were no easy questions.\nPreparation I studied 80 hours for this exam in a month. I work full-time as a DevOps Engineer, so I study in the evenings and on weekends. I have my Azure Fundamentals and CKA, but I only work with Azure occasionally in my current role.\nHere is what I did to prepare for my exam:\nGo through all of the Microsoft Learn modules for the AZ-104 Watch the entire AZ-104 study list by John Savill Practice exams on TutorialsDojo until I could pass them with 90%+ scores Microsoft ESI practice exams Microsoft AZ-104 Exam prep videos Microsoft Learn You really need to master all of the subject matter. Only completing the Microsoft Learn modules is not enough preparation. They are more like summaries. At the end of each module, they provide links to the documentation for the subject for further study. Unfortunately, Microsoft does not go easy on you. It expects you to know obscure details of nearly every service this exam covers. Therefore, I advise going beyond the Microsoft Learn modules and studying the linked articles after each module.\nYouTube AZ-104 Study Playlist by John Savill I\u0026rsquo;m not sure if it\u0026rsquo;s better to watch this playlist first and then do the Microsoft modules or the other way around. I did the Microsoft modules first, but for my next exam (AZ-400 DevOps Expert), I\u0026rsquo;ll start with the videos and then do the Microsoft Learn modules.\nTutorialsdojo These practice exams are excellent. I used them in preparation for my fundamentals exam.\nThe best thing about them is that they provide extensive documentation and explanation of the questions. So after you finish the exam, you can study a lot with these examples.\nESI Practice Exams You\u0026rsquo;re lucky if your organization participates in Microsoft\u0026rsquo;s Enterprise Skills Initiative. The practice exams provided in the ESI environment give you a good indication of what you can expect at the exam. I first did the Tutorialsdojo exams and then moved on to the ESI exams, and I was humiliated. The ESI questions are very complex and hard to solve, and I learned a lot from these exams.\nThere are 210 questions total, and I worked through all of them, and whenever I failed a question, I did a deeper dive into the question\u0026rsquo;s theme.\nstudying I take notes in Obsidian, and I use Anki for spaced repetition. I highly recommend keeping a deck of Anki cards and continuously testing yourself. You will need to memorize a lot of details. For example, you are expected to remember that storage accounts of the FileStorage type do not support Geo Redundant Storage. You can find my Anki deck in the GitHub repo.\nlinks AZ-104 Exam page with learning modules\nJohn Savill\u0026rsquo;s AZ-104 Study playlist\nGitHub repo containing my notes and Anki deck\nMicrosoft AZ-104 Exam prep videos\n","permalink":"https://mischavandenburg.com/zet/articles/az-104-study-guide/","summary":"TLDR It took me 80 hours of studying to gain this certification. Here are my notes and Anki deck: GitHub repo\nIntroduction When I did my English degree at University, exams were usually handwritten essays that needed to be done within a 3-hour timeframe. Sometimes we had multiple-choice tests, and I would always consider them a guaranteed pass because multiple-choice was much easier in my experience.\nThat opinion has changed since I\u0026rsquo;ve obtained a few IT certifications.","title":"Study Guide: AZ-104 Azure Administrator Associate"},{"content":"This weekend I had a lot of fun with a project. I wanted to learn more about GitOps and try out ArgoCD.\nMy goal was to be able to deploy an application from a GitHub repo to my local Kubernetes cluster running in minikube. There are many options I could have used, such as running Jenkins in my cluster. But I wanted to use Azure pipelines for practice, which complicates the deployment to my local cluster, because the cluster is not running on Azure. I also wanted to try out ArgoCD and learn more about GitOps.\nThe application is a simple web app that I wrote which displays a quote in the browser:\nGitOps and Structure GitOps is used to automate the process of provisioning infrastructure. Infrastructure as code is used to generate the same environment every time the environment is deployed.\nFor my project I have two separate GitHub repos. The first repo contains the code for a simple web app I created and the Dockerfile to generate the image. I call this my application repo. The other repo is my GitOps repo which contains the manifest files to deploy the application in Kubernetes. I decided to leverage Helm to create my manifest files. This way I can create templates and define my desired values in a values.yaml file in the repo.\nUltimately my goal was to use an Azure pipeline to build an image from my application repo and push it to Docker hub. This new image is given a new tag which needs to be stored. The first pipeline should trigger a new pipeline that makes a pull request to the GitOps repo to update the tag in my Helm chart.\nArgoCD will then scan the GitOps repo and realize that the tag has been updated, and deploy the new tag to my cluster.\nMinikube I used minikube to deploy my local Kubernetes cluster. Another option is kind (Kubernetes In Docker) but I wanted to use a VM approach this time.\nArgoCD ArgoCD is a declarative GitOps continuous delivery tool for Kubernetes. This is the solution I used to continuously scan my GitOps repo. When ArgoCD detects a change in the desired state, it will compare it with the state in my running cluster and make changes accordingly. I found a really good tutorial to run ArgoCD in minikube.\nAzure Pipelines With my cluster running on my local machine and my repos set up, I needed to use Azure Pipelines to bring it all together. Building the image and pushing it to Docker Hub wasn\u0026rsquo;t a big deal. But I had two big challenges in my desired setup: I needed to pass the new tag number to a new pipeline, and I needed to use Azure Pipelines to create a new PR to my GitOps repo.\nPassing a value from one pipeline to another Interestingly, this wasn\u0026rsquo;t as easy as it sounds, and from my internet searching it seemed that many people struggled with this. I decided to use the Variable Groups in Azure DevOps. However, after I finished writing my pipeline, I discovered I had no problems with reading the value from the Variable Groups, but it was impossible to update it using existing pipeline tasks. So I had to a bit of hacking to make it work. In the end I had to use the Azure CLI from within the pipeline to update my variable:\n- stage: update_tag jobs: - job: update_tag_variable displayName: Update Tag Variable steps: - bash: | az pipelines variable-group variable \\ update --group-id 202 \\ --org $(System.CollectionUri) \\ --project $(System.TeamProject) \\ --name tag --value $(Build.BuildId) env: AZURE_DEVOPS_EXT_PAT: $(System.AccessToken) This didn\u0026rsquo;t feel like a very elegant solution, but it was the only solution I could come up with.\nI also struggled a lot with permissions. I needed to find the correct service principal to assign the administrator rights to. This post really helped to solve my problem.\nSubmitting a PR to a GitHub repo When I started writing my pipeline I thought it would be very straightforward to just submit a PR to a repo, but I quickly discovered that this is not natively supported in Azure pipelines yet. In fact, I could not find a way to submit a PR at all. I had to settle for a solution that checks out the GitOps repo and creates a new branch. This new branch updates the tag in the values.yaml with the new tag that was passed from the previous pipeline.\nvariables: - group: mischa-quote - name: passed_tag value: $[variables.tag] - name: branch_name value: \u0026#34;pipeline-$(passed_tag)\u0026#34; pool: vmImage: ubuntu-latest steps: - checkout: self persistCredentials: true clean: true - script: | git config --global user.email \u0026#34;mischa@pipeline.com\u0026#34; git config --global user.name \u0026#34;Mischa Pipeline\u0026#34; git switch -c \u0026#34;$(branch_name)\u0026#34; sed -i \u0026#34;s/tag:.*/tag: $(passed_tag)/\u0026#34; values.yaml git add . git commit -m \u0026#34;Update tag to $(passed_tag)\u0026#34; git push --set-upstream origin \u0026#34;$(branch_name)\u0026#34; This also felt a bit hacky to do with explicit shell commands, but it was the only way I could find to achieve my goal. I used sed to update the tag.\nResult The resulting deployment pipeline is as follows.\nI make a commit to my application repo, which triggers a build pipeline in Azure DevOps: This resulted in an image pushed to my Docker Hub: The pipeline created a new branch in my GitOps repo. Unfortunately, I have to make the PR myself, but as you can see, the pipeline successfully updates the values.yaml with the new tag which we also saw in Docker Hub: When I merged the pull request, ArgoCD detected the change and deployed a new pod with the new tag. Running a kubectl describe on the pod also verifies that we have the correct image: Conclusion This was a fun challenge, but I learned a lot from solving the problems I encountered and my entire Saturday flew by in an uninterrupted flow state. I had some good practice in setting up Azure pipelines, learned about Helm, and did my first implementation of GitOps. Not bad for a day\u0026rsquo;s work!\nLinks Application GitHub repo\nGitOps repo\nminikube\nkind\nArgoCD\ntutorial to run ArgoCD in minikube\n","permalink":"https://mischavandenburg.com/zet/articles/lab-argocd-azure-pipelines/","summary":"This weekend I had a lot of fun with a project. I wanted to learn more about GitOps and try out ArgoCD.\nMy goal was to be able to deploy an application from a GitHub repo to my local Kubernetes cluster running in minikube. There are many options I could have used, such as running Jenkins in my cluster. But I wanted to use Azure pipelines for practice, which complicates the deployment to my local cluster, because the cluster is not running on Azure.","title":"Lab Project: GitOps with ArgoCD, Azure Pipelines and Minikube"},{"content":"Introduction Have you ever thought about your identity on the internet? How does LinkedIn know it is you when you log in to LinkedIn? And when you allow LinkedIn to post to your Twitter, how does LinkedIn access your account and not your kindergarten teacher\u0026rsquo;s account?\nThis is a \u0026ldquo;Mischa Explains\u0026rdquo; article where I attempt to explain a concept in my own words as simply as possible. I use the Feynman technique and pretend to explain it to a 12-year-old.\nIdentity The first step in this process is identity. You need a starting point; for many of us, this can be our Google account. You signed up for this account and probably verified this with your phone number.\nThis relates to authentication. Authentication is the process of verifying identity. You\u0026rsquo;ll need to provide the correct password when you log in to your Google account. You must give a valid password to log in to your account and access the resources. Google uses your password to authenticate that it is you.\nAuthorization Then we have authorization. Authorization means granting access to particular resources. For example, let\u0026rsquo;s say you are working in the science classroom at school. In the classroom is a bookcase that everybody can use: it is not dangerous, and every student can take the books they need without asking for permission. In the back of the science classroom is a cabinet that contains chemicals. It would be very dangerous if everybody could go into the cabinet and take out the sulphuric acid. Not everybody might know how dangerous sulphuric acid is. That\u0026rsquo;s why the cabinet is locked.\nIf you need something from the chemicals cabinet, you need to ask permission from the teacher. You need to be authorized by the teacher to take out the sulphuric acid. When you make your request, the teacher may ask you questions to ensure you know what you are doing. He might even ask you for your school ID card because he has not seen you before. The teacher authenticates you by asking for your school ID, and then he authorizes you to take out the sulphuric acid.\nTokens How do we accomplish this on the internet?\nTo verify identities on the internet, we have identity providers. Google is an identity provider. Azure AD is also an identity provider. An open-source identity provider is Keycloak.\nIdentity providers use tokens to verify identity and authorize access to resources. There are two types of tokens: ID tokens and access tokens. And for each token, there is an associated protocol.\nID tokens OpenID Connect, also known as OIDC, is an open standard for authentication. Identity providers have agreed with each other that they will use this standard. When you go through an OpenID workflow, the result is an ID token, proving that the user has been authenticated.\nYour school ID card is the ID token in our science class example. When you started at your school, you went through a registration process. Your parents probably handled this. Your name was written down, and the school verified that it was you by looking at your passport and talking to your parents. The result of this process was your school ID card, which you use to borrow books from the library. The school ID card proves that you are a student of that school and that you can use the facilities at the school.\nAccess tokens These are specifically designed to allow access to a resource. For example, this resource could be a file on a server or a database.\nAccess tokens are strictly for authorization and use the OAuth 2.0 standard.\nIn our science class example, the token would be the key to the chemicals cabinet. The teacher authorizes you to access the cabinet and gives you the key to the cabinet.\nPutting it Together Now let\u0026rsquo;s put it together with an example.\nYou just created a new Facebook account and want to add all your friends. However, you have a Google account, and Facebook can use the contacts in your Google account to automatically add all of your friends.\nYour Google account can only be accessed by you, and your contacts are locked away behind a password. But it is possible to grant Facebook access to this.\nOn Facebook, you select the \u0026ldquo;import contacts from Google\u0026rdquo; function. Facebook sends you to Google, and Google will ask you to log in. Google is the teacher in our science class example. Google needs you to authenticate to prove that it is you. When this is done, Google generates an ID token using OIDC for Facebook: Google gives Facebook a school ID that it can use.\nNext, Facebook needs access to the contacts in your Google account. In our example, Facebook asks to take the sulphuric acid from the chemicals cabinet. You will see a menu that specifies what Facebook wants to do, and you need to give your permission. When you give your permission, Google generates an OAuth 2.0 token for Facebook. In other words, Google gives the key to the chemicals cabinet to Facebook, and Facebook is now authorized to take the sulphuric acid.\nWhen both of these tokens are generated, Facebook contacts Google and asks if it can take the sulphuric acid from the chemicals cabinet.\nGoogle, the teacher, asks Facebook for the school ID, and Facebook shows the ID card it received earlier. When Google is satisfied with the ID and successfully authenticates Facebook, it gives Facebook the key to the chemicals cabinet. Facebook is now authorized to take out the sulphuric acid. Facebook is now authorized to access the contacts in your Google account.\nLinks You can use these resources to learn more about this topic:\nAn Illustrated Guide to OAuth and OpenID connect\nID Tokens vs Access Tokens - Do you know the difference?\nMicrosoft Learn: ID Tokens\nMicrosoft Learn: Security Tokens\n","permalink":"https://mischavandenburg.com/zet/articles/identity/","summary":"Introduction Have you ever thought about your identity on the internet? How does LinkedIn know it is you when you log in to LinkedIn? And when you allow LinkedIn to post to your Twitter, how does LinkedIn access your account and not your kindergarten teacher\u0026rsquo;s account?\nThis is a \u0026ldquo;Mischa Explains\u0026rdquo; article where I attempt to explain a concept in my own words as simply as possible. I use the Feynman technique and pretend to explain it to a 12-year-old.","title":"Tokens and Identity on the Internet"},{"content":"An interesting podcast episode describing the system that runs Norway\u0026rsquo;s welfare payments. It was interesting to hear that they focus on open source and that everything runs one one big kubernetes cluster.\nIt was also very inspiring to hear that they went from 4 nightly deployments a year to 1700 deployments a week.\nLink to the podcast episode:\nThe system that runs Norway\u0026rsquo;s welfare payments\n","permalink":"https://mischavandenburg.com/zet/articles/nav-podcast/","summary":"An interesting podcast episode describing the system that runs Norway\u0026rsquo;s welfare payments. It was interesting to hear that they focus on open source and that everything runs one one big kubernetes cluster.\nIt was also very inspiring to hear that they went from 4 nightly deployments a year to 1700 deployments a week.\nLink to the podcast episode:\nThe system that runs Norway\u0026rsquo;s welfare payments","title":"Podcast Tip: The system that runs Norway's welfare payments"},{"content":"I came across this little book while doing some research for my Jiro Dreams of Sushi article. While scanning the book\u0026rsquo;s description, I was intrigued by this sentence: \u0026ldquo;the happiness of always being busy.\u0026rdquo; Although I work hard every day, I also enjoy ticking off the last item on my to-do list and enjoying some rest. Is there more happiness in being in constant activity?\nIkigai \u0026ldquo;He who has a why to live for can bear with almost any how.\u0026rdquo;\nFriedrich Nietzsche According to Japanese culture, everybody has a purpose in life: the ikigai. \u0026ldquo;Ikigai is the reason we get up in the morning.\u0026rdquo; The book explores this concept through interesting stories and brings related notions from various areas to explain ikigai to the Western mind.\nThere are many different ikigai, and people can have several ikigai simultaneously. For example, some people have their vegetable garden as their ikigai, while others have drawing or calligraphy. Jiro has making sushi as his ikigai. Even though many people have their work as their ikigai, it does not necessarily have to be the case. Ikigai can also be of a social nature, such as a family or local community.\nRetirement When I watched Jiro Dreams of Sushi for the first time, I was surprised that Jiro still worked at his restaurant every day at age 85. This book explores that further.\nApparently, many people never really retire in Japan. The concept of retirement, as we know it in the West, does not exist in the Japanese language: there is \u0026ldquo;no word in Japanese that means retire in the sense of \u0026ldquo;leaving the workforce for good\u0026rdquo; as in English.\u0026rdquo; People continue doing what they like doing as long as their health allows. They keep following their ikigai.\nThis concept was very refreshing to me and had a profound influence on how I imagine my own future. After becoming more intentional about my career, I became interested in FIRE and the possibility of retiring early. However, as I contemplated this strategy, I discovered that I didn\u0026rsquo;t want to stop working because I really enjoy my work.\nWhy would I want to retire if I\u0026rsquo;m doing what I love? That was the whole point of switching my career to IT, and this is where Western culture can learn a lot from Japanese culture. In Japanese culture, there is more emphasis on aligning your career with your interests and continuing that passion well into old age.\nWhen to quit? The job you have worked so hard for? I never once hated my job. I fell in love with my work and gave my life to it. Even though I\u0026rsquo;m 85 years old, I don\u0026rsquo;t feel like retiring.\nJiro Longevity A large part of the book is about the residents of Okinawa: the island with the highest number of people 100 years old or older. The authors tell the story of their visit to the island to reveal the secret of the Okinawan\u0026rsquo;s long lives. It is not uncommon to see people working in the fields who are well into their eighties. The Okinawans freely share their secrets with us, and the authors do a great job translating the Japanese principles into actionable advice.\nMuch of the advice is centered around dietary habits. Not only what to eat, but also how to eat it: \u0026ldquo;Okinawans stop eating when they feel their stomachs reach 80 percent of their capacity, rather than overeating\u0026rdquo;, a practice which is named \u0026ldquo;hara haci bu.\u0026rdquo; They consume a daily average of 1800 to 1900 calories, a significant difference from the 2200 to 3300 calories consumed by an average person in the US every day.\nAnother theme that keeps returning is the crucial function of social groups for finding meaning in life, especially in old age: \u0026ldquo;It is customary in Okinawa to form close bonds within local communities. A moai is an informal group of people with common interests who look out for one another. For many, serving the community becomes part of their ikigai.\u0026rdquo;\nThis book may be a good resource if you are interested in longevity. Even though I only expected to read about the ikigai concept, it contains a surprisingly large amount of valuable advice and practical tips to live a longer and happier life.\nChock-full \u0026ldquo;Ikigai: The Japanese Secret to a Long and Happy Life\u0026rdquo; is a small book that covers an astonishing amount of subjects. It covers topics such as meditation, flow states, and exercise methods such as Tai Chi and Qi Gong. The disadvantage is that it does not deeply cover any of these topics. But I think this book can be a very useful springboard for further research and a great introduction to many different subjects that can improve your life.\nIt is a short read, but it contains a lot of wisdom, and I learned much from this book. It has changed the way I think of retirement and the way I approach my work. I am also very interested in health and longevity. Through the conversations with the centenarians of Okinawa, I received a lot of helpful tips, which I\u0026rsquo;m already applying to my daily life.\nHave I discovered the art of being busy? I believe I took this definition too literally when I started reading this book. I found that Okinawans lead active lives full of meaning but also take plenty of rest. Always being busy does not mean that you have to work yourself to death. However, it does mean that the secret to reaching a healthy old age is to keep doing what you love. There is no need to become sedentary and only sit in front of the TV when you turn 67. I think Western culture has a lot to learn in this area.\n","permalink":"https://mischavandenburg.com/zet/articles/ikigai-book/","summary":"I came across this little book while doing some research for my Jiro Dreams of Sushi article. While scanning the book\u0026rsquo;s description, I was intrigued by this sentence: \u0026ldquo;the happiness of always being busy.\u0026rdquo; Although I work hard every day, I also enjoy ticking off the last item on my to-do list and enjoying some rest. Is there more happiness in being in constant activity?\nIkigai \u0026ldquo;He who has a why to live for can bear with almost any how.","title":"Book Notes: Ikigai: The Japanese Secret to a Long and Happy Life"},{"content":" This book is the sequel to the Phoenix project. Both books are set at Parts Unlimited, a fictitious company that supplies car parts to DIY mechanics and repair shops. Phoenix is a new system that Parts Unlimited has worked on for multiple years. It is supposed to handle order processing and communication between manufacturing, stores, and clients. Phoenix will also play a role in sales and marketing. The company has been gathering customer data for years, but it cannot use any of this data yet. Phoenix will enable it to generate targeted marketing campaigns from the data when it\u0026rsquo;s finished.\nBut as we saw in the previous book, it is far from finished, and things go wrong all the time. The company is not doing well, the stock prices are falling, and they need an edge over the competition. Phoenix will be their edge, but they\u0026rsquo;ve been working on it for years. Eventually, management decided that Phoenix needed to be deployed in two weeks. But it is far from ready.\nThe main character in the Unicorn project is Maxine, a senior developer who temporarily transferred to a different department. She had to work on the Phoenix project against her will because of an unfortunate situation that needed a scapegoat. However, Maxine decides to make the best out of it, and she\u0026rsquo;d like to begin with one thing: to get a Phoenix build going on her laptop.\nVery quickly, she finds it impossible to run a full build of the Phoenix project due to missing files and other elements. She is appalled and makes it her mission to get the build going, but she meets another hurdle every step of the way. Missing credentials. Missing binaries and libraries. And for each of these hurdles, she must submit a ticket with a different department. Very soon, she has over 20 tickets running with long waiting times. Just to get a build going on her machine so she can work! Dozens of developers were hired to work on the Phoenix project. But when she asks them if they\u0026rsquo;ve managed to get a build going yet, Maxine is horrified to discover that they\u0026rsquo;ve tried for several months but haven\u0026rsquo;t made any progress. Maxine has made more progress in a week.\n\u0026ldquo;Everyone around here thinks features are important because they can see them in their app, on the web page, or in the API. But no one seems to realize how important the build process is. Developers cannot be productive without a great build, integration, and test process.\u0026rdquo;\nUnicorn Project After a few weeks, Maxine receives an invitation to have a drink with a group of people who are very interested in her. When she arrives at the bar, she meets the Rebellion: a group of developers, managers, and people from Operations, who are tired of the old organizational structure and want to make real changes. They think out of the box and experiment with new technologies, even though they are not authorized to do so.\nWith the Rebellion, Maxine significantly improves the build and deployment process. They recognized that Phoenix actually never was being built in its entirety. Developers were always working on parts of the application. However, after a lot of struggle, they create a build process that enables each developer to become operational on his first day.\nThis is the first step of a long series of exciting events that lead to Phoenix becoming a success. By the end of the book, they have a completely new development and testing process, and they can deploy changes to production without needing to take the entire application down. This allows them to create targeted marketing campaigns and respond to changes in the market. The first campaign was a huge success and generated the highest sales ever.\nMaxine\u0026rsquo;s struggle with the build process was an eye-opening experience for me. It gave me a very practical example of the need for DevOps principles to enable delivering value to customers. It is also something I recognize in my current organization. For example, projects can get stuck on a firewall change that needs to be approved by an external party. By implementing DevOps principles and arranging teams according to the \u0026ldquo;you build it, you run it\u0026rdquo; principle, teams can be responsible for the entire process from idea to production and therefore have a very short release cycle for their application.\nI thoroughly enjoyed the first part of the book. However, the second part was less engaging to me. It became long-winded and felt like butter spread over too much bread. The author demonstrates a high level of technical experience and knowledge through his descriptions of processes, deployments, and fictional applications. Although I understand the intention of making Parts Unlimited a believable company, I think it could have been accomplished with much less detail and words.\nThe second part has more corporate drama, such as temporarily suspended managers without any clear reason. The focus shifts from a development and operations perspective to a managerial perspective. Maybe I will reread the book in a few years and this part will make a lot more sense to me then. The same happened when I reread the Phoenix project. I could not understand some aspects of the book, which became much clearer to me when I revisited it after gaining experience in the field.\nI highly recommend this book to anyone working as a developer, DevOps Engineer, or in operations, especially if you are starting your career. The book gave me a lot of insights into \u0026ldquo;the old way of working\u0026rdquo; and a better understanding of the need for DevOps principles in the modern IT landscape. However, make sure to read the Phoenix project first.\nThe Unicorn Project: A Novel about Developers, Digital Disruption, and Thriving in the Age of Data by Gene Kim ","permalink":"https://mischavandenburg.com/zet/articles/unicorn-project/","summary":"This book is the sequel to the Phoenix project. Both books are set at Parts Unlimited, a fictitious company that supplies car parts to DIY mechanics and repair shops. Phoenix is a new system that Parts Unlimited has worked on for multiple years. It is supposed to handle order processing and communication between manufacturing, stores, and clients. Phoenix will also play a role in sales and marketing. The company has been gathering customer data for years, but it cannot use any of this data yet.","title":"Book Notes: The Unicorn Project"},{"content":"Last week I watched \u0026ldquo;Jiro Dreams of Sushi\u0026rdquo; again. It must be the eighth time I revisited this piece of art. I have very little interest in sushi, but there is much more to this documentary. It was first recommended to me by my good friend Anders more than five years ago, but I couldn\u0026rsquo;t grasp its underlying life lessons the first time around.\nEven though I couldn\u0026rsquo;t fully understand the message, I was intrigued by it and kept returning to the documentary. Over the years, I managed to grasp more of its deeper meaning. The documentary is about Jiro: one of the best sushi chefs in the world. His tiny restaurant, located in a subway station, even received a 3 Michelin star rating. So how does a man operating from a subway restaurant become the best in the world? And what can a DevOps Engineer learn from a sushi chef?\nJiro is referred to as a shokunin, a Japanese cultural phenomenon that doesn\u0026rsquo;t have a direct translation into English. A shokunin is an artisan or a craftsman who has devoted his entire life to becoming a master of his craft. For example, there are shokunin carpenters, weavers, and blacksmiths. The shokunin achieve mastery by doing the same action over and over again and trying to improve with every repetition.\n\u0026ldquo;Once you decide on your occupation, you must immerse yourself in your work. You have to fall in love with your work. Never complain about your job. You must dedicate your life to mastering your skill. That\u0026rsquo;s the secret of success and is the key to being regarded honorably.\u0026rdquo;\nJiro We get to know Jiro as a man who is devoted to his occupation. He follows the same routine every day; he even gets on the train from the same position. He says he dislikes holidays and prefers to be at work. He used to get up at 5:00 and get home after 22:00. Even at age 85, he\u0026rsquo;s still working at the restaurant every day, simply because he doesn\u0026rsquo;t want to do anything else.\nBut it\u0026rsquo;s not just about putting in long hours and working past the retirement age. It\u0026rsquo;s also about the mindset and the attitude that Jiro has adopted towards his craft:\n\u0026ldquo;I\u0026rsquo;ve seen many chefs who are self-critical, but I\u0026rsquo;ve never seen another chef who is so hard on himself. He sets the standard for self-discipline. He\u0026rsquo;s always looking ahead. He\u0026rsquo;s never satisfied with his work. He\u0026rsquo;s always trying to find ways to make the sushi better or to improve his skills. Even now, that\u0026rsquo;s what he thinks about every day.\u0026rdquo;\nMasuhiro Yamamoto, Food Critic. Finding my Craft It was hard to decide on my occupation. It took me 32 years and a lot of trial and error to find the answer. I have always envied people who knew they wanted to become a nurse since childhood.\nWhat occupation should I choose? I answered this question by answering another question: what do I like to do in my free time? The answer was clear: I tinkered with computers. I had already coded several websites and loved experimenting with game automation. I ran \u0026ldquo;bot farms\u0026rdquo; on Linux servers which I configured myself from the command line.\nI decided to make IT and tech my occupation. Fortunately for me, the job market was in my favor, and employers were much more willing to consider candidates without a formal background in IT. If you\u0026rsquo;d like to read more, I wrote about my journey into DevOps in this article.\nThe Ways of the Shokunin After making the career change, I adopted the shokunin mindset and dedicated my life to mastering my craft. In practical terms, I needed to commit to devoting my time to my profession and avoid straying from it. Making this commitment wasn\u0026rsquo;t very difficult because I chose my career based on my free-time activities. It\u0026rsquo;s more about adopting a mindset of striving to make everything relate to each other. For example, I try to select hobby projects that directly relate to what I\u0026rsquo;m working with during the day. This way, my leisure activities will strengthen my professional skills, and my professional expertise will improve my hobby projects, creating a feedback loop that will eventually lead to results.\nHowever, sticking to one thing does not necessarily come naturally to me. I have a tendency to pick up many different hobbies and get very excited about them. First, I will become extremely interested in a particular subject. Then, after approximately three months, I put it aside and become excited about something else. This isn\u0026rsquo;t necessarily a case of Shiny Object Syndrome because I tend to return to these hobbies in a cyclical pattern. As I apply myself to learning the skill with a feverish intensity, I\u0026rsquo;m able to make a lot of progress during these bouts of obsession. But to the outside world, it might seem like I\u0026rsquo;m constantly changing my mind about what I want.\nI always considered this a negative character trait, but over the past few years, I\u0026rsquo;ve learned to embrace it and guide this tendency in the right direction. Because I chose IT as my occupation and DevOps as an area of specialization, I gave myself a broad scope of interest with many sub-skills to learn. If you look at the DevOps Roadmap, you\u0026rsquo;ll see that a good DevOps Engineer must master many different skills.\nIt\u0026rsquo;s like I\u0026rsquo;ve given myself a large playground with a fence around it, full of exciting things to learn. I can go down a Python rabbit hole for a few months and improve my coding skills. Later, I find myself sucked into builing my own OS and learning more about Linux in the process. The fence around the playground isn\u0026rsquo;t locked, but I do my best to stay inside the fence. When, for example, I start getting the urge to get back into music production again, I consider that it will take up a few hours a day to get back on track. Investing these hours into something that brings me joy, but is also related to my occupation, would be a better option. Reflecting in this way keeps me focused on my goals.\nMy website is another example of this fusion between professional and free-time activities. I love to write. It\u0026rsquo;s a relaxing activity, even though it is very challenging sometimes. After I learn about a new subject, I try to write about it. This is the best way I know to verify whether I\u0026rsquo;ve really understood the topic. When I sit down to write about something, I force myself to think clearly about it and to make sure everything aligns properly in my mind. When I cannot explain the concepts in a few clear sentences, I know I don\u0026rsquo;t fully understand it yet.\n\u0026ldquo;Clear thinking becomes clear writing; one can\u0026rsquo;t exist without the other.\u0026rdquo;\nWilliam Zinsser, On Writing Well Another way my blog supports my professional development is because I prefer to write my blog posts in neovim in the markdown format. Writing and editing text often involves moving a lot of words and paragraphs around. Because I use vim to do my writing, I\u0026rsquo;m also constantly practicing the keybinds I use for writing and editing code during my day job. I often learn a new motion for my blog writing, which I use the next day at work.\nNever Finished \u0026ldquo;All I want to do is make better sushi. I do the same thing over and over, improving bit by bit. There is always a yearning to achieve more. I\u0026rsquo;ll continue to climb, trying to reach the top, but no one knows where the top is. Even at my age, after decades of work, I don\u0026rsquo;t think I have achieved perfection. But I feel ecstatic all day. I love making sushi. That\u0026rsquo;s the spirit of the shokunin.\u0026rdquo;\nJiro Jiro taught me never to be satisfied with my skills. If I desire to become a master of my craft, there will never be a point where I can lean back and think that I\u0026rsquo;ve learned enough. There\u0026rsquo;s always something to improve, which is why I chose an IT career. My thirst for learning cannot be quenched, and there\u0026rsquo;s always something to learn in this field.\nDevotion Although it might seem like a simple food documentary, do not be deceived. Jiro Dreams of Sushi is full of valuable life lessons. It\u0026rsquo;s one of the documentaries which I revisit regularly. It\u0026rsquo;s relaxing to watch, and I get hugely inspired by Jiro\u0026rsquo;s devotion to his craft and powerful teachings every time.\nJiro\u0026rsquo;s guidance has changed my life. He made me realize that if I wished to become a master of my craft, I needed to devote my life to it. I changed my lifestyle, so the things I do in my free time strengthen my professional skills. I also became more mindful of the content I consume and the things I read, trying to keep it related to my occupation. He showed me the true meaning of the phrase \u0026ldquo;my work is my hobby.\u0026rdquo;\n\u0026ldquo;Always look beyond and above yourself. Always try to improve on yourself. Always strive to elevate your craft. That\u0026rsquo;s what he taught me.\u0026rdquo;\nYoshikazu, Jiro\u0026rsquo;s oldest son. ","permalink":"https://mischavandenburg.com/zet/articles/jiro-sushi/","summary":"Last week I watched \u0026ldquo;Jiro Dreams of Sushi\u0026rdquo; again. It must be the eighth time I revisited this piece of art. I have very little interest in sushi, but there is much more to this documentary. It was first recommended to me by my good friend Anders more than five years ago, but I couldn\u0026rsquo;t grasp its underlying life lessons the first time around.\nEven though I couldn\u0026rsquo;t fully understand the message, I was intrigued by it and kept returning to the documentary.","title":"I'm In Love with my Work: Lessons from a Japanese Sushi Master"},{"content":"If you are just starting your Linux journey, you might have noticed that a few camps exist in the Linux world. Just like in any other area of life, it seems that groups of human beings enjoy dividing themselves instead of living in harmony. There are camps centered around Linux distributions (I use Arch, btw) but also around text editors.\nThe Beginning The reason why I started to use vim is rather practical. When I was studying to become a Cloud Engineer, I had access to subscriptions on AWS and Azure to experiment with virtual machines. This was a perfect place to learn to work with Ansible. Many of the labs projects I did involved setting up a few virtual machines, and I destroyed many VMs when I made some big mistakes in the configuration.\nI was using Visual Studio Code at the time on my local machine, but I had to connect to a new virtual machine multiple times a day. It became very tiresome to set everything up with VSCode every time, or pulling the files to my local machine and copying them over again. So I just ssh\u0026rsquo;ed into the machines to edit the text files with the included editor, which happened to be vim.\nObsession in its Infancy When you first use vim, it is a rather disorienting experience. But in every tutorial, I was told it would be difficult in the beginning but much faster and more effective in the end. I found this very appealing because I like to do things the hard way and challenge myself.\nI discovered that there were people out there who did all of their text editing and coding in vim. I met programmers who refuse to use anything else and people who write entire books in vim. So there had to be something to it.\nIt also fitted very well with my intention of working on the command line as much as possible and moving away from GUI applications whenever possible. I like to move in this direction because I love the idea of controlling your entire workflow with your keyboard instead of using your mouse, and vim fits perfectly into this picture.\nWhat I like after Nine Months At this point, I\u0026rsquo;ve been using vim as my primary text editor for about nine months. In my current job, I work a lot with yaml files stored in private git repositories.\nI only work with these files from the command line, and I don\u0026rsquo;t have any other code editor installed. I use ripgrep and fzf (fuzzy file finder) to search through the files, and I use neovim to edit them. When I need to search for files from within vim, I use the awesome Telescope plugin.\nIn these months, I\u0026rsquo;ve picked up a few tricks, and I am starting to see the power of vim. The best thing I like about it is that I don\u0026rsquo;t have to leave my terminal window to do the tasks I need to do. Instead, I can search through the files I need to work with, open them, make adjustments, and commit them to the repository. Then I enter the command to run the ansible playbook, and it all happens in the same window, and I don\u0026rsquo;t have to lift my fingers from the keyboard.\nKeyboard Shortcuts Now that I am gaining more experience with vim, I\u0026rsquo;m picking up more advanced usages that significantly improve my workflow. For example, \u0026ldquo;da(\u0026rdquo; meaning \u0026ldquo;delete around parentheses\u0026rdquo; to quickly delete the text between two parentheses. Or \u0026ldquo;da\u0026lt;\u0026rdquo; to very quickly delete HTML tags. Another great feature is the visual block mode, where I can add comment tags to many lines simultaneously, for example.\nSearching and Navigation Navigating large text files has become incredibly quick since I started using vim. Of course, it takes some getting used to, but it is a lovely experience to open a file, press / to search and enter the keyword and immediately arrive at the point I need to be—no scrolling with the mouse and no need to lift my hands from the keyboard.\nI also love the ability to jump from sentence to sentence using ) or paragraphs using }.\nMultiple Files It takes a little while to get used to, but when you get into it, it is effortless to open up two files at a time if you need information from both. Often I need data from 4 or more files, and opening them quickly with keyboard commands has significantly improved my workflow speed.\nCustomization One of the things I enjoy most about vim is the ability to customize it exactly to my needs. I\u0026rsquo;m completely in charge of the plugins which are loaded into vim and which colors it uses, and this appeals a lot to me. However, it can be rather overwhelming in the beginning. To be honest, it is still overwhelming after ten months. It can be tough to get an idea of where to start, which plugins you need, and which settings you need to change.\nI just started with the base install of vim and started from there. Every time I required a particular functionality, I searched around to see if a plugin was available. Very often, someone out there had the same problem as you and created a plugin for it. For example, I recently installed a plugin for using emojis in vim 😄\nHow to Get Started The short answer is to simply start using vim for all of your text editing, whether it be coding or writing for pleasure. It is a cliche to say, but it will be hard in the beginning, but I promise you it will pay off in the end.\nThe second thing I\u0026rsquo;d recommend is to run vimtutor on a Linux machine. Do this once a day for a couple of weeks, and you\u0026rsquo;ll know how to edit text files on any Linux system for the rest of your life, which is a precious skill.\nFinally, don\u0026rsquo;t spend too much time reading about all the available plugins. Your needs will become apparent to you as you start to use vim for all of your tasks, and you can search for plugins to address those needs. This way, you start with a minimal editor, which you\u0026rsquo;ll build according to your needs.\nGood Luck! ","permalink":"https://mischavandenburg.com/zet/articles/how-started-vim/","summary":"If you are just starting your Linux journey, you might have noticed that a few camps exist in the Linux world. Just like in any other area of life, it seems that groups of human beings enjoy dividing themselves instead of living in harmony. There are camps centered around Linux distributions (I use Arch, btw) but also around text editors.\nThe Beginning The reason why I started to use vim is rather practical.","title":"How and Why I Started Using Vim"},{"content":" When I started my DevOps traineeship, I borrowed this book from my boss and read it from cover to cover. I loved the story and the characters; it helped me understand \u0026ldquo;the old way\u0026rdquo; of doing things and the merits of implementing DevOps principles.\nI reread the book ten months later. In the meantime, I\u0026rsquo;ve learned many new skills and technologies and started working full-time as a DevOps engineer.\nNot surprisingly, it made a lot more sense to me this time, and I\u0026rsquo;m sure it will be even better when I reread it a few years later. In this article, I\u0026rsquo;ll share my thoughts and notes on the book.\nThe Story The main character is Bill Palmer, a mid-level IT manager in a manufacturing company called Parts Unlimited. Within a few pages, he is called into the CEO\u0026rsquo;s office, and he is promoted to the VP of Operations, putting him in charge of IT, much against his own will or desire.\nThe situation Bill enters is a humorously chaotic one. We are thrown straight into a Sev 1 incident where managers point fingers and shout at each other. We quickly get the impression that this is a dysfunctional department that only performs tasks for the manager who shouts the loudest while fighting off crippling outages.\nIt’s like the Wild West out here. We’re mostly shooting from the hip.”\nThe Phoenix Project\nThe bulk of the story revolves around how Bill, together with his team of managers, Wes, Patty, and John, manage to turn this chaos into a department that does work according to a streamlined plan in a much more predictable manner.\nTo achieve this goal, Bill is introduced to Erik, a prospective board member of the company. Erik becomes Bill\u0026rsquo;s mentor and guides Bill through the process of creating order in the chaos. Their interaction reminds me of Zen masters training their disciples by asking deep questions which don\u0026rsquo;t have an immediately apparent answer.\nMaster \u0026amp; Disciple Erik takes Bill to the manufacturing plant of Parts Unlimited and tries to impress upon Bill that manufacturing planning principles from Lean can be applied to IT work. Erik argues that an IT department could be structured like a factory production line, but Bill is not ready to accept this.\nA fundamental notion from manufacturing principles is that work should always be moving forwards along the production line, never backward. But unfortunately, this is very often the case in the \u0026ldquo;old\u0026rdquo; way of working: the development team works on an application for several months, and when they are finished with it, they throw it over the fence to the Operations people, whose job it is to deploy the application.\nOne of the developers had actually walked in a couple of minutes ago and said, “Look, it’s running on my laptop. How hard can it be?”\nThe Phoenix Project\nHowever, as we see happening time after time in the book, usually the application is incompatible with the infrastructure it is deployed to. As a result, the application needs to go back to development. According to manufacturing theory, this is a situation where work goes backward through the production line, which we must avoid at all costs.\nImplementation Erik challenges Bill to start doing ten deployments a day instead of one deployment every nine months. Understandably, this is a ridiculous notion to Bill. The last few deployments were disastrous events that required his entire department to pull all-nighters through the weekend, and still, the stores were not managing to process all orders and payments.\nHowever, Bill takes his mentor\u0026rsquo;s advice and figures out a way to do it together with his team. One of the main problems they uncovered was the inconsistent deployment and production environments.\nThe solution to this problem was to involve the operations people in the development stage right from the beginning, so the development environment matched the production environment exactly. The environments were standardized and put in code with version control, and things started progressing quickly.\nAs Wes talks, I think about Erik challenging me to think like a plant manager as opposed to a work center supervisor. I suddenly realize that he probably meant that I needed to span the departmental boundaries of Development and IT Operations. The Phoenix Project\nThis is just one of the problems addressed by melting away the fence between Development and Operations. By the end of the book, the two camps started to work together much better. They come closer to the target of 10 deployments a day, and the DevOps way of working was born.\nClosing Thoughts I think this book is a must-read for anyone considering entering the DevOps field or anyone already working with DevOps.\nAs a nerd who loves structure and organization, the theme of the story is incredibly entertaining and satisfying to me. The authors excellently capture the transition from an utterly disorganized situation to a predictable environment with happy co-workers. Actually, I\u0026rsquo;m a little embarrassed by how much joy this transition brings me.\nEspecially the second time around, it helped me better understand the underlying principles that enable the DevOps way of working in an organization. Moreover, it paints a great picture of how an organization can change for the better by embracing DevOps principles and how these changes express themselves in the improved quality and speed of software development and deployment. All of these advantages lead to delivering better value to the customer, which is the core focus of any productive and creative endeavor involving customers and end users.\nThe Phoenix Project, written by Gene Kim, Kevin Behr, George Spafford ","permalink":"https://mischavandenburg.com/zet/articles/phoenix-project/","summary":"When I started my DevOps traineeship, I borrowed this book from my boss and read it from cover to cover. I loved the story and the characters; it helped me understand \u0026ldquo;the old way\u0026rdquo; of doing things and the merits of implementing DevOps principles.\nI reread the book ten months later. In the meantime, I\u0026rsquo;ve learned many new skills and technologies and started working full-time as a DevOps engineer.","title":"Book Notes: The Phoenix Project"},{"content":"I recently obtained my CKA certification. I started this certification journey with zero knowledge of Kubernetes. However, I was already working as a DevOps Engineer, and I know a fair bit of Linux. I daily drive Arch Linux and have LPIC-1 certification. It was handy to know where files are located on Linux systems and how to interact with systemd services. I also knew yaml quite well because I work with Ansible daily. I passed on my first try, and I did one session of killer.sh.\nMy preparation KodeKloud CKA Course Killer.sh Mock Exam Killercoda I kept track of the time I spent on this certification. In total, I spent 80 hours on study and practice.\nIn Hindsight I spent too much time repeating things during the KodeKloud course. This is the one thing I would do differently if I could start over. I went over some modules multiple times and kept meticulous notes. However, I have hardly used any of those notes. But they will be nice to have for the future.\nI learned most from the killer.sh exams. So I would advise you to go through the KodeKloud course and do all the exercises, but don\u0026rsquo;t spend too much time repeating stuff. If you don\u0026rsquo;t understand the topic at all, it is, of course, necessary to repeat it. But you don\u0026rsquo;t need to know all the details.\nKiller.sh After I finished the KodeKloud course, I purchased the exam voucher and started the killer.sh on Saturday morning. I wanted to simulate the exam experience as much as possible, so I set the timer and did not allow myself to stand up for two hours. My first round was humiliating. I only managed to get 24 out of 125 points. A little shocked by the experience, I spent the whole Saturday going through all the solutions of the exercises that killer.sh provides. The explanations they give are extensive, and I found them helpful. Saturday evening, I went out for dinner with friends, and on Sunday morning, I passed killer.sh. I spent the whole Sunday studying the solutions more and more, and on my last try on Sunday evening, I scored 115 out of 125.\nTips: I know tmux quite well and used it extensively during the killer.sh, but it was not necessary during the exam. No need to learn it if you don\u0026rsquo;t know it already.\nKnowing vim well will save you a lot of time at the exam. For example, dG to delete all lines until the end of the file from your current location. Run \u0026ldquo;vimtutor\u0026rdquo; on a Linux system to learn the basics.\nYou cannot use bookmarks. Learn how to search the docs efficiently. One handy one I figured out was to control + F and enter \u0026ldquo;kind: Pod\u0026rdquo; or \u0026ldquo;kind: PersistentVolume\u0026rdquo; to immediately go to the example YAML.\nmy exam environment did not need much extra configuration. All I added to my .bashrc was alias v=vim and export do=\u0026quot;\u0026ndash;dry-run=client -o yaml\u0026quot; so you can use \u0026ldquo;k run Nginx $do \u0026gt; Nginx.YAML\u0026rdquo;\nThe exam environment is not as bad as people make it out to be on the internet. There is a little delay while scrolling through the docs in the browser, but working in the terminal didn\u0026rsquo;t give me any problems. Get used to the environment on killer.sh, and there should not be any surprises in the real exam environment.\nSkip questions you cannot solve immediately. But don\u0026rsquo;t spend time reviewing all the questions, sorting by the highest % and doing those first. You will lose a lot of time evaluating all of these questions. It is much better to solve the questions during your first pass through and skip the ones you cannot immediately solve.\nWhen the 120-minute timer ran out, I was presented with a screen that said \u0026ldquo;quit\u0026rdquo; or \u0026ldquo;request more time.\u0026rdquo; I was pretty sure I could not get more time for this exam, so I just pressed \u0026ldquo;quit.\u0026rdquo; After I pressed quit, the application closed immediately, and there was no confirmation whatsoever that they received my exam results or anything. This was extremely disorienting, and I was left doubting if I had done it correctly. Eventually, I could see in the Linux Foundation portal that my exam was in Grading status.\nSpeed is of the essence. An hour before my exam, I used killercoda to get into the mood and get things up to speed. Learn to solve things quickly and don\u0026rsquo;t spend time having to arrange terminal windows on your screen or stumbling around in vim. You cannot afford to lose time on these things.\nFinally, this video is an excellent summary of all the necessary tips and information: ","permalink":"https://mischavandenburg.com/zet/articles/cka-tips/","summary":"I recently obtained my CKA certification. I started this certification journey with zero knowledge of Kubernetes. However, I was already working as a DevOps Engineer, and I know a fair bit of Linux. I daily drive Arch Linux and have LPIC-1 certification. It was handy to know where files are located on Linux systems and how to interact with systemd services. I also knew yaml quite well because I work with Ansible daily.","title":"Certified Kubernetes Administrator (CKA) Exam Tips"},{"content":"NOTE: In this article, I use a rather broad definition of “Operating System.” I do not intend to appear as if I wrote and compiled my own Linux kernel, nor do I understand the inner workings of the kernel written in C. Instead, with “building my OS from scratch,” I intend to convey that I used a minimal Linux distribution as a starting point and started building from there.\nintroduction I started using GNU/Linux as a daily driver about six months ago, and I have not regretted the decision ever since. There has not been a single use case where I needed to use Windows for anything at all.\nAs I was getting more used to daily driving Linux, I noticed how much I enjoyed the ability to customize my operating system and workflow. Finally, after spending a weekend going down the customization rabbit hole, I had a good-looking terminal and customized neovim to perform as I needed it.\nNot much later, I came across Arch Linux and the idea of building your own operating system from the ground up. I was instantly intrigued and knew I wanted to do the same. A few months have passed since I first came across Arch Linux, and now I am writing this blog post in neovim on my custom OS that I created from scratch. The font, the spacing, the colors, everything is exactly how I like it, and I love using it.\nWhen you first install Arch Linux, all you get is a black screen with a blinking cursor. However, the experience of creating a fully functional graphical environment from “nothing” has been extremely satisfying, and I learned so much about GNU/Linux in the process. I also realized that this could be seen as a creative activity, like a painter creating his masterpiece from a blank canvas or a sculptor carefully chipping away at a block of marble.\nnot just graphics When I say customization, I am not just referring to the visual aspects of the operating system. The things going on “under the hood” must also be carefully configured when you use a minimal distro such as Arch Linux.\nArch Linux comes with very few packages preinstalled, and every time you wish to add something to your system, you need to install it and enable the service in systemd. For example, after I did the installation and created my user account, I needed to run a command with root privileges. To my great surprise, even the “sudo” command was unavailable and needed to be installed.\nThis is the aspect I learned most from. Whenever I desired a certain functionality from my operating system, I needed to install and enable it. This has given me a much better understanding of the processes and daemons running on my system. It has also given me a greater appreciation of all the elements needed to provide a working environment.\ngraphic violence When you create an Arch Linux installation USB and boot it up, you are greeted with a command line and nothing else.\nWhen you install something more beginner-friendly, such as Ubuntu or Manjaro KDE, your installation will include a graphical desktop environment. But on Arch Linux, you must install and configure this yourself. Furthermore, to be able to render a graphical environment, you will also need to install and configure a display server such as Xorg.\nWhen I started on my journey, I intended to create something that used minimal resources with a minimal look. Having used GNOME on Manjaro for a few months, I was very satisfied, but I wanted to try a tiling window manager to shave down even more resource usage. After some research, I ended up with the Awesome Window Manager. Here are some screenshots of the final result:\nThis is what my desktop looks like when I boot up.\nHere I’m editing my window manager configuration file, while I have a browser open and keep an eye on my system resources\nmy music listening setup, using mpd + ncmpcpp, cava and sptlrx. the lyrics are shown in real time as the music is played.\ncreativity The Cambridge Dictionary defines creativity as “the ability to produce original and unusual ideas, or to make something new or imaginative.”\nWhen you embark on a journey, such as creating your operating system, you will probably start with a particular intention or a goal that you will work towards. With this goal in mind, you can start searching for the tools and color schemes you need to create the system that you have in mind. The result is a unique combination of tools, colors, fonts, and programs specifically tailored to your needs and wants and chosen by you.\nIs this any different from a painter starting with a blank canvas or a musician starting with a fragment of a melody, ending up with a complete symphony? Entering commands into a computer terminal might not strike everybody as a creative activity. Still, I have found that it is a very effective and satisfying way of expressing myself and creating something I love to use daily. As an IT professional, I spend most of my time behind my computer. Doesn’t it make sense to put effort into building something customized to your needs?\nresources If you want to start building your own OS, I recommend these resources:\nArch Linux\nArch Wiki\nr/unixporn – a subreddit about customization\n","permalink":"https://mischavandenburg.com/zet/articles/linux-creativity/","summary":"NOTE: In this article, I use a rather broad definition of “Operating System.” I do not intend to appear as if I wrote and compiled my own Linux kernel, nor do I understand the inner workings of the kernel written in C. Instead, with “building my OS from scratch,” I intend to convey that I used a minimal Linux distribution as a starting point and started building from there.\nintroduction I started using GNU/Linux as a daily driver about six months ago, and I have not regretted the decision ever since.","title":"Building my Own OS: Linux as a Creative Activity"},{"content":"Last week I upgraded Jenkins to the latest version on the server infrastructure at work. Starting with the Jenkins 2.357 release, Java 11 or Java 17 will be required to run Jenkins. Also, the upcoming LTS release will require Java 11.\nThis means that I also needed to update Java on our Jenkins servers. Here are the steps that I did to perform the Jenkins and Java upgrade.\nSSH into the server and stop the service. Then get the latest upgrades for your server, which is good practice:\nservice jenkins stop apt-get update apt-get upgrade Depending on your setup, the apt-get upgrade command might upgrade Jenkins to the latest version that does not require Java 11+. In my case, that was 3.346.\nWhen you get a question about updating your current config file, take the default option. This option keeps your current configuration.\nHowever, if your Jenkins is installed from a binary or another source, you might need to upgrade Jenkins to 3.346 using the Jenkins.war file:\ncd /usr/share/jenkins mv jenkins.war jenkins.war.old wget https://updates.jenkins-ci.org/latest/jenkins.war service jenkins start When you start Jenkins, it will be updated to the latest version that does not require Java 11 or higher. You will notice that there will be a new folder called migrate in /usr/share/jenkins , and the jenkins.war is now located in /usr/share/java\nThis is where I got confused because it did not patch to the latest version, only up to 3.346 and the jenkins.war file was no longer being updated from the /usr/share/jenkins folder.\nThe reason is that this update moves the .war file to the /usr/share/java directory.\njava To get Jenkins to the latest version, we need to install or update Java and check if it has worked:\napt-get install default-jre java -version Now that you have updated the java version, you are ready to update Jenkins to the latest version.\nNotice that we use the /usr/share/java folder now, instead of /usr/share/jenkins\nservice jenkins stop cd /usr/share/java mv jenkins.war jenkins.war.old wget https://updates.jenkins-ci.org/latest/jenkins.war service jenkins start nodes When I accessed the Jenkins GUI, everything seemed fine, and my version was up to 3.358.\nHowever, I noticed that the build nodes were all offline. When inspecting the logs, I saw the following error:\njava.io.EOFException at java.base/java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2905) at java.base/java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:3400) at java.base/java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:936) at java.base/java.io.ObjectInputStream.\u0026lt;init\u0026gt;(ObjectInputStream.java:379) at hudson.remoting.ObjectInputStreamEx.\u0026lt;init\u0026gt;(ObjectInputStreamEx.java:49) at hudson.remoting.Command.readFrom(Command.java:142) at hudson.remoting.Command.readFrom(Command.java:128) at hudson.remoting.AbstractSynchronousByteArrayCommandTransport.read(AbstractSynchronousByteArrayCommandTransport.java:35) at hudson.remoting.SynchronousCommandTransport$ReaderThread.run(SynchronousCommandTransport.java:61) Caused: java.io.IOException: Unexpected termination of the channel at hudson.remoting.SynchronousCommandTransport$ReaderThread.run(SynchronousCommandTransport.java:75) Observing that the error had something to do with Java, I ssh’d into the build nodes and updated Java there as well with the same command:\napt-get install default-jre After updating Java on the build node, head back to the GUI on the master node and restart the build node.\nIt should now be online again.\n","permalink":"https://mischavandenburg.com/how-to-upgrade-java-and-jenkins-on-ubuntu-18-04/","summary":"Last week I upgraded Jenkins to the latest version on the server infrastructure at work. Starting with the Jenkins 2.357 release, Java 11 or Java 17 will be required to run Jenkins. Also, the upcoming LTS release will require Java 11.\nThis means that I also needed to update Java on our Jenkins servers. Here are the steps that I did to perform the Jenkins and Java upgrade.\nSSH into the server and stop the service.","title":"How to Upgrade Java and Jenkins on Ubuntu 18.04"},{"content":"In this post, I’d like to share the five main reasons why I changed my career to IT in my thirties. Making a career change can be daunting, especially when you are past your twenties, and employers can get more skeptical of hiring and training you. However, when I passed my twenties and became more serious and intentional about my life and career, I decided to take the plunge and hope for the best. These are listed in no particular order.\n#1 job opportunities IT has always been a field with many job opportunities, and with the current movements of digitalization and automation, there is no sign that it will slow down.\nAccording to the U.S. Bureau of Labor Statistics, “Employment in computer and information technology occupations is projected to grow 13 percent from 2020 to 2030, faster than the average for all occupations. These occupations are projected to add about 667,600 new jobs. Demand for these workers will stem from greater emphasis on cloud computing, the collection and storage of big data, and information security.”\nThe situation is no different here in the Netherlands. Currently, there is a shortage of people in IT, and employers are much more willing to provide training to motivated individuals to make a change.\n#2 remote work I think remote work is one of the best parts of living in post-pandemic 2022. I am an introvert, and having a quiet, stable space without distractions, which is the same from day to day, is a huge boost to my productivity.\nSecondly, I think it is crucial to be mindful of your posture and body while working at a desk. For example, I am dependent on having a standing desk which I adjust more than ten times a day. I also need a chair suitable for my body type to avoid getting stiff and getting a sore back. Although some offices take care of providing these facilities to their employees, I think it is beneficial to invest in your own setup, which you can tailor to your own needs.\nThirdly, working from anywhere in the world is a massive advantage. I am not very interested in living a digital nomad lifestyle, working from a MacBook in coffee shops, but I think it’s great that you can spend some time abroad while working from that location.\n#3 personal interest This is a big one. You should not change your career to IT just because it earns well or because you think you can work from the beach in Thailand. I have been tinkering with computers and programming languages since I was a kid and have always enjoyed it. I always found myself “the computer guy” in groups of friends or colleagues.\nHowever, for some reason, I never managed to make my career out of it until now, and I get a lot of satisfaction from my work every day after I made the change.\n#4 high income It is no secret that tech jobs are some of the best paying jobs in the U.S., having a mean wage of $99,860. And if you work your way up into management, there are even higher salaries. Here in the Netherlands, it is also a financially sound choice, with a mean wage of €47.200\n#5 fast-changing field IT is a broad field with many little niches you can get into, and every niche is constantly developing. Being in IT means you will need to stay on board by continuing to learn the new technologies and languages to keep on track.\nThis is also what makes it exciting to me, being a life-long learner. There is always more to learn and some cutting-edge technology to become familiar with, which can improve your workflow and your deployments.\nlinks: Computer and Information Technology Occupations – US Bureau of Labor Statistics\nMay 2021 National Occupational Employment and Wage Estimates\nSalaries in IT - Dutch\n","permalink":"https://mischavandenburg.com/5-reasons-why-i-changed-my-career-to-it-in-my-thirties/","summary":"In this post, I’d like to share the five main reasons why I changed my career to IT in my thirties. Making a career change can be daunting, especially when you are past your twenties, and employers can get more skeptical of hiring and training you. However, when I passed my twenties and became more serious and intentional about my life and career, I decided to take the plunge and hope for the best.","title":"5 Reasons Why I Changed my Career to IT in my Thirties"},{"content":"I have installed my OS on a 240GB SSD, and I prefer to keep my data on a different disk to leave enough space to work with. I wanted to move my steam game files to a separate hard drive on Linux. I’ll show you what I did to make this happen in this article. I use Manjaro GNOME.\nFirst, you need a correctly partitioned hard drive.\nTo wipe your drive clean and have a single partition on it, we’ll use GParted.\nSelect your disk in the upper right corner.\nThen go to Device and select Create Partition Table:\nFollow the wizard and use an ext4 filesystem. NTFS can cause problems because Steam cannot read it properly.\nmounting To use a disk or a partition in Linux, it needs to be mounted.\nList your devices and identify the one you wish to mount by using the “lsblk” command.\nIn my case, I wish to mount the drive sdc1\nOn Linux, all filesystems need to be mounted before they can be used. I wanted my whole disk to be available in the directory /mnt/data\nBefore mounting, I created the directory.\ncd /mnt\nsudo mkdir data\nWhen you make the directory by using sudo, the directory owner will be the root user. This means that you cannot access the directory and write to it from your own user.\nUse this command to change the ownership of the directory. Replace “mischa” with your username.\nsudo chmod mischa:mischa data\nVerify that the directory now has the correct ownership:\nNow you can mount your directory, so it is available for use.\nmount /dev/sdc1 /mnt/data\nmounting on boot For the mount to happen automatically on startup, you’ll need to add it to the /etc/fstab file. We start by finding the UUID of our disk.\nUse the following command:\nls -al /dev/disk/by-uuid/\nIn my case the UUID will be 50d608bc-a7ad-4ff6-bf44-bb6f26efa4f6\n/etc/fstab open the file in your favorite editor. I like to use vim.\nsudo vim /etc/fstab\nAdd a new entry to your /etc/fstab file and use the following parameters:\nUUID=50d608bc-a7ad-4ff6-bf44-bb6f26efa4f6 /mnt/data ext4 defaults 0 0\nBefore we go further, verify that we did this correctly by using the following command:\nfindmnt --verify\nThis will verify the /etc/fstab file. Not meaning to scare you, but an incorrectly configured fstab may lead to an unbootable system.\nNow reboot your system and check if your disk is mounted automatically.\nIt is also a good idea to cd to your mounted directory and touch a file to see if you have write permissions.\nSteam Now it’s time to set things up in Steam. Open Steam and open your settings.\ngo to Downloads –\u0026gt; Steam Library Folders\nClick the + button and navigate to your mounted drive.\nTo test, install a game and reboot your system.\n","permalink":"https://mischavandenburg.com/how-to-move-your-steam-game-files-to-a-separate-hard-drive-on-linux/","summary":"I have installed my OS on a 240GB SSD, and I prefer to keep my data on a different disk to leave enough space to work with. I wanted to move my steam game files to a separate hard drive on Linux. I’ll show you what I did to make this happen in this article. I use Manjaro GNOME.\nFirst, you need a correctly partitioned hard drive.\nTo wipe your drive clean and have a single partition on it, we’ll use GParted.","title":"How To Move Steam Game Files To a Separate Hard Drive on Linux"},{"content":"After getting my LPIC-1 certification, my interest in Linux continued to grow. When I started my new job, I performed more sysadmin tasks, such as increasing the size of filesystems or removing backups, and it felt good to put the theory into practice.\nI was still using Windows in my personal setup, and I started running into limitations. Finally, I realized how much I appreciated the freedom and control over my system that Linux gave me. So I decided I wanted to switch to Linux for my daily operating system. But a few things were holding me back. Will I be able to play my favorite games? Will I run into a lot of problems with my sound and microphone? Are all the apps I need for working from home available on Linux? Can I even edit videos on Linux?\npreparing to switch I made a dual boot install of Ubuntu to try things out to answer these questions. I started things off by setting small goals for myself. For example, I need to be able to work from home. Which programs do I need? And I started from there.\nI had no problems installing Slack, Zoom, Teams, and all the other programs I needed for my work. I was very surprised by how well all of the external hardware worked. My Bluetooth keyboard and mouse worked immediately without needing to install any drivers. Even my USB webcam worked instantly without any problems.\nTo put this into perspective, I spent an entire afternoon getting my keyboard to work correctly on my MacBook. It needed a lot of extra drivers. Still, there is a 4-second delay when I press my volume up/down keys when working on my MacBook. All of this works perfectly on my Linux OS without any delays and without any drivers or extra configuration.\nI kept my dual boot setup for a while as I was trying out different distros, and I eventually ended up with Manjaro as my distro of choice. I like it because it is based on Arch Linux, and it gives you access to the Arch User Repository (AUR). I want to use Arch eventually, and I figured this might be a good stepping stone.\nfate decided for me This dual boot setup continued for a while as I was warming up to the idea of completely abandoning Windows. I set Linux as my default boot option, and after a few weeks, I realized I hadn’t needed to boot into Windows for anything at all. However, I still didn’t feel quite ready to switch completely to Linux.\nOne evening I wanted to make another fresh install to check out the GNOME version of Manjaro. I was doing a lot of chores at the same time, and it was getting quite late, but I wanted to have the installer running as I was doing other tasks. Probably not my most brilliant move.\nYou probably know what is coming: in between my chores, I started the installation. In a moment of carelessness, I managed to point the installation to my Windows partition, and it was completely wiped and replaced with a sparkling fresh Linux install.\n“Well, I guess I am moving to Linux today!” I thought while I suppressed a hint of panic as I racked my brain to see if I had lost any important files. I knew that most of my important stuff was safely backed up in the cloud. But if I had formatted my Windows drive by choice, rather than by accident, I would have backed up a lot more files.\nfirst week without windows A week ago, I lost my complete Windows install, but there hasn’t been a single moment where I regretted making the switch. Fortunately, it also seems that I did not lose anything important.\nI am learning so much by forcing myself to use Linux as a daily driver. Most things are correctly configured out of the box. But sometimes, you have to do some work to get the configuration you like.\nFor instance, after installing Steam, I wanted to have the game files located on a different hard disk because my OS SSD is only 256GB. This required me to format my data SSD to an ext4 filesystem and mount it in a folder. I also needed to add it to my /etc/fstab file to make sure that it mounts automatically when I boot my PC.\nThese tasks have been great practice for the things I need to do on my servers at work, and they will make me approach these tasks with a little more confidence because I have done them before on my personal setup. This is the great advantage of having Linux as a daily driver if you are becoming a DevOps Engineer or Linux System Administrator.\n","permalink":"https://mischavandenburg.com/goodbye-windows-hello-linux-switching-to-linux-as-my-daily-driver/","summary":"After getting my LPIC-1 certification, my interest in Linux continued to grow. When I started my new job, I performed more sysadmin tasks, such as increasing the size of filesystems or removing backups, and it felt good to put the theory into practice.\nI was still using Windows in my personal setup, and I started running into limitations. Finally, I realized how much I appreciated the freedom and control over my system that Linux gave me.","title":"Goodbye Windows, Hello Linux! Switching to Linux as my Daily Driver"},{"content":"This week I learned about yadm: yet another dotfile manager. It is the perfect way to keep track of all your custom configuration files, known as dotfiles.\nEven if you have only a little bit of experience with Linux, you know that everything is managed in files. All configuration parameters are set or changed in text files stored on the hard disk. These files are usually located in your home directory and are hidden by default. This is indicated by prefixing the file with a period. So the configuration file for the vim editor is .vimrc, and for zshell you use the .zshrc. This is why configuration files are referred to as dotfiles.\ncustomisation The more I work with Linux, the more I appreciate the ability to customize things. When I first started, I was pretty satisfied with the vanilla experience. You punch your commands into the terminal, and you do your tasks. What more could you need?\nThis started to change when I began working with senior engineers who built their custom setups over the years. I saw them opening 3 terminal windows in a perfect ratio with beautiful colorschemes or previewing files in a file browser directly in vim so they could split them vertically and edit them side by side.\nI wanted to create a similar setup by adding settings and plugins to the .vimrc and .zshrc files. However, before going down this rabbit hole, I asked myself the following question. How can I bring this configuration with me to other machines? What happens if my laptop gets stolen and I lose my precious configuration files?\nyet another dotfile manager Enter yadm. I had thought of putting my dotfiles in a GitHub repo, but this brings up a whole set of new challenges where you would need to create symbolic links across your system to have the files in their correct places. Yadm solves this problem.\nYadm turns your home directory ( ~/ ) into a Git repo which can be pushed to a host of your choice. You can add your files one by one, and yadm will track them. The best thing is that you can add the files from all over your system, and yadm will not bother with any of the other files in your home directory.\nyou want git for your dotfiles Setting up your configuration files in a git repository has a lot of advantages:\nconfiguration is saved in multiple places easily share your configuration across machines version control Version control is especially useful. You will always be able to trace back that one plugin you used a few years ago, but you cannot remember the name of. And it is fun to watch your configuration grow over time.\nsetting up yadm Installing yadm is a breeze. For my mac I just used\nbrew install yadm or you can use the apt-get or dnf install equivalents if you are on Linux.\nYou interact with yadm the same way you interact with git. You simply replace the word git with yadm in the commands.\nThen you navigate to your home directory and set up the repository. If you don’t have a repository yet:\nyadm init yadm add \u0026lt;important file\u0026gt; yadm commit yadm remote add origin \u0026lt;url\u0026gt; yadm push -u origin \u0026lt;local branch\u0026gt;:\u0026lt;remote branch\u0026gt; Or if you already have a dotfiles repository:\nyadm clone \u0026lt;url\u0026gt; yadm status And that’s it. Now add your configuration files and push them to your hosted repo:\nyadm add ~/.vimrc yadm add ~/.zshrc yadm commit -m \u0026#34;first commit\u0026#34; yadm push You will notice that yadm expects you to add all the files every time you want to make a new commit. Use this command to stage all the files you added previously:\nyadm add -u enjoy your synched customisation Having your dotfiles in a GitHub repo makes it easy to set up your preferred settings on a new machine or environment. So install yadm and pull your repo, and off you go!\nI hope you will enjoy it as much as I do. Crafting a customized setup takes a lot of time and effort, and now that I finally have an excellent solution to keep track of my files, I am ready to dive into the customization rabbit hole.\nDownload yadm here. Here you will also find all the necessary information to install and configure your yadm.\n","permalink":"https://mischavandenburg.com/yadm-keep-track-of-your-precious-dotfiles/","summary":"This week I learned about yadm: yet another dotfile manager. It is the perfect way to keep track of all your custom configuration files, known as dotfiles.\nEven if you have only a little bit of experience with Linux, you know that everything is managed in files. All configuration parameters are set or changed in text files stored on the hard disk. These files are usually located in your home directory and are hidden by default.","title":"Yadm: Keep Track of Your Precious Dotfiles"},{"content":"I recently obtained my LPIC-1 certification, and in this blog post, I’ll share the strategy and techniques I used to pass this exam and share my thoughts on the certification. Because I am a Linux novice, the exam was a pretty tough grind for me. This article offers a beginner’s perspective on the LPIC-1 certification. Is the LPIC-1 hard to pass? Keep reading to find out.\nBefore this certification, I had only a little bit of experience. I deployed LAMP stacks using Ansible and configured VMs to be able to communicate with each other using only the command line. I also did a “Linux Fundamentals” video training. I could navigate the filesystem, edit text files and work in the terminal, but that was about it.\nIs it hard? For a beginner: yes, it was hard! But if you are a Linux administrator with a few years of experience, these exams probably are not very difficult to pass. However, even if you are experienced, be prepared to do a lot of memorization. Even though the requirements on the website seem very basic and straightforward, when you dig into the study resources, you will soon discover that you need to learn a large host of commands and many of their accompanying parameters. For example, you will need to know what grep -H does precisely, the difference between passwd -l, chage -l, and chage -L, the location of the directory that contains all the timezones, and the directory that contains the printer configurations for CUPS.\nThe certification The LPIC-1 certificate requires candidates to pass the LPI 101-500 and 102-500 exams. These exams test the candidates on various subjects, such as file management, boot loaders, networking fundamentals, user and group management, file systems and partitioning, and much more.\nEach exam has a 600-page syllabus, so to get your LPIC-1 certification, you need to work through 1200 pages and memorize a few hundred commands and parameters. However, if you work as a Linux sysadmin, you’ll probably know many of these commands and concepts.\nStudy Materials I attended a 4-day course that covered both exams. However, because of the large amount of information that needs to be covered, the teacher could only address the subjects on a superficial level. Therefore, I would advise you to be suspicious of any courses that promise to prep you for the exams in 4 days if you are a beginner. I estimate that you need at least double that amount to get some proper explanation of the material.\nLPI Syllabus After finishing the course, it became clear that I needed a lot of studying to pass the exams. Fortunately, LPI has created a syllabus for each exam. These are available for free on the LPI.org website.\nLPI Practice Exams It is crucial to test your knowledge. This is the resource I used:\nLPIC-1 Linux Professional Institute Certification Practice Tests: Exam 101-500 and Exam 102-500 If you are a member of O’Reilly’s, you can read the book there. It contains around 90 practice questions for every chapter in the LPI syllabus. The questions test your knowledge in detail and are a great way to determine whether you have fully grasped the material.\nHowever, the book was written in 2019 and contains questions about certain subjects that have since been removed from the exams. So if you suddenly encounter questions that do not seem familiar at all, make sure to double-check that it is actually an exam objective.\nLastly, I used these practice exams on Udemy.\nMemorization As I have stated before, the exams require you to do a lot of memorization. Fortunately, we have some tools and techniques available to help us with this task.\nThe primary tool is Anki. If you are not familiar with it, Anki is a very simple and free program that allows you to create flashcards that you can use to study and test your knowledge. The best thing about Anki is that it implements spaced repetition. You can download and learn more about Anki here: https://apps.ankiweb.net\nSecondly, I am fond of memory techniques. You can remember things much more quickly by visualizing them in your mind or utilizing techniques such as Memory Palaces or the Method of Loci. If you are interested in learning more about memory techniques, I highly recommend Dr. Anthony Metivier’s YouTube channel.\nStrategy Here is the strategy that I used to pass the exams:\nRead through a chapter and take notes. Make Anki flashcards for all the commands and flags that you do not know yet Do the exercises at the end of the chapter Do the practice exam for your chapter from the exam book, which should give you a good indication of how well you have grasped the material. Make flashcards of all the questions that you answered wrong (trust me, there will be quite a few) Use Anki to test yourself and memorize all of the commands and exam questions Tips: Do your Anki reviews every day. On some days I was adding more than 100 new cards, which will lead to a lot of reviews in the coming days Although the syllabus for exam 101 explained things very well, the 102 syllabus sometimes is very meager in its explanations and you might need to supplement with reading man pages, YouTube videos, and other tutorials. For example, I needed to find quite a bit of supplementary material for chapter 109 Networking Fundamentals. Ask for help if you don’t understand a certain topic Don’t think you can get away with skipping a topic. You will be tested on absolutely everything that’s in the syllabus, trust me. Try doing it together with someone else. I was doing it together with my friend and colleague, and it was extremely useful to be able to share things I struggled with and to discuss things with him to understand them better. Thank you for the good times, Gino! My thoughts on the certification The subject matter is extensive, and I know my way around Linux much better now. Therefore, if I encounter a problem, I am better positioned to assess where the cause might be and then solve the problem from there. I also feel I have a much better grasp of basic networking concepts, which will prove to be very useful in many situations in my work as a DevOps Engineer.\nHowever, there are also a few drawbacks to this certification. I think there is too much emphasis on memorizing commands and their flags. I think it is not necessary to memorize all of the possible parameters of the chage command because, in the real world, I would take a quick look at the man page to find the parameter that I need. The exams force you to memorize many parameters in a short time, and to be honest, you will probably forget about them very quickly anyway.\nBut overall, I am pleased and grateful to my employer that I was able to obtain this certification, and it has made me hungry for more, and I am very eager to continue my learning in this domain.\n","permalink":"https://mischavandenburg.com/lpic-1-study-guide/","summary":"I recently obtained my LPIC-1 certification, and in this blog post, I’ll share the strategy and techniques I used to pass this exam and share my thoughts on the certification. Because I am a Linux novice, the exam was a pretty tough grind for me. This article offers a beginner’s perspective on the LPIC-1 certification. Is the LPIC-1 hard to pass? Keep reading to find out.\nBefore this certification, I had only a little bit of experience.","title":"LPIC-1 Study Guide"},{"content":"Two months ago I knew nothing about GitHub. This week my first pull request got merged into master!\nProgramming tutorials and books very often suggest that you should try to contribute to open source in order to practice your skills. Even though I am still on the beginner level in Python, I managed to find something I could contribute with. But there were a few things I needed to learn in order to be able to do so.\nGitHub is a place where many open source projects are hosted. Projects are hosted in “repositories” available to the public. Everyone can go in and take a look at the code. And the great thing about it is that everyone can contribute to the code as well.\nTwo months ago I knew almost nothing about GitHub. Surely, I had often downloaded software from GitHub, and I knew it had to do with version control. But I had no idea that it was such a powerful system of enabling collaboration for software projects.\nDuring an assignment in my DevOps Traineeship I spent some time learning about Github and the Git language. I learned about repositories, branches, commits and pull requests. Now I wanted to take it to the next level and make a contribution of my own somewhere.\nthe project As I have mentioned in other posts, I love game automation, and recently I discovered the Botty project, which is a bot written for the game Diablo 2: Resurrected. The bot is written in Python, which means that it is a great way of applying my Python learning to something I am passionate about.\nThe bot uses computer vision in order to recognise what is on the screen and run scripts accordingly. The monsters in the game drop items, and if you want the bot to pick up items, it will need to be taught which items it needs to pick up.\nThis is done by adding some images to its image database and adding the filenames to a list of items. When the bot scans the screen for items, it will look for a match in its image database, and when it matches, it will click the corresponding pixels on the screen to pick up the item.\nHere’s what an image in the database looks like:\nI am doing a Holy Grail project in this game, which means that I am collecting every item in the game. It is quite an undertaking as there are 506 items in the game, and some items have a drop chance of 1 : 1.000.000. Luckily I have a bot to help me with this project.\nNot surprisingly, many items were still missing from the bot because it is a fairly new project that is still in development. And as I needed my bot to pick up the items I needed, I decided to add these 46 missing items to the database.\nforks, commits and pull requests After doing the work I still needed to figure out how I should offer these items to the project. Luckily someone shared a few very helpful tutorials in the project’s discord. This is the tutorial I used for my first contribution:\nStep-by-step Guide to Contributing on Github\nYou begin with “forking” the project repository, which basically means making your own copy of all the code in the project. Then you add your contribution to the fork by cloning it to your local machine and making your changes to a new branch.\nWhen you have committed your changes and pushed your new branch to your fork, you are ready to make your first pull request. A pull request is a way of telling the project that you have something to contribute. You are sharing your version of the project repository including your proposed changes, and someone from the project will take a look at your suggestions and see if they are useful and compatible.\nmerged into master After a few days someone had a look at my contribution and requested me to make a few small adjustments. When I managed to incorporate those my contribution was accepted, and my changes were “merged into master”, which means that my contribution was brought into the main version of the project’s code.\nI learned a lot! As I am typing out this article, I am very satisfied with how much I have learned in the past few months already. I remember being very confused about all the GitHub terminology when I attended my first meetings during my DevOps traineeship.\nGoing through the process of making a contribution to open source on GitHub has been an enriching experience. It seemed quite intimidating at the start, but by following a good tutorial I managed to successfully submit my first pull request. I feel I have a much better understanding of Git, GitHub and the workflow.\nAnother valuable lesson I learned is that you don’t need to be a Senior Engineer in order to be able to contribute to open source. Although this project is written in Python, my contribution had very little to do with code, but I provided assets which were required by the code. So if you are a beginner at programming, you can look for other ways to contribute, such as fixing spelling mistakes in the documentation, providing images or writing wiki pages.\n","permalink":"https://mischavandenburg.com/my-first-contribution-to-open-source/","summary":"Two months ago I knew nothing about GitHub. This week my first pull request got merged into master!\nProgramming tutorials and books very often suggest that you should try to contribute to open source in order to practice your skills. Even though I am still on the beginner level in Python, I managed to find something I could contribute with. But there were a few things I needed to learn in order to be able to do so.","title":"My First Contribution to Open Source"},{"content":"The best part of learning Python is trying to identify things in my life which I can automate by writing a script. Learning a programming language involves doing a lot of exercises that sometimes lack a connection with the real world. But after I decided to go for it, I am always on the lookout for projects. Not only for my job as a DevOps Engineer, but also for my private life. In this case, I needed to write a program that parses log files from a bot so I could get a total number of runs. You can have a look at the final result in my Diablo 2 GitHub repo. Like I wrote in my journey into DevOps article, I love automating games. Diablo 2 is a game that was originally released in 2002 and which recently was remastered. Diablo 2 always had a very strong presence of bots in the online game, and it didn’t take long before I also joined the ride.\nA few months after the remaster the first bots have started emerging as well. There is a a particularly good one written in Python which is an open source project, which is a perfect opportunity for me to learn more about Python by trying to understand its code and solving problems. I was very excited to discover it because I was playing quite a few hours a week. Diablo 2 is a very grindy game and it takes a lot of time to find the needed gear. Now I could finally outsource my grinding to the computer again.\nthe problem Although the bot is very functional and does several tasks very well, there are still features missing because it is relatively early in its development. One of these features is keeping track of the total amount of runs that the bot has done. In Diablo, every time you play you start a “game” or an instance. In that game there are certain bosses you can kill, and when you are finished you exit your game. This is called a run. Then you create a new game and everything is reset, and you get another shot at killing the bosses to get the precious gear.\nBeing the nerd that I am, I like to keep track of the total amount of runs that the bot has done. On these numbers I like to apply some calculations to see how many items I get per xxxx runs and suchlike. The bot keeps track of the amount of runs it does per session and stores them in a log file. But there is no functionality of seeing the total amount of runs you have done, and when I discovered this, I realised I had my first little Python project.\nlog files Every time you close the bot after a session, a log file is created that looks like this:\nIt is formatted as a .txt file and shows information about the bosses that were run and the items that were found. Most importantly, it contains the amount of games that were done in the session. Even after only using the bot for a short time I had over 100 log files to go through, and that’s where I needed a script that would go through these files for me and add the numbers in order to get the total amount of runs.\nthe script After completing chapter 9 and 10 in the Automate the Boring Stuff book, I learned about file paths and opening files and reading from them. Now it was time to apply that knowledge. The process went pretty well and soon I had a script that would open the files for me.\nThings got a little bit more complicated when I needed to read information from the files. And of course, this operation would almost certainly involve the dreaded topic of regex. In the end it was not as bad, and I ended up with the following regex:\nregex_games = re.compile(r\u0026#34;Games:\u0026#34;) regex_dict = { \u0026#39;nihla\u0026#39;: \u0026#39;Nihl|Nihlatak\u0026#39;, \u0026#39;pindle\u0026#39;: \u0026#39;Pin|Pindle\u0026#39;, \u0026#39;eldritch\u0026#39;: \u0026#39;Eld\u0026#39; } As you will see afterwards, I needed a way to check every line for a certain statement. However, rather than hardcoding every operation, I wanted it to loop over a list of terms. This meant that I could easily go back to the code and add a few more search statements if I needed them. I ended up storing them in a dictionary as you can see above. I really like the way you can make dictionaries in Python and have every entry on a new line. It makes the code very readable and structured.\nAnd this is the actual looping sequence that I ended up with:\nfor folder_name, sub_folder, file_names in os.walk(source): for filename in file_names: p = PurePath(folder_name, filename) with open(p, \u0026#39;rt\u0026#39;) as my_file: # search for games number line for line in my_file: # find number of games and add to total games if regex_games.search(line): g = line.split() total_games += int(g[1]) f = my_file.read() # check which runs were done by using the regex dict for key in regex_dict: location = regex_dict[key] reg = re.compile(location) # if there is a match, add the numbers to the total variable if reg.search(f): var_name = key + \u0026#39;_total\u0026#39; globals()[var_name] += int(g[1]) This sequence loops through the folder, the subfolder, and opens each file one by one. When the file is opened it looks for the “Games: 25” line and adds the number to a variable. However, I was not only interested in the total number of games. I also wanted to get more insight in how many Pindle runs or Nihla runs I had done. So I set up another regex search and made sure that the number of games are added to a “pindle_total” or “nihla_total” variable.\nresult When running the script in the shell, the result looks like this:\nmischa@MischaMacBook stats_parser % python3 total_runs.py Total runs: 7159 Pindle runs: 6926 Eldritch + Shenk runs: 367 Nihla runs: 232 mischa@MischaMacBook stats_parser % Exactly what I wanted. Now I can just paste my stats files into a folder and see how many runs I’ve done. Maybe I’ll improve it by building a GUI. Another fun idea I have is to create a little pipeline where this script would be run once an hour and the stats would be uploaded to a webpage somewhere, so others could see the amount of runs of my bot. Not that anyone is interested in that, but it is a fun project for me to do. Let’s see what happens!\nFor now I am very happy with the result. It was a very satisfying experience to identify a problem that I had and to be able to come up with an automated solution. Of course it is still very rudimentary programming, and there is a long long way ahead of me, but it was fun to finally do something practical that solved a particular problem in my life.\nThe final result is in my Diablo 2 GitHub repo.\n","permalink":"https://mischavandenburg.com/my-first-useful-python-script/","summary":"The best part of learning Python is trying to identify things in my life which I can automate by writing a script. Learning a programming language involves doing a lot of exercises that sometimes lack a connection with the real world. But after I decided to go for it, I am always on the lookout for projects. Not only for my job as a DevOps Engineer, but also for my private life.","title":"My Mirst Useful Python Script"},{"content":"I am currently working through the book Automate the Boring Stuff by Al Sweigart . I can already highly recommend it to anybody who is learning Python.\nChapter 9 is about reading and writing files, and there are two assignments at the end of the chapter. Here I’ll discuss my solution of the Mad Libs assignment.\nhere is the full assignment text: Mad Libs Create a Mad Libs program that reads in text files and lets the user add their own text anywhere the word ADJECTIVE, NOUN, ADVERB, or VERB appears in the text file. For example, a text file may look like this: The ADJECTIVE panda walked to the NOUN and then VERB. A nearby NOUN was unaffected by these events. The program would find these occurrences and prompt the user to replace them. Enter an adjective: silly Enter a noun: chandelier Enter a verb: screamed Enter a noun: pickup truck The following text file would then be created: The silly panda walked to the chandelier and then screamed. A nearby pickup truck was unaffected by these events. The results should be printed to the screen and saved to a new text file. Looks pretty simple, right? I went into it with a lot of zeal and started writing a long list of if statements. My first attempts at the solution involved matching the words NOUN and ADJECTIVE directly, like so:\nif word == \u0026#39;ADJECTIVE\u0026#39;: inv = input(\u0026#39;Enter an adjective: \u0026#39;) However, this is problematic because as you can see, the sentence can contain words with a period attached, such as “VERB.” in the above example.\nno, please no regex! I’ve understood that here is a general anxiety around regex. I have certainly noticed it in myself and some of my junior engineer friends. As soon as I read the word regex, or realise that an assignment is going to involve regex, I get a constricting feeling in my throat and a rise in my heart rate.\nI’ve had to struggle with it quite a bit during my freeCodeCamp Front End Development Certificate, and the memories are still fresh in my mind.\nSo, like any other ‘rational’ human being, I tried to approach this relatively simple assignment with all sorts of ways trying to account for a period ending the word:\ndef period_check(x): for letter in x: if letter == \u0026#34;.\u0026#34;: return True # loop over the array and prompt user for word in source_text: period = False if period_check(word): word = word.replace(\u0026#34;.\u0026#34;, \u0026#34;\u0026#34;) period = True if word == \u0026#39;ADJECTIVE\u0026#39;: invoer = input(\u0026#39;Enter an adjective: \u0026#39;) if period: result_text.append(invoer + \u0026#34;.\u0026#34;) period = True else: result_text.append(invoer) elif word == \u0026#39;NOUN\u0026#39;: It’s quite funny to see the lengths we go through to avoid regular expressions. However, as you maybe deduce from the code above, it didn’t work, and after a couple of hours of fiddling I gave up, and like any other frustrated programmer, I started to google.\nI quickly found solutions to this assignment and they all involved regex, and I realised I could not walk away from my fears anymore.\nan elegant solution Eventually I ended up with the following result for the part of my assignment that needed to recognise and replace the words with the user input. Of course I heavily borrowed from my Google search results.\n# set up and match the regex grammar_regex = re.compile(r\u0026#39;ADJECTIVE|NOUN|VERB|ADVERB\u0026#39;) match_regex = grammar_regex.findall(source_text) # replace the matches with user input for i in match_regex: ask_user = input(\u0026#39;Please enter \u0026#39; + i + \u0026#39;: \u0026#39;) source_text = source_text.replace(i, ask_user, 1) When I say elegant, I mean elegant in total Python beginner terms. I’m sure there are enough Seniors out there who will burst out laughing when they see this. But to me, it was quite an eye-opening experience to see this little piece of code do exactly what I had intended to achieve with 3 different functions and long blocks of if statements.\nAlso, I was pleasantly surprised with how simple regex can be in Python. In this case there were no scary [Az ^**/!!${}aa{}aA{nF}] statements. We simply defined which words we wanted and called the findall() module to generate a list with all the matches.\nThen we iterate over the list of matches and for each match we ask the user for the desired word, and replace it in the source_text.\nfinal result Having sorted out the pattern matching and replacing part, it was only a matter of implementing reading from files and writing to a new file.\n# Automate the Boring Stuff chapter 9 # Mad Libs assignment # Mischa van den Burg from pathlib import Path import re # ask the user which file to open file_name = input(\u0026#39;Enter the filename. For example, grammar.txt: \u0026#39;) # my script and .txt file are located in ~/python/automatetheboringstuff/ text_file = open(Path.home() / \u0026#39;python\u0026#39; / \u0026#39;automatetheboringstuff\u0026#39; / file_name ) # read the file and store in variable \u0026amp; close source_text = text_file.read() text_file.close() # set up and match the regex grammar_regex = re.compile(r\u0026#39;ADJECTIVE|NOUN|VERB|ADVERB\u0026#39;) match_regex = grammar_regex.findall(source_text) # replace the matches with user input for i in match_regex: ask_user = input(\u0026#39;Enter \u0026#39; + i + \u0026#39;: \u0026#39;) source_text = source_text.replace(i, ask_user, 1) # write to the new file and print the result new_file = open(\u0026#39;new_\u0026#39; + file_name, \u0026#39;w\u0026#39;) new_file.write(source_text) new_file.close() print(source_text) I was getting into some better functionality, such as accounting for existing filenames, and making the pathing relative so it could be run from anywhere. But I decided to save that for a later assignment.\nThe assignment was clear and did not require such functionality. I need to learn to keep things simple, and I decided to do just what I was asked and not go into any other rabbit holes.\nLessons Learned All in all the assignment is pretty simple, but I learned surprisingly much from it. I decided I’ll need to change and learn to love regex rather than fear it, because it showed me how powerful it can be.\nAlso, I got some insight into my own mind and how I tend to work. I realised I have a tendency to make things much more complicated than they need to be. I need to learn to keep things simple.\n","permalink":"https://mischavandenburg.com/python-project-mad-libs/","summary":"I am currently working through the book Automate the Boring Stuff by Al Sweigart . I can already highly recommend it to anybody who is learning Python.\nChapter 9 is about reading and writing files, and there are two assignments at the end of the chapter. Here I’ll discuss my solution of the Mad Libs assignment.\nhere is the full assignment text: Mad Libs Create a Mad Libs program that reads in text files and lets the user add their own text anywhere the word ADJECTIVE, NOUN, ADVERB, or VERB appears in the text file.","title":"Python Project: Mad Libs"},{"content":"In order to learn more about Docker and Ansible I am working on an assignment to take an existing application and to break it down into containers. However, in order to be able to understand this process properly, I first needed to understand more about Docker and containerisation.\nI wrote a playbook that installs Docker and deploys a fully containerised LEMP stack on a virtual machine.\nYou can have a look at the Github repo with the result here. The repo is using the “ansible-galaxy init” role structure. You will find the playbooks as follows: roles/your_choice/tasks/main.yml\nDocker I was very excited to learn more about Docker and containerisation. I was familiar with the concept of virtualisation, which is creating virtual versions of fully functional machines on a host operating system. But the concept of containerisation was new to me.\nAs I understand it, containerisation differs drastically from virtualisation because containers are able to use resources from host directly. They do not need an entire operating system to run, and therefore they are a much more lightweight.\nThis means that resources can be used much more efficiently which eventually can mean cost reduction in your cloud infrastructure.\nDocker is a very popular platform for building and running containers. It seemed like the best option to get started with deploying my own containers.\nLEMP Stack My colleague recommended me this tutorial to become more familiar with Docker. It uses a LEMP stack as an example application. When I told friends about the fact that I was building a LEMP stack, they corrected me and said it was a LAMP stack.\nThe LAMP stack is a collection of software built out of these elements:\nL – Linux: the operating system\nA – Apace: webserver\nM – MySQL: database\nP – PHP: server scripting language\nHowever, in a LEMP stack, we use NGINX as a webserver, which is pronounced “Engine X”, hence the E in LEMP stack. Therefore, LEMP is the correct way to spell it, and it is used in all the tutorials that I have been using.\nI highly recommend the tutorial in order to learn how to deploy your first collection of containers. Deploying one container is relatively easy with Docker, but it gets a little more complicated when deploying several containers and making them communicate with each other in order to combine them into one application. But this tutorial does a great job at showing you how it’s done and it is especially good at explaining the steps along the way.\nDocker Compose vs. Ansible Docker Compose is a tool you can use to run multi-container applications. With the help of the tutorial, it was fairly easy to understand and hit the ground running by deploying multiple containers into one network.\nNow let’s have a look at how we actually set up the containers. In the Docker Compose file, the NGINX container was defined like this:\nversion: \u0026#39;3.8\u0026#39; #Services services: #Nginx Service nginx: image: nginx:1.19 ports: - 80:80 volumes: - ./src:/var/www/php - ./.docker/nginx/conf.d:/etc/nginx/conf.d depends_on: - php It looks pretty straightforward, right? Almost like pseudocode. We tell Docker which image to pull from the Docker Hub, and we tell it to route container’s port 80 to our host’s port 80. This ensures that the web server can be accessed from the outside, provided you have opened this port in the firewall.\nNext there is the volumes section: this mounts certain directories on the host into the container so it is accessible. In this case this was necessary to transfer the web server configuration and the index.php which we wanted to serve to the outside.\nHaving successfully deployed my LEMP using Docker Compose, the next step was to automate this process by using Ansible. Ansible is a very powerful tool which enables you to automate configuration management and application deployment by writing scripts called playbooks.\nWhy was it necessary to introduce Ansible? By using Docker Compose, you would need to have Docker and Docker Compose installed on the virtual machine before you could start running the containers.\nHowever, Ansible gives you the power to take a completely fresh virtual machine, configure it from scratch, and install Docker and its necessary dependencies, followed by deploying the containers.\nNow let’s take a look at the same container defined in Ansible: - name: start nginx docker_container: name: nginx image: nginx:1.19 detach: yes ports: - 80:80 networks: - name: network_one volumes: - /src:/var/www/php - /.docker/nginx/conf.d:/etc/nginx/conf.d Although there are some differences, they look very similar. Converting my Docker Compose file to an Ansible playbook was quite a natural and easy experience. It also helps that both are written in YAML and therefore use the same indentation conventions.\nA few differences we can observe:\nIn the Ansible playbook we invoke the docker_container module, whereas they are defined as services in the Docker Compose file. Another difference is that we need to set up the network ourselves. In the Docker Compose file, we just specified the containers and Docker Compose created a network automatically and made sure that all containers were connected to it.\nHowever, it isn’t very complicated in Ansible either:\n- name: setup network docker_network: name: network_one We simply call the docker_network module and tell it to make a network called network_one. All we need to do then is make sure to set the networks: parameter to network_one in the docker_container module as we saw above.\nThe last point to note is the detach parameter. This means that the container will keep running in the background after it is started.\nResult After some debugging here and there and making sure all of the elements were in place, eventually we get the satisfying message that everything went according to plan:\nThe result is a webpage being served on the server ip:\nI know, it is not the prettiest or most intricate design. But remember that I am working towards becoming a DevOps Engineer, not a Front End Developer 😉\nWe can also enter the phpMyAdmin dashboard by adding port 8080 to our ip in the browser:\nConclusion The assignment of deploying a LEMP stack in separate containers has been very useful and I learned a lot from the process. There were a few more modules that needed to be configured in Ansible as opposed to the Docker Compose method, but the tradeoff is that Ansible is much more powerful and enables you to configure the server from scratch. You can have a look at the code in the GitHub repo to see all of the changes I needed to do.\nThe only part that I needed to do by hand is to create the VM in the Microsoft Azure portal, open the ports and configure the SSH keys. The next step in my learning process will be to learn how I can automate this step as well. This means that I will need to learn Terraform.\nBy using Terraform I will be truly deploying this stack as Infrastructure as Code, but doing all of these steps with Ansible has given me a much better understanding of Infrastructure as Code already.\n","permalink":"https://mischavandenburg.com/docker-lemp-stack-deployed-with-ansible/","summary":"In order to learn more about Docker and Ansible I am working on an assignment to take an existing application and to break it down into containers. However, in order to be able to understand this process properly, I first needed to understand more about Docker and containerisation.\nI wrote a playbook that installs Docker and deploys a fully containerised LEMP stack on a virtual machine.\nYou can have a look at the Github repo with the result here.","title":"Docker LEMP Stack deployed with Ansible"},{"content":"I always struggled with finding an online nickname for myself. I was never given one, and over the years I used a few here and there, ranging from Nordic gods to my favourite fantasy character: Gandalf the Grey.\nI intended to create a tech blog to document my journey ever since I started my career change. But for months I was stopped by trying to decide on a name. (That’s me in a nutshell), because it needed to be perfect. Eventually I decided to just use my full name.\nAlthough I have zero connections with Russia, neither genetically nor culturally, my parents decided to give me a name of Russian origin: Mischa.\nAccording to this website, Mischa has the following meaning:\n“The name Mischa is primarily a gender-neutral name of Russian origin that means Who Is Like God“\nAlthough I admire my parents for giving me such an ambitious name, I must confess that I turned out to be of a much more earthy and less godly nature.\nWhen I was younger, an old gymnastics teacher once told me that Mischa meant “bear” in Russian, and I always liked that connotation much more. I also turned out to be more bear-like than god-like, being 190cm tall and having thick and bristly curly hair.\nIndeed, this post on Quora confirms that my name indeed has the meaning of bear:\n\u0026ldquo;In Russia Mishka sometimes also used to denote a bear, particularly a bear cub.\u0026rdquo; Misha Sivan, Born in USSR.\nAlthough I am conveniently ignoring the “cub” part of his explanation, I thought it was pretty cool that my actual name had connotations with our ursine friends.\nMoreover, I lived 9 years in Norway where I roamed the mountains for weeks at a time, just me and my tent and a fishing rod. I developed a very close connection with nature during those years. Not only its beauty, but also its merciless forces and awe-inspiring ingenuity.\nAlthough I never encountered a bear myself, they are most definitely present in the Norwegian nature. One time the newspapers told me that there was a bear sighted 4 kilometers from where I was camping one weekend.\nBut this is what the bear represents to me: the time I was allowed to spend in the North and the resulting connection with nature. The solitary character of the male, calmly roaming for many miles in search of sustenance. A force to be reckoned with when aggravated, but in general preferring to stay at a distance.\n","permalink":"https://mischavandenburg.com/why-i-chose-a-bear-for-my-logo/","summary":"I always struggled with finding an online nickname for myself. I was never given one, and over the years I used a few here and there, ranging from Nordic gods to my favourite fantasy character: Gandalf the Grey.\nI intended to create a tech blog to document my journey ever since I started my career change. But for months I was stopped by trying to decide on a name. (That’s me in a nutshell), because it needed to be perfect.","title":"Why I Chose a Bear as a Logo"},{"content":"In 2021 I had reached a certain stage in my life where I had the liberty to make a choice. I was 31 years old and had just finished an important chapter of my life, and was ready to begin a new one. After thinking about where I wanted to steer my professional career, I decided to follow my passion for programming and automation and attempt to make a career out of my interests in tech and IT.\nI was in a fortunate situation, because there were large shortages of people on the IT job market in The Netherlands. Consequently, employers were much more willing to train their employees to perform the roles that they required to fulfill.\nBots and Scripts I have always loved messing around with computers, and I learned to write small and simple programs at a very young age. At the same time I was an avid gamer, and spent many hours slaying monsters in online RPG’s. One day I came across the concept of a bot: a program that plays the computer game for you, and I was hooked. This is where I developed my ‘fetish’ for automation.\nI was very lucky that I had a friend who shared my interests, and together we built our own automation projects (called ‘botfarms’) in which we ran large amounts of bots that played a certain game for us. This army of bots generated in-game currency which we could sell for actual money. They weren’t huge profits, but it was an amazingly satisfying feeling to be the overlords of an army of automations that actually generated some income for us.\nFrom Bots to DevOps These botfarms were hosted on servers which set up ourselves. In order to save costs we rented Linux servers, and I spent many evenings figuring out how configure them via the command line. Often I would suddenly snap out of my flow at 3am and realise I had to go to work at 7 in the morning again.\nAlthough I did not manage to make these personal interests into a personal career, my friend eventually became a Data Engineer. After making the decision to make a career switch to IT I needed to figure out which direction I wanted to go in, because IT covers a very broad range of topics and skills. Based on on our shared interests and previous projects, he recommended me to become a DevOps Engineer.\nI very quickly realised that he was right on the money with his suggestion and I started to become very enthusiastic to learn how to make a living by working with computers and automation.\nTraineeship As I mentioned before, employers in The Netherlands are now willing to train potential candidates, and I used my hobby projects as a way to demonstrate my genuine interest and affinity with IT and automation. I was offered a traineeship to become a DevOps Engineer in 2021. After a period of training I started working for the City of Amsterdam and I\u0026rsquo;ve been part of an IT4IT operations team since.\n","permalink":"https://mischavandenburg.com/my-journey-into-devops-so-far/","summary":"In 2021 I had reached a certain stage in my life where I had the liberty to make a choice. I was 31 years old and had just finished an important chapter of my life, and was ready to begin a new one. After thinking about where I wanted to steer my professional career, I decided to follow my passion for programming and automation and attempt to make a career out of my interests in tech and IT.","title":"My Journey Into DevOps So Far"},{"content":"Favorites These articles came out particularly well, and might provide some value to you, or help you get to know me a little better.\nI\u0026rsquo;m In Love with my Work: Lessons from a Japanese Sushi Master My Neovim Zettelkasten: How I Take Notes in Markdown Using Vim and Bash How and Why I Started Using Vim What is a Zettelkasten? Zettelkasten is a system of note taking and personal knowledge management. Zettelkasten means \u0026ldquo;slip box\u0026rdquo; in German. Traditionally it is a physical box of slips of paper or index cards containing smaller notes. Each of these cards have a unique identifier, and these identifiers are used to create links between the cards.\nIt was used by Niklas Luhmann who was one of the most prolific scholars in history. He wrote 70 books and nearly 400 academic articles, and he credited the Zettelkasten with making his productivity possible. It contained around 90,000 index cards.\nI first came across this concept by reading the book How to take Smart Notes by Sönke Ahrens.\nHow to Navigate this Website To see an index of my entire Zettelkasten sorted in chronological order, visit Index (Zet). Longer articles can be viewed at the articles page.\nIf you want to see an overview of topics and notes centered around certain topics, use the Tags page. My Zettelkasten can also be searched.\nAbout This Website What you are seeing here is the public section of my personal Zettelkasten. I keep a large collection of notes which are stored as markdown files on my harddisk. Whenever I\u0026rsquo;m studying a topic by reading or watching videos I keep notes about what I learn and store them in my Zettelkasten.\nWhen I want to reflect on a certain topic or to write an article, I go through my collection of notes and link them together. Very often I find that the creation of these links will stimulate even more new connections and associations, and I end up with new ideas for even more topics of study or things that I want to write about. I use my Zettelkasten as a vehicle for reflection, learning and creativity.\nI decided to make part of my notes public so I can share my learning journey in the exciting field of DevOps, cloud computing and coding. And I recently started publishing my notes on other topics such as health and execrice as well.\nAbout me I’m Mischa. I’ve worked across various fields ranging from project management in the oil industry to health care, but my driving passion has always been IT. Fascinated since childhood, I’ve always passionately worked on projects that leveraged computers and automation to reach my goals.\nIn 2007 I started learning to code to automate the video games I used to play and to build my own websites. In 2014 I taught myself to use Linux and utilized these skills to run large-scale video game and social media automation projects to drive traffic to the websites I coded at night.\nI lived in Norway for 9 years, and when I moved back home to the Netherlands, I succeeded in following my dream and turning my passion into my career, and I’ve been working as a full-time DevOps Engineer since 2021.\nI\u0026rsquo;m in love with my work. After I finish my day\u0026rsquo;s work, I usually sit down to study more about my field. There is always something new to learn. Learning can take many forms. Very often I read or watch videos and courses and take notes. But you cannot learn everything from the books alone. You also need to learn by doing. I work on coding projects or building my Kubernetes home lab. I created a lab repo where I store all the files I use to explore these topics.\nI\u0026rsquo;m curently focused on learning Go, and I store all of my learning projects in my go repo\nLiving On The Command Line I\u0026rsquo;m a nerd. I use neovim for all of my text editing. I have a fully customized setup with dozens of custom keybindings so I can work a little more effectively, and this blog is entirely written on the command line using vim.\nI love UNIX based operating systems and I\u0026rsquo;ve built my custom setup in Arch Linux. I wrote an article about how customizing and maintaining my tailored operating system is a creative activity to me.\nIn the words of Rob Muhlenstein, \u0026ldquo;Using the Linux Bash terminal command-line is coding. Every command you type on every line is part of an interactive, ongoing program interpreted by Bash and processed by Linux.\u0026rdquo; Having this dialogue with the computer through commands is one of the most fascinating things on this earth to me, and I feel fortunate that I can work with this every day as a DevOps engineer.\nWhy Take Notes? “Your professional success and quality of life depend directly on your ability to manage information effectively.”\nTiago Forte, Building A Second Brain\nIn the modern age we consume very large amounts of information, much more than we can remember. It is therefore very important to manage your intake of information, but also the retention of that information. I find that taking notes on the topics that may be of interest to me at any given time helps me understand the subject much better, because it forces me to clearly formulate my thoughts and convert them to written form.\nOver time a large collection is built up which can be used to generate new ideas.\nI publish certain sections of my note collection hoping it might provide value to others.\nTags and links To organize my notes for the website I use tags, but in my personal note taking system I use markdown links. This is why you will sometimes see [[health]] or [[coding]] in my notes. These are the links that I make in my personal system which I use for navigation and idea generation. It\u0026rsquo;s not very pretty to have these included in the published notes, but it\u0026rsquo;s the side effect of mixing a public and private Zettelkasten.\nCode If you\u0026rsquo;re curious about the writing and publishing process of this blog, see this note.\nYou can find the source code of the blog at github.com/mischavandenburg/blog, and the contents of the zettelkasten are hosted in the zet directory at /blog/content/zet/\n","permalink":"https://mischavandenburg.com/aboutme/","summary":"Favorites These articles came out particularly well, and might provide some value to you, or help you get to know me a little better.\nI\u0026rsquo;m In Love with my Work: Lessons from a Japanese Sushi Master My Neovim Zettelkasten: How I Take Notes in Markdown Using Vim and Bash How and Why I Started Using Vim What is a Zettelkasten? Zettelkasten is a system of note taking and personal knowledge management.","title":"Welcome!"}]